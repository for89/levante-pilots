```{r}
library(tidyverse)
library(glue)
library(here)
source(here("02_score_data/irt_helpers.R"))

task_data_nested <- read_rds(here(glue("01_fetched_data/task_data_nested.rds")))
task_data_combined <- task_data_nested |>
  unnest(data)

run_data_combined <- read_rds(here("01_fetched_data/run_data.rds"))
```

Sumscores for all tasks

```{r sumscores}
sumscores <- task_data_combined |>
  # filter(corpus_trial_type != "Number Line Slider") |> # TODO
  group_by(site, item_task, user_id, run_id) |>
  summarise(prop_correct =  mean(correct),
            total_correct = sum(correct),
            n_trials = n()) |>
  ungroup() |>
  pivot_longer(-c("site", ends_with("_id"), "item_task"),
               names_to = "metric_type", values_to = "metric_value")
write_rds(sumscores, here("02_scored_data","scores","sumscores.rds"))
```

Thetas for tasks that have them

```{r thetas}
thetas <- run_data_combined |>
  filter(!is.na(test_comp_theta_estimate)) |>
  # group_by(site, item_task, user_id, run_id) |>
  # filter(server_timestamp == max(server_timestamp)) |>
  # ungroup() |>
  mutate(metric_type = "ability (CAT)") |>
  select(site, task_id, user_id, run_id, metric_type,
         metric_value = test_comp_theta_estimate) |>
  mutate(item_task = str_replace(task_id, "-es", ""),
         item_task = str_replace(item_task, "-de", "")) 

write_rds(thetas, here("02_scoring_outputs","scores","roar_thetas.rds"))
```

# SRE

Custom scoring for SRE. 

Scoring is per https://roar.stanford.edu/technical/intro-sre.html#sec-sre-scoring

"ROAR-Sentence is a two alternative forced choice (2AFC) task and is scored as the total number of correct responses minus the total number of incorrect responses in the alloted (3 minute) time window. This scoring method controls for guessing by controlling for the number of incorrect responses in the calculation of the scores. Raw Scores: The studentâ€™s raw score will range between 0-130."

Things look terrible across sites though. 

Let's debug what happened with SRE. Working hypothesis: 

- we did the standard 3min version for DE
- we did the standard 3min version for CO and we have a lot of incomplete runs
- in canada we did the 2 block 6 min version and need to select the first block.

```{r}
#| eval: false

tdc <- task_data_combined |>
  filter(str_detect(item_task, "sre")) |>
  mutate(item_num = as.numeric(str_extract(item_id, "[:digit:]+")), 
         ai = str_detect(item_id, "ai"))

tdc_sub <- tdc |>
  group_by(site, run_id) |>
  summarise(num_items = length(unique(item_id)), 
            num_non_ai_items = length(unique(item_id[!ai])), 
            duration = server_timestamp[max(trial_number)] - server_timestamp[min(trial_number)], 
            duration_noai = server_timestamp[max(trial_number[!ai])] - server_timestamp[min(trial_number[!ai])])

  
ggplot(tdc_sub, aes(x = num_items)) + 
  geom_histogram() + 
  facet_wrap(~site)

ggplot(tdc_sub, aes(x = num_items)) + 
   geom_histogram() + 
  facet_wrap(~site)
  
```


So - to score fairly, we need to divide by some kind of constant. Let's start by clipping to the first 180s for Canada, then doing (correct - incorrect) / 180 - "guessing-adjusted correct per second".

```{r sre}

sre_all <- task_data_combined |>
  filter(str_detect(item_task, "sre")) 


sre <- sre_all |>
  group_by(site, item_task, user_id, run_id) |> # corpus_trial_type
  # take only the first block for Canada site
  filter(site != "ca_pilot" | timestamp < timestamp[min(trial_number)] + 180) |>
  summarise(metric_value = (sum(correct) - sum(!correct))/180) |>
  ungroup() |>
  mutate(metric_type = "guessing_adjusted_number_correct")
```

```{r}
#| eval: false

ggplot(sre, aes(x = metric_value)) + 
  geom_histogram() + 
  facet_wrap(~site)
```


# MEFS

```{r}
#| eval: false

mefs_scores <- read_csv(here("02_scoring_outputs","mefs_data","LEVANTE_20250403_1132.csv")) |>
  janitor::clean_names() |>
  select(child_id, a1_total_score, a1_standard_score) |>
  rename(user_id = child_id, 
         total_score = a1_total_score, 
         standard_score = a1_standard_score) |>
  pivot_longer(total_score:standard_score, names_to = "metric_type", values_to = "metric_value") |>
  mutate(item_task = "mefs", run_id = NA) 
```

Figure out how many MEFS userids fit. 

```{r}
#| eval: false

site_users <- task_data_combined |>
  select(user_id, site) |>
  distinct() 

mefs <- left_join(mefs_scores, site_users) |>
  filter(!is.na(site))
```

#  Combine scores

```{r}
# scores_combined <- bind_rows(sumscores, thetas)
# scores_combined <- bind_rows(sumscores, thetas, mg, sre, mefs)
# scores_combined <- bind_rows(sumscores, thetas, mg, sre, mefs, pa) # FIXME: pa not defined here..
# write_rds(scores_combined, here("02_scored_data/scores/scores_general.rds"))
```
