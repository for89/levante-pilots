```{r}
library(tidyverse)
library(glue)
library(here)

sites <- c("ca_pilot", "co_pilot", "de_pilot", "us_pilot")

task_data_nested <- sites |>
  set_names() |>
  map(\(s) read_rds(here(glue("01_processed_data/{s}/task_data_nested.rds")))) |>
  list_rbind(names_to = "site")

task_data_combined <- task_data_nested |>
  select(-task_id) |>
  unnest(data)
```

Sumscores for all tasks

```{r sumscores}
sumscores <- task_data_combined |>
  filter(corpus_trial_type != "Number Line Slider") |> # TODO
  group_by(site, task_id, user_id, run_id) |>
  summarise(prop_correct =  mean(correct),
            total_correct = sum(correct),
            n_trials = n()) |>
  ungroup() |>
  pivot_longer(-c("site", ends_with("_id")),
               names_to = "metric_type", values_to = "metric_value")
```

Thetas for tasks that have them

```{r thetas}
thetas <- task_data_combined |>
  filter(!is.na(theta_estimate)) |>
  group_by(site, task_id, user_id, run_id) |>
  filter(server_timestamp == max(server_timestamp)) |>
  ungroup() |>
  mutate(metric_type = "ability (CAT)") |>
  select(site, task_id, user_id, run_id, metric_type, metric_value = theta_estimate)
```

Custom scoring for memory game

```{r mg}
mg <- task_data_combined |>
  filter(task_id == "memory-game") |>
  mutate(span = str_count(response, ":")) |>
  filter(correct) |>
  group_by(site, task_id, user_id, run_id) |> # corpus_trial_type
  summarise(metric_value = max(span)) |>
  ungroup() |>
  mutate(metric_type = "longest_span")
```

Custom scoring for SRE. 

Scoring is per https://roar.stanford.edu/technical/intro-sre.html#sec-sre-scoring

"ROAR-Sentence is a two alternative forced choice (2AFC) task and is scored as the total number of correct responses minus the total number of incorrect responses in the alloted (3 minute) time window. This scoring method controls for guessing by controlling for the number of incorrect responses in the calculation of the scores. Raw Scores: The studentâ€™s raw score will range between 0-130."

We normalize. 

```{r sre}
sre <- task_data_combined |>
  filter(str_detect(task_id, "sre")) |>
  group_by(site, task_id, user_id, run_id) |> # corpus_trial_type
  summarise(metric_value = (sum(correct) - sum(!correct))/130) |>
  ungroup() |>
  mutate(metric_type = "guessing_adjusted_number_correct")
```


Combine scores

```{r}
scores_combined <- bind_rows(sumscores, thetas, mg, sre)
write_rds(scores_combined, here("02_scored_data/scores_general.rds"))
```
