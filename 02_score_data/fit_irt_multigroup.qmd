This notebook fits multigroup IRT models, in contrast to `fit_irt.qmd`, which is by-group. 

Our task-by-task strategy for looking at measurement invariance:

* fit multigroup IRT model assuming scalar invariance - group mean and variance can vary but the item parameters are assumed to be the same

(TODO items)
* compare 1PL and 2PL (and eventually, 1D and 2D) models using IMV
* look for outliers either on outfit or slopes in 2PL
* uniform logistic DIF as a first step
* compute marginal reliabilities using `marginal_rxx` 

<!-- * use resulting item parameters for fitting CATs -->
* use EAP fscores to get thetas for each child

# General data loading

```{r load-data}
library(tidyverse)
library(glue)
library(here)
library(mirt)

set.seed(1234)
source(here("02_score_data/irt_helpers.R"))

# tasks to include in these analyses
irt_tasks <- c("egma-math",
               "matrix-reasoning",
               "mental-rotation",
               "same-different-selection",
               "theory-of-mind",
               "hearts-and-flowers",
               "memory-game",
               "trog",
               "vocab")

site_task_data_nested <- read_rds(here(glue("01_fetched_data/task_data_nested.rds"))) |>
  filter(task_id %in% irt_tasks)

task_data_nested <- site_task_data_nested |>
  unnest(data) |>
  rename(group = site, item_id = item_uid) |>
  filter(!is.na(item_id)) |>
  nest(data = -task_id) 
```

Get participants. 

```{r}
run_ages <- read_rds(here(glue("01_fetched_data/run_data.rds"))) |>
  select(site, task_id, user_id, run_id, age)
```


# Model fitting

```{r}
# now removes missing item ID data
task_data_prepped <- task_data_nested |>
  mutate(data = map2(task_id, data, rescore_hf)) |>
  mutate(#data = map(data_with_ids, \(df) filter(df, !is.na(item_id))),
         data_filtered = map(data, \(df) df |> #filter_repeat_runs() |>
                               remove_nonshared_items() |>
                               dedupe_items() |> remove_no_var_items()),
         data_wide = map(data_filtered, to_mirt_shape_grouped), 
         data_prepped = map(data_wide, \(df) df |> select(-group)),
         groups = map(data_wide, \(df) df |> pull(group))) |>
  # pull out chance values
  mutate(guess = NA) # TODO
  # mutate(guess = map(data_filtered, # TODO: check that this gives correct order
  #                    \(df) df |> distinct(item_inst, chance) |> pull(chance)))


item_types <- c("Rasch", "2PL") #, "3PL") # set of parameterizations
model_types <- c(1) #, 2) # set of dimensionalities
# model_types <- c(1) # set of dimensionalities

# add arguments for model fitting to data
task_data_args <- task_data_prepped |>
  # duplicate rows per dimensionality x parameterization
  expand_grid(model_type = model_types, item_type = item_types) |>
  # generate model string with item constraints + dimensionality
  mutate(model_str = pmap_chr(list(data, data_prepped, item_type, model_type),
                              generate_model_str_numeric))
```

Joint model fitting. 

```{r fit-models}
# multidyplr attempt
library(multidplyr)
cluster <- new_cluster(14) # needs generalizing
cluster_library(cluster, "tidyverse")
cluster_library(cluster, "mirt")
cluster_library(cluster, "glue")
cluster_copy(cluster, "fit_mirt")
cluster_copy(cluster, "fit_multigroup")


# fit all the models!
task_models <- task_data_args |>
  partition(cluster) |>
  mutate(mod = pmap(list(row_number(), data_prepped, item_type, model_str,
                         model_type, task_id, guess),
                    fit_mirt)) |>
  collect() 


# get each model's coefs, scores, BIC
task_results <- task_models |>
  mutate(coefs = map(mod, mirt_coefs),
         scores = pmap(list(mod, data_filtered, data_prepped), mirt_scores),
         bic = map_dbl(mod, mirt_bic))

# write_rds(task_results, here("02_scored_data/irt_outputs/multigroup_task_results.rds"),
          # compress = "gz")


# best fitting model for each task
task_best <- task_results |>
  group_by(task_id) |>
  filter(bic == min(bic)) |>
  ungroup()
  # select(site, task_id, item_type, model_type, coefs, scores)

# scores from best fitting models
task_scores <- task_best |>
  select(task_id, item_type, model_type, scores, groups) |>
  rename(site = groups) |>
  unnest(cols = c(scores, site)) |>
  mutate(#item_type = fct_recode(item_type, "1PL" = "Rasch"),
         metric_type = glue("merged_data ability ({item_type}-{model_type})")) |>
  select(site, task_id, user_id, run_id, metric_type, metric_value = ability)

write_rds(task_scores, here("02_scored_data","scores","scores_fullpooling.rds"),
compress = "gz")
```

Now fit multigroup models.



```{r}
model_types <- tribble(~item_type, ~invariance_type,
                       "Rasch","configural",
                      "Rasch", "metric",
                      "Rasch", "full",
                      "2PL","configural",
                      "2PL", "metric",
                      "2PL", "scalar_intercepts",
                      # "2PL", "scalar_slopes_and_intercepts", # cut for space in the anova (total hack)
                      "2PL", "full")

multigroup_models <- task_results |>
  full_join(model_types, relationship = "many-to-many") |>
  partition(cluster) |>
  mutate(mod = pmap(list(row_number(), data_prepped, item_type, groups, model_str, 
                                invariance_type, task_id), fit_multigroup)) |>
  collect()

write_rds(multigroup_models,
          here("02_scored_data/irt_outputs/multigroup_outputs.rds"),
          compress = "gz")
```

```{r}
# this is obviously terrible but somehow the following doesn't work 
# do.call(anova, mod) 
# even though anova(mod[[1]], mod[[2]]) works. 
# also somehow we can only do 7 models here not 8?!?!

task_anova <- multigroup_models |>
  group_by(task_id) |>
  summarise(model_types = list(model_types), 
            anova = list(anova(mod[1][[1]], mod[2][[1]], mod[3][[1]], mod[4][[1]], 
                               mod[5][[1]], mod[6][[1]], mod[7][[1]]))) |>
  unnest(cols = c(model_types, anova)) |>
  group_by(task_id) |>
  mutate(delta_bic = BIC - min(BIC), 
         model_type = fct_relevel(glue("{item_type}-{invariance_type}"), 
                                  "2PL-full",
                                  "2PL-scalar_intercepts", "2PL-metric", "2PL-configural",
                                  "Rasch-full", "Rasch-metric", "Rasch-configural"),
         best = delta_bic == 0)


ggplot(task_anova, aes(x = model_type, pch = best, 
                       y = delta_bic, col = item_type, size = as.numeric(best))) + 
  geom_point() + 
  coord_flip() + 
  scale_size_continuous(guide = FALSE, range = c(1,3)) + 
  facet_wrap(~task_id)
ggsave(here("02_score_data", "plots","multigroup_anova.png"))
```


```{r}
best_multigroup <- multigroup_models |>
  mutate(bic = map_dbl(mod, mirt_bic)) |>
  group_by(task_id) |>
  filter(bic == min(bic)) 
```

NOW - we want to refit the best multigroup model with the full group of items, even those that aren't shared across groups. 

The logic here is: if the best model is CONFIGURAL, then we need to use separate models for scoring and nothing is comparable at all.  

Otherwise, we use the best fitting model, but REFIT to all the items. So then we are using all the items under the assumption that if the overlapping items are being fit correctly and are metric/scalar invariant, then so are the non-overlapping items. 

Right now scores for configural models use only overlapping data though. 


```{r}
best_multigroup_refit <- best_multigroup |>
  mutate(data_filtered = map(data, \(df) df |> #filter_repeat_runs() |>
                               dedupe_items() |> remove_no_var_items()),
         data_wide = map(data_filtered, to_mirt_shape_grouped), 
         data_prepped = map(data_wide, \(df) df |> select(-group)),
         groups = map(data_wide, \(df) df |> pull(group))) 

# add arguments for model fitting to data
best_multigroup_refit <- best_multigroup_refit |>
  mutate(model_str = ifelse(invariance_type != "configural", 
                            pmap_chr(list(data, data_prepped, item_type, model_type),
                              generate_model_str_numeric), model_str)) |>
    partition(cluster) |>
  mutate(mod = ifelse(invariance_type != "configural", 
                      pmap(list(row_number(), data_prepped, item_type, groups, model_str, 
                                invariance_type, task_id), fit_multigroup), 
                      mod)) |>
  collect()
```

Now do the scoring. 

```{r}
#independent_task_scores <- read_rds(here("02_scoring_outputs/scores/independent_scores.rds"))

best_multigroup_refit <- best_multigroup_refit |>
  ungroup() |>
  mutate(fscore = map(mod, \(df) fscores(df)[,1]), 
         coefs = map(mod, multigroup_coefs))


task_scores <- best_multigroup_refit |>
  select(task_id, item_type, model_type, invariance_type, groups, scores) |>
  unnest(scores) |>
  mutate(metric_type = glue("ability ({item_type}-{model_type}D-{invariance_type})")) |>
  select(task_id, user_id, run_id, metric_type, metric_value = ability) |>
  left_join(select(run_ages, site, user_id, run_id))

# save scores
write_rds(task_scores, here("02_scoring_outputs/scores/multigroup_scores.rds"),
          compress = "gz")
```







<!-- # Plot fscores.  -->

<!-- ```{r} -->
<!-- # this is terrible column alignment -->

<!-- groups <- multigroup_models |>  -->
<!--   select(task_id, groups, data_wide) |>  -->
<!--   mutate(user_id = map(data_wide, rownames)) |> -->
<!--   select(-data_wide) |> -->
<!--   unnest(c(groups, user_id)) -->


<!-- all_fscores <- best_multigroup |> -->
<!--   mutate(ids = map(data_filtered, \(df) df |> select(user_id, run_id) |>  -->
<!--                      distinct())) |> -->
<!--   select(task_id, ids, invariance_type, item_type, fscore, groups) |> -->
<!--   mutate(metric_type = glue("multigroup ({item_type}-{invariance_type})")) |> -->
<!--   unnest(cols = c("groups", "fscore", "ids")) |> -->
<!--   rename(site = groups,  -->
<!--          metric_value = fscore) |> -->
<!--   left_join(run_ages) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- ggplot(all_fscores, -->
<!--        aes(x = age, y = metric_value, col = site)) +  -->
<!--   geom_point(alpha = .05) +  -->
<!--   geom_smooth(method = "loess", span = 1) +  -->
<!--   xlab("Age (years)") +  -->
<!--   xlim(5,13)+  -->
<!--   ylab("Ability") +  -->
<!--   facet_wrap(~task_id) +  -->
<!--   ggthemes::theme_few() + -->
<!--   ggthemes::scale_color_solarized() +  -->
<!--   theme(legend.position = "bottom") -->
<!-- ``` -->

<!-- Save out scores.  -->

<!-- ```{r} -->
<!-- all_fscores |> -->
<!--   select(site, task_id, user_id, run_id, metric_type, metric_value) |> -->
<!--   write_rds(here("02_scored_data","scores","scores_multigroup.rds"), -->
<!--             compress = "gz") -->
<!-- ``` -->

# Marginal reliabilities

Note, some of these look a little different than with the by-group models.

```{r}
task_rxx <- best_multigroup |>
  mutate(site = map(mod, \(m) m@Data$groupNames),
         group_models = map(mod, multigroup_extract_groups)) |>
  select(-mod, -bic, -fscore) |>
  unnest(cols = c("group_models", "site")) |>
  mutate(rxx = map_dbl(group_models, marginal_rxx)) |>
  select(site, task_id, item_type, model_type, rxx)

task_rxx_wide <- task_rxx |> 
  pivot_wider(names_from = "site", values_from = "rxx") 

write_rds(task_rxx, here("02_scored_data/irt_outputs/multigroup_task_rxx.rds"),
          compress = "gz")
write_csv(task_rxx, here("02_scored_data/irt_outputs/multigroup_task_rxx.csv"),
          compress = "gz")
write_csv(task_rxx_wide,
          here("02_scored_data/irt_outputs/multigroup_task_rxx_wide.csv"),
          compress = "gz")
```
