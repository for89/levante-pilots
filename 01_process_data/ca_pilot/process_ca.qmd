```{r load-data}
library(tidyverse)
library(glue)
library(here)

site <- "ca_pilot"
source(here("01_process_data", "processing_helper.R"))
trials_coded <- read_rds(here(glue("00_prepped_data/{site}/trials_coded.rds")))
task_data <- clean_trial_data(trials_coded)
```

```{r}
# merge in unique item_ids from corpus (ToDo: add other corpora)
math_items <- load_math_items()
trog_items <- load_grammar_items()
```

```{r}
vocab <- task_data |>
  filter(task_id == "vocab", corpus_trial_type=="test") |> # or assessment_stage=="test_response"
  mutate(item_id = paste0("vocab-",item))

trog <- task_data |>
  filter(task_id == "trog", item!="") |>
  select(-item_id) |>
  left_join(trog_items |> 
              select(answer, item_id)) 
```

EF tasks 

```{r}
sds <- process_sds(task_data)
hearts_and_flowers <- process_hearts_and_flowers(task_data)
mg <- process_mg(task_data)
```

```{r}
# egma needs special processing to identify items
egma <- task_data |>
  filter(task_id == "egma-math", item!="") |>
  mutate(item = case_when(item=="{'0': 0, '1': 10}" ~ "0,10", 
                          item=="{'0': 0, '1': 100}" ~ "0,100",
                          item=="{'0': 0, '1': 1000}" ~ "0,1000",
                          item=="{'0': 0, '1': 1}" ~ "0,1",
                          .default = item)) |>
  # recode subtasks for shorter item strings
  mutate(corpus_trial_type = corpus_trial_type |> fct_recode(
    num_id = "number identification",
    num_comp = "number comparison",
    miss_num = "missing number",
    add = "addition",
    sub = "subtraction",
    line_4afc = "number line 4afc",
    mult = "multiplication",
    line_slid = "number line slider",
    frac = "fraction"
  )) |>
  # item_id = "[subtask] [item] [answer]"
  #mutate(item_id = paste(corpus_trial_type, item, answer))
  rename(distractors = distractors_cln)

egma_no_slide <- egma |> 
  filter(corpus_trial_type != "line_slid") |>
  select(-item_id) |>
  left_join(math_items |> 
              filter(trial_type!="Number Line Slider") |>
              select(item, distractors, item_id, answer) |> 
              rename(corpus_answer = answer)) |> # chance_level
  mutate(answer = ifelse(answer!=corpus_answer, corpus_answer, answer)) |> # correct "7/8" issue
  select(-corpus_answer)

# score just number line trials
threshold <- 0.15
numberline <- egma |>
  filter(corpus_trial_type == "line_slid") |>
  mutate(correct = pmap_lgl(list(item, answer, response), \(item, answer, response) {
    # get slider max from item ("{'0': 0, '1': [max_value]}")
    max_value <- as.numeric(str_extract(item, "\\d+$"))
    # get distance b/w response & answer, scale to max, compare to threshold
    abs(as.numeric(response) - as.numeric(answer)) / max_value < threshold
  })) |>
  mutate(chance = threshold * 2) |>
  left_join(math_items |> select(item, distractors, answer, item_id))

# recombine numberline with rest of egma
egma_numberline <- egma_no_slide |>
  bind_rows(numberline)
```

```{r}
# theory of mind is separated by assessment_stage +
# has special processing to identify items +
# hostile attribution correctness recoding
# hostile_values <- read_csv(here("assessment/colombia/stage2/task_analyses/hostile-attribution-coding.csv"))
tom <- task_data |>
  filter(task_id == "theory-of-mind", item!="") |>
  mutate(corpus_trial_type = str_remove_all(corpus_trial_type, "_question")) |>
  mutate(task_id = fct_collapse(corpus_trial_type,
                                "theory-of-mind" = c("false_belief", "reality_check", "reference"),
                                "hostile-attribution" = c("action", "attribution"),
                                "emotion-reasoning" = "emotion_reasoning")) |> #count(task_id, corpus_trial_type)
  group_by(user_id, run_id, task_id, item, corpus_trial_type) |>
  mutate(i = 1:n(), n = n()) |> # sequentially number items
  ungroup() |>
  # item_id = "item [question type]" (+ "[i]" if multiple same type items)
  mutate(item_id = paste(item, corpus_trial_type),
         item_id = paste(item_id, i)) |>
  # item_id = if_else(n == 1, item_id, paste(item_id, i))) |>
  select(-i, -n)
# left_join(hostile_values) |>
# mutate(correct = if_else(task_id == "hostile-attribution",
#                          value %in% c("purpose", "hostile"),
#                          correct)) |>
# select(-value)
# write_rds(tom, here("assessment","colombia","stage2","task_analyses","data","tom_data.rds"))
```

```{r}
# replace separated out data
task_data_coded <- task_data |>
  filter(!task_id %in% c("egma-math", "same-different-selection", "memory-game", "theory-of-mind", "trog", "vocab", "hearts-and-flowers")) |>
  bind_rows(egma_numberline) |> 
  bind_rows(tom) |> 
  bind_rows(trog) |> 
  bind_rows(vocab) |>
  bind_rows(hearts_and_flowers) |>
  bind_rows(mg) |>
  bind_rows(sds) |>
  clean_item_ids() |>
  select(matches("id"), trial_index, corpus_trial_type,
         chance, correct, rt, response, server_timestamp)

task_data_nested <- task_data_coded |>
  # filter(is.na(rt_fast) | !rt_fast, is.na(rt_slow) | !rt_slow) |> # remove too slow/fast RTs
  # select(-starts_with("rt")) |> # drop all RT columns
  nest(data = everything(), .by = task_id) # nest data by task

write_rds(task_data_nested, here(glue("01_processed_data/{site}/task_data_nested.rds")))
```
