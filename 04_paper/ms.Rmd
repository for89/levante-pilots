---
title             : "Creation and validation of the LEVANTE core tasks: Internationalized measures of learning and development for children ages 5-12 years"
shorttitle        : "LEVANTE"

author: 
  - name          : "George Kachergis*"
    affiliation   : "1"
  - name          : "Fionnuala O'Reilly*"
    affiliation   : "1"
  - name          : "Mika Braginsky"
    affiliation   : "1"
  - name          : "Amy Lightbody"
    affiliation   : "1"
  - name          : "Katherine Adams Shannon"
    affiliation   : "1"
  - name          : "Zachary Watson"
    affiliation   : "1"
  - name          : "Xingyao Xiao"
    affiliation   : "1"
  - name          : "Lijin Zhang"
    affiliation   : "1"
  - name          : "Rebecca Zhu"
    affiliation   : "1"
  - name          : "Anya Wanjing Ma"
    affiliation   : "1"
  - name          : "Tonya Murray"
    affiliation   : "1"
  - name          : "Jason Yeatman"
    affiliation   : "1"
  - name          : "Michael Sulik"
    affiliation   : "1"    
  - name          : "Jelena Obradović"
    affiliation   : "1"
  - name          : "Nichola Jenkins"
    affiliation   : "2"
  - name          : "Daniel Ansari"
    affiliation   : "2"
  - name          : "Maria Camilla Perfetti"
    affiliation   : "3"
  - name          : "Julian Mariño"
    affiliation   : "3"
  - name          : "Luise Hornoff"
    affiliation   : "4"  
  - name          : "Manuel Bohn"
    affiliation   : "5"  
  - name          : "Nilam Ram"
    affiliation   : "1"
  - name          : "Benjamin W. Domingue"
    affiliation   : "1"    
  - name          : "Michael C. Frank"
    affiliation   : "1"
    role:
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
      - "Supervision"

affiliation:
  - id            : "1"
    institution   : "Stanford University"
  - id            : "2"
    institution   : "Western University"
  - id            : "3"
    institution   : "Universidad de los Andes"
  - id            : "4"
    institution   : "Max Planck Institute for Evolutionary Anthropology"
  - id            : "5"
    institution   : "Leuphana University"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

abstract: | 
  We present the Learning Variability Network Exchange (LEVANTE) core tasks, a set of nine short and engaging computer adaptive tasks designed to assess learning and development in children ages 5--12 years across a wide range of languages and cultures. Using a simple and uniform multi-alternative forced choice format, these tasks measure constructs including math, execxutive function, reasoning, and social cognition and can be administered on a tablet or computer both in person or remotely. We describe the design and selection of these instruments, and then report on their reliability and validity in a pilot sample of XYZ children recruited in Colombia, Germany, and Canada. Tasks are scored using item response theory models. These models can be used to create computer adaptive versions of the tasks, allowing the entire battery to be given in under an hour. We discuss the use and extension of these tasks in the service of creating an open dataset to describe variability in children's development and learning across contexts. 


keywords: "cognitive development"
wordcount: "X"

bibliography: "library.bib"

floatsintext: no
linenumbers: yes
draft: no
mask: no

figurelist: no
tablelist: no
footnotelist: no

classoption: "man"
output:
  papaja::apa6_pdf:
    latex_engine: xelatex
    keep_tex: true
    includes:
      in_header: preamble.tex
---


```{r, include = FALSE}
# authors
# Kachergis*, O’Reilly* 
# 
# Major contributors:
# Mika
# Doria
# Mike
# Nilàm
# 
# Other authors:
# Rebecca 
# Kat
# Amy
# Zach
# Lijin
# Ben
# 
# Pilot data contributors
# DE: Luise & Manuel
# CO: Camila & Julian
# CA: Nichola & Daniel 
# 
# Task contributors
# Jason, Tonya (TROG), Anya (VV) 
# Michael Sulik, Jelena Obradovic (EF)

```


```{r setup, include = FALSE}
library(here)
library(papaja)
library(tidyverse)
library(glue)
library(viridis)
library(lavaan)
library(dplyr); library(tidyr); library(readr)
source(here("03_summaries","plotting_helper.R"))
source(here("plot_settings.R"))
source(here("03_summaries","scores_helper.R"))

sites <- c("Colombia", "Germany", "Canada") #, "pilot_langcog_us", 
site_pal <- solarized_pal()(length(sites)) |> rlang::set_names(sites)
# r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE)
```

# Introduction

Developmental variability and change during childhood is a focus of intense theoretical and practical interest. From tracking children's growth over time to evaluating intervention outcomes or exploring environmental and contextual moderators, a wide range of scientific goals require accurate assessments. Ideal psychological measures provide efficient, reliable, and valid measures of particular constructs that can be applied across a range of ages, situations, and contexts. Yet, in most cases, a large gap separates the situation of a researcher searching for measures from this ideal. 

Because children's overall capacities are so dependent on their age, it can be very challenging to use the same measure across children of different ages. Young children require simple tasks that are not verbally demanding, while older children can answer more complicated questions. In addition, younger children typically require shorter tasks, often reducing measurement reliability. Yet giving different tasks to different ages can mean that scores are not comparable to one another, making tracking developmental growth challenging in many domains.  

A second set of challenges concern cross-context comparisons.  Ideal developmental measures should be validated in a global context and applicable to children across many cultures and languages. Child development is an issue of global importance [LANCET CITES], yet the vast majority of measures are developed in very specific (often English-speaking) contexts. Providing cross-culturally validated measures allows for the collection of comparable data across contexts, opening up opportunities for theoretical synthesis.

A final set of challenges has to do with accessibility. Many gold-standard measures are commercially distributed. They are costly for researchers to use, and in addition, publishers may place barriers on new translation and adaptation. Publishers also typically hold both item information and normative data closely, blocking many types of secondary investigation. 

In the context of these challenges, we describe the Learning Variability Network Exchange (LEVANTE) [@frank2025]. LEVANTE provides a technical framework for data collection: researchers can use the LEVANTE dashboard to assign both surveys and tasks to children, caregivers, and teachers. Data collected via the dashboard are harmonized and validated and become accessible through a data repository, first to the researchers who collect them and eventually -- through regular releases -- to the broader research community. Through a partnership with the Jacobs Foundation, sites around the world are funded to collect longitudinal data from children using the LEVANTE framework. The eventual goal of LEVANTE is to create a large dataset documenting children's learning and development across contexts. 

The current manuscript introduces the LEVANTE core tasks, a suite of behavioral measures for children developed for LEVANTE. In our initial development of the framework, we cast a broad net for important constructs in child development with well-accepted measures that had been used internationally. This process is described in @frank2025. The broad constructs that we selected were executive function, language, mathematics, reasoning, and social cognition, with these being instantiated through a number of well-accepted tasks. 

To create our core tasks, we selected pre-existing measures from the literature that tapped each of these constructs. When possible we prioritied measures with strong psychometric properties, previous use across a broad range of cultures, applicability across a broad range of ages, and lack of commercial or licensing constraints. This process yielded a series of measures, which we implemented in an open source web platform. Table \@ref(tab:tasks) shows these tasks, organized by construct. 

In addition to the constructs described above, we were also interested in the assessment of literacy. The LEVANTE core tasks battery makes use of a number of previously-validated literacy tasks from the Rapid Online Assessment of Reading [@yeatman2021] , including single word reading, sentence reading efficiency, and phonological awareness. We do not report on these tasks here, though we make use of them for validation of language measures. Similarly, we included a commercially-available broad measure of executive function, the Minnesota Executive Function Scale (MEFS) [@mefs], for validation purposes. 
 

```{r tasks, output="asis"}
tasks <- tribble(~`Construct`, ~`LEVANTE name`, ~`Prior names / Source task name`, ~`Adaptive?`, ~Reference,
                 "Executive Function", "Hearts and Flowers", "Hearts and Flowers", "", "XYZ",
                 "", "Memory", "Corsi Block Task", "X", "",
                 "", "Same and Different", "Same Different Selection Task", "X", "",
                 "Language", "Vocabulary", "Picture Vocabulary", "X", "",
                 "", "Sentence Understanding", "Test for Reception of Grammar (TROG)", "X","",
                 "Math", "Math", "Early Grades Math Assessment (EGMA)", "X", "",
                 "Reasoning", "Pattern Matching", "Matrix Reasoning", "X", "",
                 "", "Shape Rotation", "Mental Rotation", "X", "",
                 "Social Cognition", "Stories", "Theory of Mind", "X", "")

papaja::apa_table(
  tasks, landscape = TRUE,
  caption = "The LEVANTE core tasks, presented with their internal label as well as prior labels used in the literature.")
```

Here we report on the development and validation of these tasks. This is an iterative process in which data from 5--12 year old children has been collected across three sites: Bogota, Colombia; Leipzig, Germany; and Ottawa, Canada. In some cases that we note below, we used these data during the data collection process to make minor changes to the tasks. We use data from these three pilot sites both to provide initial evidence on the reliability and validity of the measures and to develop efficient, computer adaptive (CAT) versions of nearly all of the tasks. 

A key component of this process is the use of psychometric models based on item-response theory (IRT) [@embretson2001]. IRT models provide a family of models that allow the joint estimation of the difficulty of individual task items (e.g., math questions) and the ability of individual children. A fitted IRT model provides task parameters that can be used to estimate the ability of a new test taker given their responses on some or all of the same items. In addition, IRT parameters are used in the construction of CATs, which choose the most relevant items to give to estimate the ability of a particular individual. Critically for our purposes, the use of IRT models means that we can provide comparable scores on the same scale to a younger child who saw mostly easier task items and an older child who saw harder items; these models thus allow us to address our first key challenge posed above.  

Because our data come from three sites, each with their own translations and adaptations of the specific tasks, we can also use multi-group IRT models to explore the question of invariance: whether measures function similarly across different groups [@bornstein2016]. While measurement invariance is more commonly discussed in the factor analytic literature, it is also applicable to IRT (where it is sometimes analyzed at the level of individual test items as "differential item function" across groups) [@thissen2024]. Here we use multigroup model comparisons (described below) to investigate whether our tasks measure similarly structured constructs across groups. In particular, where possible, we aim for *scalar invariance*, in which individuals from different groups still show the same relative ordering of difficulty across items (e.g., they still find fractions items harder than division items in a math test). In some cases, we may fall back to *metric invariance*, in which items show different difficulties across groups, or *configural invariance*, in which items show different degrees of ability discrimination as well (e.g., if some problem types are unfamiliar to children in one group and so do not discriminate between high and low ability children). These models allow us to begin to address the second challenge posed above.

<!-- https://www.tandfonline.com/doi/10.1080/00273171.2024.2396148 -->

In what follows, we begin by describing the nine LEVANTE core tasks, organized by construct. We then discuss the process of translation and adaptation that produced the Spanish and German versions of these tasks from the original English source. We then discuss our pilot data collection efforts in Colombia, Germany, and Canada. We present our IRT-based scoring techniques and the results of multi-group comparison. Using scores from these analyses, we then present preliminary evidence on the reliability and validity of the tasks, recognizing that in many cases these tasks are still under construct and we anticipate increases in reliability as we iteratively improve items. 

We end by discussing future plans for further internationalization and downward extension of the tasks. Critically, LEVANTE embraces open science values, aiming to create measures and data that are permissively licensed and reusable and extensible by the international research community. These values address our final challenge posed above: the aim of LEVANTE is to minimize barriers to reuse, accelerating progress towards a global science of learning and development. 

# The LEVANTE core tasks


```{r}
task_time_summary <- read_csv(here("03_summaries/tables/task_time_summary.csv"))
site_labels <- c("Canada" = "pilot_western_ca",
                 "Colombia" = "pilot_uniandes_co",
                 "Germany" = "pilot_leuphana_de")
all_scores <- readRDS(here("02_scoring_outputs","scores","scores_combined.rds")) |>
  # bind_rows(mefs) |>
  group_by(user_id, item_task) |>
  arrange(time_started) |>
  mutate(run_number = 1:n(),
         age_gap = age - age[1]) |>
  ungroup() |>
  mutate(site_label = site |> fct_recode(!!!site_labels)) |>
  mutate(task = task |> str_to_title() |> fct_inorder(),
         task_category = task_category |> str_to_title() |> fct_inorder())

task_map <- all_scores |> distinct(item_task, task)

task_time_table <- task_time_summary |>
  filter(!is.na(is_cat)) |>
  filter(item_task %in% core_tasks) |>
  mutate(site_label = site |> fct_recode(!!!site_labels),
         adaptive = if_else(is_cat, "Adaptive", "Non-adaptive")) |>
  left_join(task_map) |>
  select(task, site_label, adaptive, median_diff) |>
  pivot_wider(names_from = c(adaptive, site_label), values_from = median_diff) |>
  select(task, starts_with("Non"), , starts_with("Adaptive")) |>
  arrange(task)

options(papaja.na_string = "–")
task_time_table |>
  papaja::apa_table(
    col.names = c("Task", rep(c("Germany", "Colombia", "Canada"), 2)),
    col_spanners = list("Non-adaptive" = c(2, 4), "Adaptive" = c(5, 7)),
    align = c("l", rep("r", 6)),
    caption = "Median task durations (minutes).")
options(papaja.na_string = "NA")
```


The LEVANTE core tasks are implemented using jsPsych [@deleeuw] and can be presented in a web browser on a tablet or laptop, with responses possible using a touchscreen, keyboard, or mouse. Because of this variability in format of administration, they focus on response correctness not reaction time and so they are mostly untimed. The tasks are designed for simplicity and clarity so as to be accessible to children across a wide age range, and so with only modest exceptions, nearly all are in the format of a multi-alternative forced choice with a maximum of four choices. This uniformity of format means that in most cases instructions can be short and easy to understand, minimizing delays when the tasks are given in sequence as a battery. Figure \@ref{fig:tasks} shows screenshots from a number of tasks. In the remainder of this section, we briefly present each of the LEVANTE core tasks.

General task lengths are described below and average task durations are given in Table \@ref(tab:durations). Because of our interest in testing children across a wide range of ages, we intentionally included trials that we anticipated would be both very easy and very hard for children. Overall we worried that this strategy would lead to frustration for children, however, so during pilot testing, we experimented with a number of different stopping rules. In early iterations of pilot testing in Bogota, Colombia, we ended tasks after three incorrect trials; we later modified this rule to end tasks after six incorrect trials. 

All tasks are available for demonstration purposes at [http://researcher.levante-network.org](). Source code for the tasks is available at [https://github.com/levante-framework/core-tasks](). Task code and assets are licensed CC-BY-NC 4.0 for non-commercial reuse (including educational use by not-for-profit and governmental entities) with appropriate attribution. Please see repository license for more details.

## Language 

### Sentence Understanding

The Test for Reception of Grammar (TROG) [@bishop1982] is a multiple-choice measure of receptive grammatical understanding. On each trial, the child hears a spoken sentence and is asked to select one of four pictures that best matches its meaning. The original test contained 20 blocks, each with four items assessing the same grammatical structure. In our adaptation (based on the original TROG, which was permissively licensed for reuse), we removed a small number of items due to changes in cultural norms. In addition, based on early pilot testing showing that many trials were easy for older children, we added a set of several dozen more challenging sentences. All illustrations were remade with details intended to be accessible across a broad range of cultures. The task had 103 separate items.

### Vocabulary

The Vocabulary task was developed as a non-commercial, open alternative to tasks such as the Peabody Picture Vocabulary [@peabody] and the NIH Toolbox Picture Vocabulary Task [@nihtoolbox]; see @long_ma_tan_silverman_frank_yeatman_2025 for more details. In this task, children are presented with a word and four pictures. Children must select the correct picture over the distractor picture. Targets and distractors were selected from the THINGS dataset [@hebart2019things], specifically from the permissively-licensed subset of the data. Each image had a semantically close and semantically far distractor as well as an unrelated distractor image (e.g., target word "acorn" with close distractor being a coconut, and the far and unrelated distractors being keys and laundry). The task included 170 total items. 

## Math

We developed the Math task based on the EGMA (Early Grade Mathematics Assessment) [@egma]. This short, paper-and-pencil assessment is widely administered in international contexts for children ages 5--8 and includes number identification, number comparison, missing number, addition, and subtraction sub-tests. In our adaptation, we increased the breadth of the initial item bank, added multiplication, division, and fractions items to extend the age range up. We also added number line identification problems in which children had to place a marker on a number line across a range of different scales (including simpler 0--10 trials and more challenging larger scales as well as a fraction scale). Number line problems of this type have been suggested to be strongly related to math ability [@siegler other refs]. The primary version of this task included 275 items (though no child saw all of these) and the secondary version for retest purposes included an additional 237. 

## Reasoning

### Matrix Reasoning

### Shape Rotation

  <!-- > [bd: note that we describe this as 'mental rotation' below] -->

## Executive Function

### Same Difference Selection

The Same Different Selection task (Obradovi\'c \& Sulik, year?) is designed to assess cognitive flexibility in children. It draws upon elements from the ‘Something's the Same’ task (Willoughby et al., 2012) and the ‘Flexible Item Selection Task’ (Jacques \& Zelazo, 2001). In this task, children are presented with sets of items that vary along multiple dimensions, such as shape, color, size, and number. They are required to identify similarities and differences between items based on these dimensions, thereby engaging their ability to shift attention and adapt to changing rules or criteria.

### Hearts and Flowers

The Hearts and Flowers task (Davidson et al., 2006) assesses inhibitory control and cognitive flexibility. Participants respond according to stimulus type: pressing a key on the same side as a heart (congruent rule), and on the opposite side for a flower (incongruent rule). The task includes three blocks - congruent (hearts only), incongruent (flowers only), and mixed (hearts and flowers). The congruent block serves as a baseline with minimal executive demands. Inhibitory control is typically measured via performance on the incongruent block, which requires overriding a spatially dominant response, while cognitive flexibility is assessed using the mixed block, which demands switching between rules based on the stimulus (Wright \& Diamond, 2014).which demands switching between rules based on the stimulus (Wright \& Diamond, 2014).

### Memory Game

The Corsi Block task is a widely used measure of visuospatial short-term memory [@corsi1972]. In the standard version, a set of four blocks is arranged in a fixed spatial configuration. During each trial, a subset of blocks lights up one at a time in a specific sequence. The child is required to reproduce the sequence by clicking the blocks in the same order. The task begins with short sequences (e.g., two items) and gradually increases in difficulty (up to five or more) until the child fails two sequences of the same length. The longest correctly reproduced sequence reflects their visuospatial span. When adapted for younger children or digital administration, the number of visible blocks may be reduced (e.g., four blocks), and span lengths are typically capped at five to reduce task complexity.

## Social Cognition


### Stories task


# Translation and Adaptation

Initially designed in English, core tasks were subsequently adapted for Spanish and German following a set of internationalization procedures in collaboration with the sites collecting pilot data in Colombia and Germany. We began with AI-generated translations of task instructions, items, and other key terminology such as encouragement phrases that appear throughout the core tasks. A professional translator then reviewed and edited the AI translations prior to a pilot site representative confirming context and appropriateness to the task. Back translation using AI technology facilitated evaluation of meaning and context with discussion of items or wording as needed to ensure both cultural relevance and task integrity. For tasks evaluating language skills (e.g. Vocabulary; Sentence Understanding), an additional step was employed. Linguists with expertise in language acquisition reviewed items and made recommendations about relevant changes or additions specific to the Spanish and German languages.

# Pilot Data Collection

The pilot site partnership represents an integral component of the core task development. Across three countries and three languages, pilot sites provided collaboration on task adaptation and functionality, iterative task testing, and diverse settings for infrastructure deployment. Data collected from pilot sites allowed analyses of construct validity, measurement invariance, test-retest reliability, parameters for computer adaptive testing, and a demonstration of developmental growth measured by the tasks across constructs.  

One key feature of the LEVANTE framework is that all data are completely de-identified, reducing legal and ethical obstacles to data sharing. No demographic information or other identifiers are ever entered into the LEVANTE dashboard by the sites, and ages are only given to a one month precision [@frank2025]. We anticipate that sites will add sociodemographic information into the dataset at a later time via an app that is currently under development that ensures that there is no statistical reidentification risk for  individuals (e.g., due to rare combinations of traits in a community). For this reason, here we provide only minimal demographic characterization and analysis of the children from our pilot sites.  

```{r data}
# mefs <- read_rds(here("02_scoring_outputs", "scores", "scores_mefs.rds"))



included_scores <- all_scores |>
  filter(!is.na(age), age >= 5, age <= 12) |>
  filter(run_number == 1 | (run_number == 2 & age_gap > 1/6))

coretask_scores <- included_scores |>
  filter(item_task %in% core_tasks)

coretask_firstrun_scores <- coretask_scores |>
  filter(run_number == 1)
```


```{r}
ages <- coretask_scores |>
  group_by(site, site_label, dataset, user_id) |>
  summarise(age = min(age), 
            n_runs = n())
```

```{r, fig.height = 3}
ggplot(ages, aes(x = age, fill = site)) + 
  facet_wrap(vars(site_label)) + 
  geom_histogram(binwidth = 1, color = "white") +
  scale_fill_solarized(guide = "none") +
  scale_y_continuous(expand = expansion(0, 0)) +
  labs(x = "Age (years)", y = "Number of children")
```

```{r}
source("compute_counts.R")
```

<!-- For each site, exclusions (raw numbers + percent of site total): -->
<!-- - runs excluded due to: -->
<!--   - missing metadata -->
<!--   - duplicate run -->
<!--   - too short / incomplete -->
<!--   - validation failures (keymashing) -->
<!--   - kids excluded due to  -->
<!--     - missing metadata -->
<!--     - age out of range -->

<!-- Example: Germany recorded a total of X runs, of which N (X%) were excluded due to reason. -->

<!-- Of the included runs, there were a total of NNN trials, of which XYZ were excluded. For reasons of space we do not disaggregate these exclusions by task, but (descriptive text, like most of them were excluded from these two tasks).  -->

<!-- Trial exclusions -->
<!-- - due to RT issues -->


<!-- After exclusion, we ended up with XYZ kids in the site (% of total kids with any runs at all before exclusions). These kids had a total of XYZ tasks (average XYZ, range).  -->
<!-- (note Germany needs retest description too) -->

Across all tasks, the sites collected a total of `r sum(site_totals)` runs (`r get_count_row(1)`) from `r sum(user_totals$n_users)` unique children (`r user_totals |> deframe() |> collapse_value_list()`), of which some were excluded for being incomplete, invalid, or duplicate (`r get_count_row(2)`), for missing item metadata (`r get_count_row(3)`), for missing age (`r get_count_row(4)`), or for age being out of range or having too short of a retest gap (`r get_count_row(5)`), resulting in a total of `r sum(user_score_counts$n_runs)` scored runs (`r user_score_counts |> select(site, n_runs) |> deframe() |> collapse_value_list()`) from `r sum(user_score_counts$n_users)` unique children (`r user_score_counts |> select(site, n_users) |> deframe() |> collapse_value_list()`).

Within the included runs, there were a total of `r total_trial_counts$n_trials` trials (`r site_trial_counts |> select(site, n_trials) |> deframe() |> collapse_value_list()`), of which `r total_trial_counts$excluded` were excluded (`r site_trial_counts |> select(site, excluded) |> deframe() |> collapse_value_list()`) due to too fast or too slow reaction times. For reasons of space we do not disaggregate these exclusions by task, but note that the task with the highest exclusion rate was Pattern Matching (`r percentify(max(task_trial_counts$pct_excluded))` trials excluded), while all the other tasks ranged between `r percentify(min(task_trial_counts$pct_excluded))` and `r percentify(sort(task_trial_counts$pct_excluded, decreasing = TRUE)[2])` trials excluded.


## Colombia

Partners at the Universidad de Los Andes, led by Julián Mariño, utilized school-based data collection across four schools in Bogotá and three schools in the rural areas of Caquetá and Boyacá, Colombia. By partnering with schools, the research team oversaw task data collection with children using a group testing format on tablets provided by the team. The Colombia site collected data from XX children (XX female; XX male), ages 5-12 years. 

## Germany

Partners at Max Planck Institute for Evolutionary Anthropology, led by Manuel Bohn, collected data using family-based remote data collection. Participants were recruited via an existing database by first contacting families via telephone with an invitation to participate in the study. Upon agreement, families received an email with information to log into the LEVANTE system and complete the tasks assigned to them at home using their own computer or tablet. Germany provided retest data with follow-up assignments sent to families 3-6 months after initial completion of the tasks. The Germany site collected data from XX children (XX female; XX male), ages 5-12 years with XX children completing the retest assignments.

## Canada

Partners at Western University, led by Daniel Ansari, recruited participants from the local community and tested children individually in a clinic setting. The Canadian site collected data from XX children (XX female; XX male), ages 5-12 years. 


# Scoring

```{r invariance, output="asis"}
invariances <- read_csv(here("04_paper","display","invariance.csv")) |>
  mutate(across(everything(), \(v) replace_na(v, "")))

papaja::apa_table(
  invariances |> select(`Invariance`, `Model`, `Factor analysis name`,
                        `IRT explanation`, `Interpretation`),
  align = c("m{2cm}", "m{3cm}", "m{5cm}", "m{5cm}", "m{5cm}"),
  caption = "Measurement invariance across factor analysis and item response theory.",
  landscape = TRUE)
```

## IRT Calibration and Model Selection

```{r model-selection, echo=FALSE, results='asis', message=FALSE, warning=FALSE}

# -- Ensure scores are available  --
if (!exists("all_scores")) {
  sco_path <- here::here("02_scoring_outputs","scores","scores_combined.rds")
  if (file.exists(sco_path)) {
    all_scores <- readRDS(sco_path)
  } else {
    stop("Missing scores file: ", sco_path,
         "\nRun `git lfs pull` in the repo and/or knit the full Rmd so earlier chunks create `all_scores`.")
  }
}
# -- Load marginal reliability summary if present (used as fallback for selection) --
task_rxx_path <- here::here("02_scoring_outputs","task_rxx.rds")
if (file.exists(task_rxx_path)) {
  task_rxx <- tryCatch(
    readRDS(task_rxx_path),
    error = function(e) {
      if (requireNamespace("readr", quietly = TRUE)) {
        readr::read_rds(task_rxx_path)
      } else stop("Could not read task_rxx.rds; install.packages('readr') or ensure it's a valid RDS.")
    }
  )
} else {
  task_rxx <- tibble(task = character(),
                     site = character(),
                     itemtype = character(),   # expected values: "rasch" or "2pl"
                     invariance = character(), # e.g., "scalar", "metric"
                     rxx = numeric())
}
# -- Site labels should be defined earlier; if not, define a minimal default --
if (!exists("site_labels")) {
  site_labels <- c("pilot_western_ca" = "Canada",
                   "pilot_uniandes_co" = "Colombia",
                   "pilot_leuphana_de" = "Germany")
}
# -- Prefer explicit model selection export when available; otherwise infer from reliability --
sel_path <- here::here("04_paper","display","model_selection.csv")
has_sel  <- file.exists(sel_path)
has_rxx  <- nrow(task_rxx) > 0
if (has_sel) {
  # Expect columns: task, site, selected_model (e.g., "Rasch" or "2PL"), optionally BIC, delta_bic
  model_sel <- read_csv(sel_path, show_col_types = FALSE) |>
    mutate(site_label = recode(site, !!!site_labels),
           Model = selected_model)
} else if (has_rxx) {
  # Use the model with highest average reliability per task/site as a descriptive proxy
  model_sel <- task_rxx |>
    mutate(site_label = recode(site, !!!site_labels),
           Model = case_match(itemtype,
                              "rasch" ~ "Rasch",
                              "2pl"   ~ "2PL",
                              .default = toupper(itemtype))) |>
    group_by(task, site_label) |>
    slice_max(order_by = rxx, n = 1, with_ties = FALSE) |>
    ungroup()
} else {
  stop("Neither `04_paper/display/model_selection.csv` nor `02_scoring_outputs/task_rxx.rds` is available.\n",
       "Export a model selection summary or generate the reliability file and re-knit.")
}
# -- Collapse to one model per task (most common across sites) --
per_task_model <- model_sel |>
  count(task, Model, name = "n_sites") |>
  group_by(task) |>
  slice_max(order_by = n_sites, n = 1, with_ties = FALSE) |>
  ungroup()
# -- Attach mean rxx, if available, for context --
if (has_rxx) {
  rxx_table <- task_rxx |>
    mutate(site_label = recode(site, !!!site_labels),
           Model = case_match(itemtype, "rasch" ~ "Rasch", "2pl" ~ "2PL", .default = toupper(itemtype))) |>
    select(task, Model, site_label, rxx) |>
    group_by(task, Model) |>
    summarize(`Mean reliability (rxx)` = mean(rxx, na.rm = TRUE), .groups = "drop")

  display_tbl <- per_task_model |>
    left_join(rxx_table, by = c("task","Model")) |>
    arrange(task)
} else {
  display_tbl <- per_task_model |>
    mutate(`Mean reliability (rxx)` = NA_real_) |>
    arrange(task)
}
# -- Render APA-style table --
papaja::apa_table(
  display_tbl, label = "model-selection",
  caption = if (has_sel)
    "Selected calibration model per task with mean marginal reliability across sites (from model_selection.csv)."
  else
    "Inferred calibration model per task (highest average reliability across sites) with mean marginal reliability.",
  note = if (has_sel)
    "Model selection followed a BIC rule: 2PL preferred when ΔBIC ≤ −10; otherwise Rasch selected for parsimony."
  else
    "When a formal model-selection export was not available, the model with highest average rxx served as a descriptive proxy."
)

```

We calibrated each LEVANTE task using Rasch and 2PL models and selected the more constrained model unless fit indices provided strong evidence in favor of the 2PL ($\Delta$BIC $\leq$ −10). Across most tasks, the Rasch specification was adequate, indicating that item discriminations did not vary substantially. Two tasks—Mental Rotation (rxx = .93) and Same–Different Selection (rxx = .96)—were better described by the 2PL model, reflecting heterogeneity in how strongly items discriminated among children.

Estimates of marginal reliability highlighted meaningful differences across tasks (Table \@ref(tab:model-selection)). Math (.90), Matrix Reasoning (.82), and Memory Game (.93) all demonstrated high reliability, suggesting precise measurement even in the pilot samples. Vocabulary (.83) and Sentence Understanding/TROG (.73) achieved moderate reliability, consistent with tasks that are informative but may benefit from additional refinement. By contrast, Hearts and Flowers (.45) and Theory of Mind (.41) showed low reliability, underscoring that these implementations currently provide limited psychometric information and require further development before supporting valid cross-site comparisons.

In summary, Rasch calibration suffices for most tasks, while allowing variable discriminations improves model fit for more heterogeneous domains. Several tasks already demonstrate reliability suitable for adaptive testing, whereas those with lower reliabilities should undergo item revision and expansion in subsequent piloting rounds.

## Multigroup Calibration and Measurement Invariance 
```{r invariance-summary, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}

inv_path <- here::here("04_paper","display","invariance.csv")
stopifnot(file.exists(inv_path))

inv <- read_csv(inv_path, show_col_types = FALSE) |>
  mutate(across(everything(), ~ tidyr::replace_na(.x, "")))

# Pick a column to read the invariance level from, if available
lvl_candidates <- intersect(c("Level", "Invariance"), names(inv))

if (length(lvl_candidates) > 0) {
  # Use the first matching column for level extraction
  lvl_col <- lvl_candidates[1]
  inv_levels <- inv |>
    mutate(Level_norm_raw = tolower(.data[[lvl_col]]))
} else if ("Interpretation" %in% names(inv)) {
  # Fall back to Interpretation text
  inv_levels <- inv |>
    mutate(Level_norm_raw = tolower(.data[["Interpretation"]]))
} else {
  # If neither exists, mark as unspecified
  inv_levels <- inv |>
    mutate(Level_norm_raw = "unspecified")
}

inv_levels <- inv_levels |>
  mutate(Level_norm = case_when(
    str_detect(Level_norm_raw, "scalar")    ~ "Scalar",
    str_detect(Level_norm_raw, "metric")    ~ "Metric",
    str_detect(Level_norm_raw, "configur")  ~ "Configural",
    TRUE                                    ~ "Unspecified"
  ))

# Summary counts by invariance level
sum_counts <- inv_levels |>
  count(Level_norm, name = "Number of tasks") |>
  mutate(Level_norm = factor(Level_norm, levels = c("Scalar","Metric","Configural","Unspecified"))) |>
  arrange(Level_norm)

papaja::apa_table(
  sum_counts |>
    rename(`Invariance level` = Level_norm),
  caption = "Summary of multigroup invariance outcomes across LEVANTE tasks.",
  label = "invariance-summary"
)


```

```{r invariance-mapping, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
# Detailed mapping table 
papaja::apa_table(
  inv |>
    select(any_of(c("Goal", "LEVANTE name", "Factor analysis name",
                    "IRT explanation", "Interpretation"))),
  align   = c("m{2.5cm}", "m{3cm}", "m{5cm}", "m{5cm}", "m{5cm}"),
  caption = "Measurement invariance mapping across factor analysis and IRT. Scalar invariance supports direct cross-site score comparability; metric and configural indicate progressively weaker comparability.",
  label = "invariance-mapping"
)

```

We next examined whether the LEVANTE tasks functioned similarly across Colombia, Germany, and Canada using multigroup IRT models. Starting with a configural model (all parameters free), we sequentially imposed equality constraints on discriminations (metric) and then on both discriminations and difficulties (scalar). Because residual variances are not separately estimated in binary IRT, strict invariance was not tested. Model selection relied on $\Delta$BIC, retaining the most constrained model that did not show meaningful loss of fit ($\Delta$BIC $\leq$ +10).

Table \@ref(tab:invariance-summary) summarizes outcomes. Math and Matrix Reasoning supported scalar invariance, indicating no evidence of DIF and suggesting that scores can be compared directly across sites, at least in these pilot samples. Vocabulary supported metric invariance: items discriminated similarly across sites but varied in difficulty, so associations are interpretable but mean-level comparisons require caution. Hearts and Flowers and Theory of Mind showed only configural invariance, implying site-specific item functioning and the need for further revision. For the remaining tasks (e.g., TROG, Memory Game), invariance could not be established given current sample sizes.

Table \@ref(tab:invariance-mapping) aligns the factor-analytic and IRT definitions of configural, metric, and scalar invariance, clarifying what kinds of cross-site comparisons each level supports. Taken together, these findings suggest that some tasks are already suitable for international comparability, while others require additional development and piloting.


## Item-Level Diagnostics
```{r item-classical-detail, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}

# 1) load trial data
trial_path <- here("01_fetched_data", "trial_data.rds")
trial_data <- readRDS(trial_path)

# 2) keep only valid trials, 0/1 correctness, dedupe last attempt per (user, task, item)
resp_df <- trial_data %>%
  filter(valid_trial) %>%                               # logical, not string
  transmute(
    user_id  = as.character(user_id),
    task     = tolower(item_task),
    item_id  = as.character(item_uid),
    response = as.integer(correct),                     # logical -> 0/1
    ts       = timestamp
  ) %>%
  filter(!is.na(user_id), !is.na(task), !is.na(item_id), response %in% c(0L,1L)) %>%
  arrange(user_id, task, item_id, ts) %>%
  group_by(user_id, task, item_id) %>%
  slice_tail(n = 1) %>%                                 # keep most recent attempt
  ungroup() %>%
  select(-ts)

# 3) classical stats helper (proportion correct + point-biserial vs. rest)
classical_stats <- function(df_person_item, min_n_item = 30) {
  totals <- df_person_item %>%
    group_by(user_id) %>%
    summarise(total = sum(response, na.rm = TRUE),
              k     = sum(!is.na(response)), .groups = "drop")

  df_person_item %>%
    left_join(totals, by = "user_id") %>%
    mutate(rest = total - response, ok = k >= 2) %>%
    group_by(item_id) %>%
    group_modify(~{
      x_all <- .x
      x_ok  <- .x %>% filter(ok, !is.na(response), !is.na(rest))
      tibble(
        n           = sum(!is.na(x_all$response)),
        p_classical = mean(x_all$response, na.rm = TRUE),
        pbis        = if (nrow(x_ok) < 5 || sd(x_ok$rest) == 0 || sd(x_ok$response) == 0)
                        NA_real_ else suppressWarnings(cor(x_ok$response, x_ok$rest))
      )
    }) %>%
    ungroup() %>%
    mutate(
      flag_min_n     = n < min_n_item,
      flag_p_extreme = (!flag_min_n) & (p_classical < .20 | p_classical > .90),
      flag_pbis_low  = (!flag_min_n) & !is.na(pbis) & (pbis < .20),
      flag_any       = flag_pbis_low | flag_p_extreme   # you may drop p_extreme here if desired
    )
}

# 4) compute per-item and per-task diagnostics
diag_items <- resp_df %>%
  group_by(task) %>%
  group_modify(~ classical_stats(select(.x, user_id, item_id, response))) %>%
  ungroup() %>%
  arrange(task, item_id)

diag_summary <- diag_items %>%
  summarise(
    n_items     = dplyr::n_distinct(item_id),
    n_flag_minN = sum(flag_min_n,     na.rm = TRUE),
    n_flag_p    = sum(flag_p_extreme, na.rm = TRUE),
    n_flag_pbis = sum(flag_pbis_low,  na.rm = TRUE),
    n_any_flag  = sum(flag_any,       na.rm = TRUE),
    .by = task
  ) %>%
  mutate(
    prop_flag_p    = n_flag_p    / n_items,
    prop_flag_pbis = n_flag_pbis / n_items,
    prop_any_flag  = n_any_flag  / n_items
  ) %>%
  arrange(task)

# 5) detailed item table for internal use
diag_items_detailed <- diag_items %>%
  transmute(
    Task  = task,
    Item  = item_id,
    N     = n,
    p     = round(p_classical, 3),
    pbis  = ifelse(is.na(pbis), NA, round(pbis, 3)),
    flag_p     = !is.na(p)    & (p < .20 | p > .90),     
    flag_pbis  = !is.na(pbis) & (pbis < .20)
  ) %>%
  arrange(Task, Item)

# Save for QA (RDS + CSV)
saveRDS(diag_items_detailed, here::here("04_paper","display","diag_items_detailed.rds"))
readr::write_csv(diag_items_detailed, here::here("04_paper","display","diag_items_detailed.csv"))

```


```{r item-summary-table, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}

diag_summary_out <- diag_summary %>%
  transmute(
    Task       = task,
    `# Items`  = n_items,
    `Extreme p`    = sprintf("%d (%.0f%%)", n_flag_p,    100 * n_flag_p    / n_items),
    `Low pbis`     = sprintf("%d (%.0f%%)", n_flag_pbis, 100 * n_flag_pbis / n_items),
    `Any flag`     = sprintf("%d (%.0f%%)", n_any_flag,  100 * n_any_flag  / n_items)
  )

# Add a proper Total row (direct sums + weighted %)
totals <- diag_summary %>%
  summarise(
    Task        = "Total",
    `# Items`   = sum(n_items, na.rm = TRUE),
    n_flag_p    = sum(n_flag_p,    na.rm = TRUE),
    n_flag_pbis = sum(n_flag_pbis, na.rm = TRUE),
    n_any_flag  = sum(n_any_flag,  na.rm = TRUE)
  )

totals_fmt <- totals %>%
  mutate(
    `Extreme p` = sprintf("%d (%.0f%%)", n_flag_p,    100 * n_flag_p    / `# Items`),
    `Low pbis`  = sprintf("%d (%.0f%%)", n_flag_pbis, 100 * n_flag_pbis / `# Items`),
    `Any flag`  = sprintf("%d (%.0f%%)", n_any_flag,  100 * n_any_flag  / `# Items`)
  ) %>%
  select(Task, `# Items`, `Extreme p`, `Low pbis`, `Any flag`)

diag_summary_out <- bind_rows(diag_summary_out, totals_fmt)

apa_table(
  diag_summary_out,
  align   = c("l","c","c","c","c"),
  escape  = TRUE,
  caption = "Item-level diagnostics by task. Extreme p: items with very low/high proportion correct (< .20 or > .90). Low pbis: items with point–biserial correlations < .20. Values are n (percentage of items in task).",
  label   = "item-diagnostics-summary"
)
```
In addition to overall model calibration, we examined item-level performance to identify potential weaknesses in the pilot instruments. Classical item statistics included the proportion correct and point-biserial item–total correlations. Items with very low or very high proportions correct (below .20 or above .90) were flagged as potentially uninformative, and items with point-biserial correlations below .20 were flagged as weakly discriminating. Complementary IRT-based diagnostics included inspection of item infit and outfit statistics, although these results are not displayed here. Together, these indices provide a first pass at identifying problematic items that may warrant revision, removal, or further testing in subsequent piloting rounds.

.... DORIA stopped here ....

Classical indices included proportion correct (flagged if <0.20 or >0.90) and point-biserial correlations (flagged if <0.20). 
IRT-based fit statistics included infit and outfit mean squares.
DIF
Removal and Revision of Problematic Items
Ability Estimation
CAT-Specific Scoring

Multigroup models and invariance
Model selection approach
1PL/2PL and different degrees of invariance
Item-level diagnostics
Removal of outlier items
Tests for item-level DIF in cases where we use multigroup scoring

# Psychometric properties of tasks

## Developmental change

```{r, fig.width = 8, fig.height = 8}
ggplot(coretask_scores, aes(x = age, y = metric_value)) +
    ggh4x::facet_nested_wrap(vars(task_category, task),
                             nest_line = element_line(), solo_line = TRUE,
                             axes = "x",
                             scales = "free_y") +
    geom_smooth(aes(group = site_label, color = site_label),
                method = "gam", formula = y ~ s(x, bs = "re")) +
    geom_point(aes(colour = site_label), alpha = 0.3) +
    scale_x_continuous(breaks = seq(6, 14, 2)) +
    .scale_colour_site() +
    guides(color = guide_legend(override.aes = list(fill = "white"))) +
    labs(x = "Age (years)", y = "Ability (IRT score)", colour = "Site") +
    theme(legend.position = "bottom")
```

## Reliability 

### Marginal reliability estimates

```{r}
task_rxx <- read_rds(here("02_scoring_outputs", "task_rxx.rds"))


task_rxx_table <- task_rxx |>
  rename(item_task = task) |>
  left_join(task_map) |>
  mutate(site_label = site |> fct_recode(!!!site_labels),
         itemtype = itemtype |> fct_recode("Rasch" = "rasch", "2PL" = "2pl"),
         invariance = invariance |> str_to_title()) |>
         # model_label = paste(itemtype, invariance) |> str_trim()) |>
  select(task, itemtype, invariance, site_label, rxx) |>
  pivot_wider(names_from = site_label, values_from = rxx) |>
  # mutate(across(everything(), \(v) v |> replace_na("–"))) |>
  arrange(task)

options(papaja.na_string = "–")
task_rxx_table |>
  rename(Task = task, Parameters = itemtype, Invariance = invariance) |>
  papaja::apa_table(col_spanners = list("Marginal reliability" = c(4, 6)))
options(papaja.na_string = "NA")
```

### Test-retest
<!-- > ### Test-retest [bd edit: can we put correlations in each panel?  ] -->


```{r, output="asis"}
de_retest_scores <- coretask_scores |> 
  filter(site == "pilot_leuphana_de") |>
  group_by(user_id, task) |>
  arrange(user_id, task, age) |>
  filter(n() > 1) 

retest_wide <- de_retest_scores |>
  group_by(user_id, task) |>
  mutate(has_retest = any(age_gap > .05), 
         mean_age = mean(age), 
         age_gap = max(age_gap)) |>
  filter(has_retest) |>
  ungroup() |>  
  pivot_wider(
    id_cols = c(user_id, task_category, task, mean_age, age_gap),
    names_from = run_number,
    values_from = metric_value,
    names_prefix = "run_"
  ) |>
  select(user_id, task_category, task, age = mean_age, age_gap, run_1, run_2)

trt <- retest_wide |>
  group_by(task) |>
  summarise(test_retest_r = cor(run_1, run_2, use = "complete.obs"), 
            n = n(), 
            age_gap = mean(age_gap)*12) |>
  rename(`Task` = task,
         `r` = test_retest_r, 
         `N` = n, 
         `Retest interval (months)` = age_gap)

papaja::apa_table(trt, digits = 2, caption = "Task-wise test-retest correlations computed for Germany data.")
```

```{r, fig.width = 8, fig.height = 8}
ggplot(retest_wide, aes(x = run_1, y = run_2, color = age)) +
  ggh4x::facet_nested_wrap(vars(task_category, task),
                         nest_line = element_line(), solo_line = TRUE,
                         axes = "x",
                         scales = "free_y") +
    # facet_wrap(vars(task)) +
  geom_point() +
  geom_smooth(aes(group = 1), color = "darkgrey", method = "lm") +
  scale_color_viridis(limits = c(5, 12)) +
  labs(x = "Score 1", y = "Score 2", color = "Age (years)") +
  theme(legend.position = "bottom")
```

## Validity 
### Construct validity

```{r prep-sem, include=FALSE, cache=TRUE}

# one row per user (metadata + task scores from first run)
meta <- coretask_firstrun_scores %>%
  group_by(user_id) %>%
  summarise(
    age        = min(age, na.rm = TRUE),
    site       = dplyr::first(na.omit(site)),
    site_label = dplyr::first(na.omit(site_label)),
    dataset    = dplyr::first(na.omit(dataset)),
    .groups = "drop"
  )

# wide task scores (first run only)
wide_scores <- coretask_firstrun_scores %>%
  select(user_id, item_task, metric_value) %>%
  distinct() %>%  # guard against accidental duplicates
  tidyr::pivot_wider(
    names_from  = item_task,
    values_from = metric_value
  ) %>%
  left_join(meta, by = "user_id") %>%
  relocate(user_id, site, site_label, dataset, age)

# keep only the core task columns + metadata
vars_core <- c("math","matrix","mrot","sds","hf","mg","trog","vocab","tom")
wide_scores <- wide_scores %>%
  select(user_id, site, site_label, dataset, age, any_of(vars_core))

# standardise
z <- function(x) (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
vars_core_std <- wide_scores %>%
  mutate(across(all_of(vars_core), ~ z(as.numeric(.x))))
str(vars_core_std[vars_core])

```

```{r cfa, main model, include=FALSE, cache=TRUE}
cfa_1 <- "
  reasoning =~ matrix + mrot
  ef        =~ hf + mg + sds
  language  =~ trog + vocab

  reasoning ~ age
  ef        ~ age
  language  ~ age
  math      ~ age
  tom       ~ age"

fit_1 <- cfa(cfa_1, data = vars_core_std,
             estimator = "MLR", std.lv = TRUE, missing = "fiml")

```

```{r fig-semplot-main}

semPlot::semPaths(
  fit_1, what="mod", whatLabels="std", style="lisrel", layout="tree2",
  rotation=3, intercepts=FALSE, residuals=FALSE, thresholds=FALSE,
  # bigger nodes
  sizeMan=6, sizeLat=6, sizeInt=5,
  sizeMan2=5, sizeLat2=5, sizeInt2=4,
  # bigger text
  label.cex=1.2, edge.label.cex=1, edge.label.bg=TRUE,
  nDigits=1, nCharNodes=0,
  mar=c(6,6,6,6)
)
```

factors are very highly correlated - try with g factor

```{r second fit, echo = FALSE, cache = TRUE}
model_g_no_age <- "
  # first order factors
  reasoning =~ matrix + mrot
  ef        =~ hf + mg + sds + math
  language  =~ trog + vocab
  
  # second order g
  g =~ reasoning + ef + language

  # observed outcomes
  math ~ g
  tom ~ g"

fit_g <- cfa(model_g_no_age, data = vars_core_std,
             estimator = "MLR", std.lv = TRUE, missing = "fiml")
print(summary(fit_g, fit.measures = TRUE, standardized = TRUE))
head(lavaan::modindices(fit_g, sort.=TRUE, minimum.value=10), 15)
```

```{r add age, echo = FALSE, cache = TRUE}
model_g_age <- paste0(model_g_no_age, "\n g ~ age")
fit_g_age <- sem(model_g_age, data = vars_core_std,
             estimator = "MLR", std.lv = TRUE, missing = "fiml")

print(summary(fit_g_age, fit.measures = TRUE, standardized = TRUE))
head(lavaan::modindices(fit_g_age, sort.=TRUE, minimum.value=10), 15)

```

```{r fig-semplot-g}

semPlot::semPaths(
  fit_g_age,
  whatLabels = "std",
  residuals  = FALSE,
  intercepts = FALSE,
  thresholds = FALSE,
  layout     = "tree2",
  # text sizes
  label.cex       = 1.2,   
  edge.label.cex  = 1,   
  sizeMan = 6,
  sizeLat = 10,
  nCharNodes = 0,
  nDigits=2,
  mar = c(6,6,6,6)       
)
```

### External validity

Executive Function

```{r prep for sem - ef, echo = FALSE, cache = TRUE}

ef_tasks <- c("hf","mg","sds","mefs")

# Filter to ef tasks (from included_scores), keep first run 
ef_firstrun_scores <- included_scores %>%
  filter(item_task %in% ef_tasks) %>%
  arrange(user_id, item_task, run_number, time_started) %>%
  group_by(user_id, item_task) %>%
  slice(1) %>%
  ungroup()

# one row per user
meta_ef <- ef_firstrun_scores %>%
  group_by(user_id) %>%
  summarise(
    age        = min(age, na.rm = TRUE),
    site       = dplyr::first(na.omit(site)),
    site_label = dplyr::first(na.omit(site_label)),
    dataset    = dplyr::first(na.omit(dataset)),
    .groups = "drop"
  )

# Wide ef task scores (first run only)
ef_wide_scores <- ef_firstrun_scores %>%
  select(user_id, item_task, metric_value) %>%
  distinct() %>%  # guard against accidental duplicates
  pivot_wider(
    names_from  = item_task,
    values_from = metric_value
  ) %>%
  left_join(meta_ef, by = "user_id") %>%
  relocate(user_id, site, site_label, dataset, age)

# keep only ef columns + metadata
vars_ef <- ef_tasks
ef_wide_scores <- ef_wide_scores %>%
  select(user_id, site, site_label, dataset, age, any_of(vars_ef))

# Standardise ef indicators (same z() you used)
z <- function(x) (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
ef_wide_std <- ef_wide_scores %>%
  mutate(across(all_of(vars_ef), ~ z(as.numeric(.x))))

# str(ef_wide_std[vars_ef])
```

```{r sem fit, echo = FALSE, cache = TRUE}
cfa_ef <-  '
executive_function =~ hf + mg + sds + mefs
executive_function ~ age
'

fit_ef <- cfa(cfa_ef, data = ef_wide_std,
              estimator = "MLR", std.lv = TRUE, missing = "fiml")
# summary(fit_ef, fit.measures = TRUE, standardized = TRUE)
# print(fitMeasures(fit_ef, c("cfi","tli","rmsea","srmr")))

# plot
layout_ef <- matrix(
  byrow = TRUE, nrow = 3,
  data = c(
    "hf","mg","sds","mefs", NA,  "age",
     NA,  NA,   NA,   NA,   NA,  NA,
     NA,  NA, "executive_function", NA, NA, NA
  )
)
```

```{r fig-semplot-ef}

tidySEM::graph_sem(
  model = fit_ef,
  layout = layout_ef,
  text_size = 3,
  edge_label = "std"
)
```

Language

```{r prep for sem, echo = FALSE, cache = TRUE}

lang_vars <- c("swr","sre","pa","vocab","trog")

# first run per user–task
lang_firstrun_scores <- included_scores %>%
  filter(item_task %in% lang_vars) %>%
  arrange(user_id, item_task, run_number, time_started) %>%
  group_by(user_id, item_task) %>%
  slice(1) %>%
  ungroup()

meta_lang <- lang_firstrun_scores %>%
  group_by(user_id) %>%
  summarise(
    age        = min(age, na.rm = TRUE),
    site       = dplyr::first(na.omit(site)),
    site_label = dplyr::first(na.omit(site_label)),
    dataset    = dplyr::first(na.omit(dataset)),
    .groups = "drop"
  )

# create wide df 
lang_wide_scores <- lang_firstrun_scores %>%
  select(user_id, item_task, metric_value) %>%
  distinct() %>%
  pivot_wider(
    names_from  = item_task,
    values_from = metric_value
  ) %>%
  left_join(meta_lang, by = "user_id") %>%
  relocate(user_id, site, site_label, dataset, age)

# keep only language columns + metadata
lang_wide_scores <- lang_wide_scores %>%
  select(user_id, site, site_label, dataset, age, any_of(lang_vars))

# standardise 
z <- function(x) (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
lang_wide_std <- lang_wide_scores %>%
  mutate(across(all_of(intersect(lang_vars, names(.))), ~ z(as.numeric(.x))))
```

```{r sem fit - lang, echo = FALSE, cache = TRUE}
# model
cfa_lang <-  '
Language =~ swr + sre + pa + trog + vocab
Language ~ age
'

fit_lang <- cfa(cfa_lang, data = lang_wide_std,
                estimator = "MLR", std.lv = TRUE, missing = "fiml")
# print(fitMeasures(fit_lang, c("cfi","tli","rmsea","srmr")))

layout_lang <- matrix(
  byrow = TRUE, nrow = 3,
  data = c(
    "swr","sre","pa","trog","vocab", NA,  "age",
    NA,    NA,   NA,  NA, NA, NA, NA,
    NA,   "Language", NA,  NA,    NA, NA,  NA
  )
)
```

```{r fig-semplot-lang}

tidySEM::graph_sem(
  model      = fit_lang,
  layout     = layout_lang,
  text_size  = 3,
  edge_label = "std",                 
  show       = c("paths","loadings","residuals")
)

```

# Adaptive task construction

<!-- [bd edit: are we planning to actually specify the models we're using somewhere? would clarify meaning here (and be generally helpful)] -->
Many of the LEVANTE tasks have been adapted and piloted as CATs (Computerized Adaptive Tests). To date, these include the Test For Reception of Grammar (TROG), Vocabulary, Shape Rotation, Matrix Reasoning, Same Difference Selection, and Math. These tasks maintain an ability score, theta, as an estimate of the participant’s skill level that is updated at the end of each trial. They then present participants with the item best suited to their estimated ability, which both improves test-taker experience and yields more information on participant skill per item, allowing for a shorter task with fewer items. 

<!-- [bd edit: i'd cut this bit about the 4 params esp given that we haven't really talked much about item params above in the general case] -->
 The CAT tasks use an adaptive algorithm made available by the jsCat JavaScript library [@ma2025], which offers an implementation of an Item Response Theory (IRT) model including up to 4 parameters: discrimination, a value representing the item’s informativeness in distinguishing high and low ability test-takers, guessing, the probability of selecting the correct response at random, upper asymptote, the maximum likelihood of a correct answer, and difficulty. The present LEVANTE CAT implementation varies difficulty and guessing for each item while holding both discrimination and upper asymptote constant at 1. Items are selected based on Maximum Fisher Information, and theta is updated according to a maximum likelihood estimator, with limits of -6 and 6. 

The LEVANTE CATs are configurable with respect to the initial value of theta and the conditions for ending the task. The starting theta is set at 0 for all CAT tasks currently in use, but can be lowered or raised according to the researcher’s prior expectation of participant ability, for example according to age. Current CAT implementations use stopping rules based on either time or number of items. TROG, Shape Rotation, and Vocabulary each have time limits currently set to 4 minutes, with Matrix Reasoning set to 6 minutes to allow for the increased time typically required to complete items in this task. Items in these tasks are presented together in a single block. Same Difference Selection and Math are each divided into three blocks presented sequentially, with per-block stopping based on number of items. These CATs select from the list of items specific to their current block and proceed to the next block once the target number of items is reached, maintaining one overarching ability estimate for the entire task. Same Difference Selection and Math have time limits of 6 minutes and 8 minutes, respectively. 



# Discussion

* Plans for internationalization
* Plans for downward extension
* Plans for updates



\newpage

# References

<!-- ::: {#refs custom-style="Bibliography"} -->
<!-- ::: -->
