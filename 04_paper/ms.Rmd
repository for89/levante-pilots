---
title             : "Creation and validation of the LEVANTE core tasks: Internationalized measures of learning and development for children ages 5-12 years"
shorttitle        : "LEVANTE"

author: 
  - name          : ""
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Building 420, 450 Jane Stanford Way, Stanford, CA 94305"
    email         : "mcfrank@stanford.edu"
    role: # Contributorship roles (e.g., CRediT, https://credit.niso.org/)
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Michael C. Frank"
    affiliation   : "1"
    role:
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
      - "Supervision"

affiliation:
  - id            : "1"
    institution   : "Stanford University"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

abstract: | 
  We present the Learning Variability Network Exchange (LEVANTE) core tasks, a set of nine short and engaging computer adaptive tasks designed to assess learning and development in children ages 5--12 years across a wide range of languages and cultures. Using a simple and uniform multi-alternative forced choice format, these tasks measure constructs including math, execxutive function, reasoning, and social cognition and can be administered on a tablet or computer both in person or remotely. We describe the design and selection of these instruments, and then report on their reliability and validity in a pilot sample of XYZ children recruited in Colombia, Germany, and Canada. Tasks are scored using item response theory models. These models can be used to create computer adaptive versions of the tasks, allowing the entire battery to be given in under an hour. We discuss the use and extension of these tasks in the service of creating an open dataset to describe variability in children's development and learning across contexts. 


keywords: "cognitive development"
wordcount: "X"

bibliography: "library.bib"

floatsintext: no
linenumbers: yes
draft: no
mask: no

figurelist: no
tablelist: no
footnotelist: no

classoption: "man"
output: papaja::apa6_pdf
---


```{r, include = FALSE}
# authors
# Kachergis*, O’Reilly* 
# 
# Major contributors:
# Mika
# Doria
# Mike
# Nilàm
# 
# Other authors:
# Rebecca 
# Kat
# Amy
# Zach
# Lijin
# Ben
# 
# Pilot data contributors
# DE: Luise & Manuel
# CO: Camila & Julian
# CA: Nichola & Daniel 
# 
# Task contributors
# Jason, Tonya (TROG), Anya (VV) 
# Michael Sulik, Jelena Obradovic (EF)

```


```{r setup, include = FALSE}
library(here)
library(papaja)
library(tidyverse)
library(glue)
library(viridis)
source(here("03_summaries","plotting_helper.R"))
source(here("plot_settings.R"))
source(here("03_summaries","scores_helper.R"))

sites <- c("Colombia", "Germany", "Canada") #, "pilot_langcog_us", 
site_pal <- solarized_pal()(length(sites)) |> rlang::set_names(sites)
# r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE)
```

# Introduction

<!-- Variability is a common denominator in the study of learning and development, but currently dominant methods tend to favor group averages, short snapshots of time, and single environments. Studying variability could facilitate our understanding of why variability might be especially prevalent in specific groups or contextual circumstances, however, and researchers also must investigate individual variability to promote more effective customized learning interventions. Yet studying variability requires large datasets and diverse samples.  -->

<!-- We introduce the Learning Variability Network Exchange (LEVANTE), a framework for collecting and sharing developmental data with the aim of understanding the nature of developmental variability. LEVANTE aims to provide researchers worldwide with a framework for examining these topics that includes a set of core constructs and measures, while also allowing for some flexibility in data collection procedures. Our goal is to gather a large, rich, multi-context dataset with global scope that measures developmental change over time in children ages 2 – 12 years. LEVANTE embraces open science values, aiming to create measures and data that are permissively licensed and reusable by the international research community.  -->

<!-- LEVANTE includes direct assessments of children's learning and cognition. These aspects of individual children will be examined in light of contextual constructs at the level of the self, the home and family, the school, and the community, measured through parent report and teacher report.  -->

To create our direct assessment suite, we selected pre-existing measures from the literature that tapped each of these constructs, when possible prioritizing measures with strong psychometric properties, previous use across a broad range of cultures, applicability across a broad range of ages, and lack of commercial or licensing constraints. This process yielded a series of measures in an open source web platform, shown in Table 1. 

```{r, output="asis"}
tasks <- tribble(~`Construct`, ~`LEVANTE name`, ~`Prior names / Source task name`, ~Reference,
                 "Executive Function", "Hearts and Flowers", "Hearts and Flowers", "XYZ",
                 "", "Memory", "Corsi Block Task", "",
                 "", "Same and Different", "Same Different Selection Task", "",
                 "Language", "Vocabulary", "Picture Vocabulary", "",
                 "", "Sentence Understanding", "Test for Reception of Grammar (TROG)", "",
                 "Math", "Math", "Early Grades Math Assessment (EGMA)", "",
                 "Reasoning", "Pattern Matching", "Matrix Reasoning", "",
                 "", "Shape Rotation", "Mental Rotation", "",
                 "Social Cognition", "Stories", "Theory of Mind", "")

papaja::apa_table(tasks, caption = "The LEVANTE core tasks, presented with their internal label as well as prior labels used in the literature.")
```


Thus far, data has been collected on these tasks with over 500 students 5-12 years of age in Bogota, Colombia; Leipzig, Germany; and Ottawa, Canada. The psychometric properties and factor structure of the tasks will be discussed, as well as criteria for revising the item banks. Finally, we will discuss the process by which each task is made adaptive using item response theory models and computer adaptive testing techniques. 

LEVANTE will yield a large, open access longitudinal dataset for long-term research use, as well as a set of psychometrically normed, open access measures. These tools will not only facilitate the science of learning variability, but also constitute the backbone of a larger multidisciplinary research network working toward improving children's learning and development globally.

<!-- Ensure to back-reference construct selection from prior LEVANTE paper -->

# Task selection

## Language 

### Sentence Understanding

The Test for Reception of Grammar (TROG; Bishop, 1982) is a multiple-choice measure of receptive grammatical understanding. On each trial, the child hears a spoken sentence and is asked to select one of four pictures that best matches its meaning. The test comprises 20 blocks, each containing four items that assess the same grammatical structure. The blocks increase in grammatical complexity as the test progresses. A block is considered passed only if the child answers all four items correctly, making the task sensitive to consistent understanding rather than chance performance.

### Vocabulary


## Math


## Reasoning

### Matrix Reasoning

### Shape Rotation


## Executive Function

### Same Difference Selection

The Same Different Selection task (Obradovi\'c \& Sulik, year?) is designed to assess cognitive flexibility in children. It draws upon elements from the ‘Something's the Same’ task (Willoughby et al., 2012) and the ‘Flexible Item Selection Task’ (Jacques \& Zelazo, 2001). In this task, children are presented with sets of items that vary along multiple dimensions, such as shape, color, size, and number. They are required to identify similarities and differences between items based on these dimensions, thereby engaging their ability to shift attention and adapt to changing rules or criteria.

### Hearts and Flowers

The Hearts and Flowers task (Davidson et al., 2006) assesses inhibitory control and cognitive flexibility in individuals aged 3.5 years to adulthood. Participants respond according to stimulus type: pressing a key on the same side as a heart (congruent rule), and on the opposite side for a flower (incongruent rule). The task includes three blocks - congruent (hearts only), incongruent (flowers only), and mixed (hearts and flowers). The congruent block serves as a baseline with minimal executive demands. Inhibitory control is typically measured via performance on the incongruent block, which requires overriding a spatially dominant response, while cognitive flexibility is assessed using the mixed block, which demands switching between rules based on the stimulus (Wright \& Diamond, 2014).

### Memory Game

The Corsi Block task is a widely used measure of visuospatial short-term memory [@corsi1972]. In the standard version, a set of four blocks is arranged in a fixed spatial configuration. During each trial, a subset of blocks lights up one at a time in a specific sequence. The child is required to reproduce the sequence by clicking the blocks in the same order. The task begins with short sequences (e.g., two items) and gradually increases in difficulty (up to five or more) until the child fails two sequences of the same length. The longest correctly reproduced sequence reflects their visuospatial span. When adapted for younger children or digital administration, the number of visible blocks may be reduced (e.g., four blocks), and span lengths are typically capped at five to reduce task complexity.

## Social Cognition


### Stories task


# Translation and Adaptation

Need some discussion here of how translation was done and checked by sites


# Pilot Data Collection

```{r data}
mefs <- read_rds(here("02_scoring_outputs", "scores", "scores_mefs.rds"))

all_scores <- readRDS(here("02_scoring_outputs","scores","scores_combined.rds")) |>
  bind_rows(mefs) |>
  group_by(user_id, item_task) |>
  arrange(time_started) |>
  mutate(run_number = 1:n(),
         age_gap = age - age[1]) 

# weird tidyverse bug means this doesn't work in pipe chain
all_scores$site_label <- fct_recode(all_scores$site, 
                                    "Canada" = "pilot_western_ca",
                                    "Colombia" = "pilot_uniandes_co",
                                    "Germany" = "pilot_leuphana_de") 

included_scores <- all_scores |>
  filter(!is.na(age), age >= 5, age <= 12) |>
  filter(run_number == 1 | (run_number == 2 & age_gap > 1/6))

coretask_scores <- included_scores |>
  filter(item_task %in% core_tasks)

coretask_firstrun_scores <- coretask_scores |>
  filter(run_number==1)
```


```{r}
ages <- coretask_scores |>
  group_by(site, site_label, dataset, user_id) |>
  summarise(age = min(age), 
            n_runs = n())

site_labels <- coretask_scores |>
  pull(site_label) |>
  unique() 
```

```{r}
ggplot(ages, aes(x = age, fill = site)) + 
  geom_histogram(binwidth = 1) +
  scale_fill_solarized(guide = FALSE) +
  facet_wrap(~site_label) + 
  theme(legend.position = "bottom") +
  xlab("Age (years)") + 
  ylab("Number of children")
```


## Colombia

## Germany 

## Canada


# Scoring

## IRT Calibration and Model Selection
## Multigroup Calibration and Measurement Invariance 
<!-- Configural Invariance -->
<!-- Metric Invariance -->
<!-- Scalar Invariance -->


Model comparisons employed likelihood ratio test (?), changes in Bayesian Information Criterion ($\Delta$ BIC). 

## Item-Level Diagnostics

Classical indices included proportion correct (flagged if <0.20 or >0.90) and point-biserial correlations (flagged if <0.20). 
IRT-based fit statistics included infit and outfit mean squares.
DIF
Removal and Revision of Problematic Items
Ability Estimation
CAT-Specific Scoring

Multigroup models and invariance
Model selection approach
1PL/2PL and different degrees of invariance
Item-level diagnostics
Removal of outlier items
Tests for item-level DIF in cases where we use multigroup scoring

# Psychometric properties of tasks

## Developmental change

```{r}
ggplot(coretask_scores, aes(x = age, y = metric_value)) +
    ggh4x::facet_nested_wrap(vars(task_category, task),
                             nest_line = element_line(), solo_line = TRUE,
                             axes = "x",
                             scales = "free_y") +
    geom_smooth(aes(group = site_label, color = site_label),
                method = "gam", formula = y ~ s(x, bs = "re")) +
    geom_point(aes(colour = site_label), alpha = 0.3) +
    scale_x_continuous(breaks = seq(6, 14, 2)) +
    .scale_colour_site() +
    guides(color = guide_legend(override.aes = list(fill = "white"))) +
    labs(x = "Age (years)", y = "Ability (IRT score)", colour = "Site") +
    theme(legend.position = "bottom")

```

## Reliability 



### Marginal reliability estimate 



### Test-retest

```{r, output="asis"}
de_retest_scores <- coretask_scores |> 
  filter(site == "pilot_leuphana_de") |>
  group_by(user_id, task) |>
  arrange(user_id, task, age) |>
  filter(n() > 1) 

retest_wide <- de_retest_scores |>
  group_by(user_id, task) |>
  mutate(has_retest = any(age_gap > .05), 
         mean_age = mean(age), 
         age_gap = max(age_gap)) |>
  filter(has_retest) |>
  ungroup() |>  
  pivot_wider(
    id_cols = c(user_id, task, mean_age, age_gap),
    names_from = run_number,
    values_from = metric_value,
    names_prefix = "run_"
  ) |>
  select(user_id, task, age = mean_age, age_gap, run_1, run_2)

trt <- retest_wide |>
  group_by(task) |>
  summarise(test_retest_r = cor(run_1, run_2, use = "complete.obs"), 
            n = n(), 
            age_gap = mean(age_gap)*12) |>
  rename(`Task` = task,
         `r` = test_retest_r, 
         `N` = n, 
         `Retest interval (months)` = age_gap)

papaja::apa_table(trt, digits = 2, caption = "Task-wise test-retest correlations computed for Germany data.")
```

```{r}
ggplot(retest_wide, 
       aes(x = run_1, y = run_2, col = age)) +
  geom_point() + 
  geom_smooth(aes(group = 1), col = "blue", method = "lm") + 
  scale_color_viridis() + 
  facet_wrap(~task) +
  xlab("Score 1") + ylab("Score 2") 
```


```{r}
ggplot(retest_wide, 
       aes(x = run_1, y = run_2, col = age)) +
  geom_point() + 
  geom_smooth(aes(group = 1), col = "blue", method = "lm") + 
  scale_color_viridis() + 
  facet_wrap(~task) +
  xlab("Score 1") + ylab("Score 2") + 
  ggtitle("Test-retest reliability (DE)")
```

## Validity 

### Construct validity

SEMs within each construct for language, number, EF?


### External validity

We next assessed external validity by examining correlations with other measures. In particular, we used ...

MEFS and ROAR

# Adaptive task construction

Many of the LEVANTE tasks have been adapted and piloted as CATs (Computerized Adaptive Tests). To date, these include the Test For Reception of Grammar (TROG), Vocabulary, Mental Rotation, Matrix Reasoning, Same Difference Selection, and Math. These tasks maintain an ability score, theta, as an estimate of the participant’s skill level that is updated at the end of each trial. They then present participants with the item best suited to their estimated ability, which both improves test-taker experience and yields more information on participant skill per item, allowing for a shorter task with fewer items. 

The CAT tasks use an adaptive algorithm made available by the jsCat JavaScript library [@ma2025], which offers an implementation of an Item Response Theory (IRT) model including up to 4 parameters: discrimination, a value representing the item’s informativeness in distinguishing high and low ability test-takers, guessing, the probability of selecting the correct response at random, upper asymptote, the maximum likelihood of a correct answer, and difficulty. The present LEVANTE CAT implementation varies difficulty and guessing for each item while holding both discrimination and upper asymptote constant at 1. Items are selected based on Maximum Fisher Information, and theta is updated according to a maximum likelihood estimator, with limits of -6 and 6. 

The LEVANTE CATs are configurable with respect to the initial value of theta and the conditions for ending the task. The starting theta is set at 0 for all CAT tasks currently in use, but can be lowered or raised according to the researcher’s prior expectation of participant ability, for example according to age. Current CAT implementations use stopping rules based on either time or number of items. TROG, Mental Rotation, and Vocabulary each have time limits currently set to 4 minutes, with Matrix Reasoning set to 6 minutes to allow for the increased time typically required to complete items in this task. Items in these tasks are presented together in a single block. Same Difference Selection and Math are each divided into three blocks presented sequentially, with per-block stopping based on number of items. These CATs select from the list of items specific to their current block and proceed to the next block once the target number of items is reached, maintaining one overarching ability estimate for the entire task. Same Difference Selection and Math have time limits of 6 minutes and 8 minutes, respectively. 


```{r}
# insert durations table here

```


# Discussion

* Plans for internationalization
* Plans for downward extension
* Plans for updates



\newpage

# References

<!-- ::: {#refs custom-style="Bibliography"} -->
<!-- ::: -->
