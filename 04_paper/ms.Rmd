---
title             : "Creation and validation of the LEVANTE core tasks: Internationalized measures of learning and development for children ages 5-12 years"
shorttitle        : "LEVANTE"

author: 
  - name          : "George Kachergis*"
    affiliation   : "1"
  - name          : "Fionnuala O'Reilly*"
    affiliation   : "1"
  - name          : "Mika Braginsky"
    affiliation   : "1"
  - name          : "Amy Lightbody"
    affiliation   : "1"
  - name          : "Katherine Adams Shannon"
    affiliation   : "1"
  - name          : "Zachary Watson"
    affiliation   : "1"
  - name          : "Xingyao Xiao"
    affiliation   : "1"
  - name          : "Lijin Zhang"
    affiliation   : "1"
  - name          : "Rebecca Zhu"
    affiliation   : "1"
  - name          : "Wanjing Anya Ma"
    affiliation   : "1"
  - name          : "Tonya Murray"
    affiliation   : "1"
  - name          : "Bria Long"
    affiliation   : "2"
  - name          : "Jason Yeatman"
    affiliation   : "1"
  - name          : "Michael Sulik"
    affiliation   : "1"    
  - name          : "Jelena Obradović"
    affiliation   : "1"
  - name          : "Nichola Jenkins"
    affiliation   : "3"
  - name          : "Daniel Ansari"
    affiliation   : "3"
  - name          : "Maria Camilla Perfetti"
    affiliation   : "4"
  - name          : "Julian Mariño"
    affiliation   : "4"
  - name          : "Luise Hornoff"
    affiliation   : "5"  
  - name          : "Manuel Bohn"
    affiliation   : "6"  
  - name          : "Nilam Ram"
    affiliation   : "1"
  - name          : "Benjamin W. Domingue"
    affiliation   : "1"    
  - name          : "Michael C. Frank"
    affiliation   : "1"
    role:
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
      - "Supervision"

affiliation:
  - id            : "1"
    institution   : "Stanford University"
  - id            : "2"
    institution   : "University of California, San Diego"
  - id            : "3"
    institution   : "Western University"
  - id            : "4"
    institution   : "Universidad de los Andes"
  - id            : "5"
    institution   : "Max Planck Institute for Evolutionary Anthropology"
  - id            : "6"
    institution   : "Leuphana University"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

abstract: | 
  We present the Learning Variability Network Exchange (LEVANTE) core tasks, a set of nine short and engaging computer adaptive tasks designed to assess learning and development in children ages 5--12 years across a wide range of languages and cultures. Using a simple and uniform multi-alternative forced choice format, these tasks measure constructs including math, execxutive function, reasoning, and social cognition and can be administered on a tablet or computer both in person or remotely. We describe the design and selection of these tasks, and then report on their reliability and validity in a sample of XYZ children recruited in Colombia, Germany, and Canada. Tasks are scored using multi-group item response theory models, allowing testing for measurement invariance. The parameters of these models can then be used to create computer adaptive versions of the tasks, allowing the entire battery to be given in around an hour. We discuss the use,  ongoing refinement, and extension of these tasks in the service of creating an open dataset to describe variability in children's development and learning across contexts.


keywords: "cognitive development"
wordcount: "X"

bibliography: "library.bib"

floatsintext: no
linenumbers: yes
draft: no
mask: no

figurelist: no
tablelist: no
footnotelist: no

classoption: "man"
output:
  papaja::apa6_pdf:
    latex_engine: xelatex
    keep_tex: true
    includes:
      in_header: preamble.tex
---


```{r setup, include = FALSE}
library(here)
library(papaja)
library(tidyverse)
library(glue)
library(viridis)
library(lavaan)
library(mirt)
library(dplyr); library(tidyr); library(readr)
source(here("03_summaries","plotting_helper.R"))
source(here("plot_settings.R"))
source(here("03_summaries","scores_helper.R"))

sites <- c("Colombia", "Germany", "Canada") #, "pilot_langcog_us", 
site_pal <- solarized_pal()(length(sites)) |> rlang::set_names(sites)
# r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE)
knitr::opts_chunk$set(error = TRUE)

```

# Introduction

Developmental variability and change during childhood is a focus of intense theoretical and practical interest. From tracking children's growth over time to evaluating intervention outcomes and exploring environmental and contextual moderators, a wide range of scientific goals require accurate assessments. Ideal psychological measures provide efficient, reliable, and valid measures of particular constructs that can be applied across a range of ages, situations, and contexts. Yet, in most cases, a large gap separates the situation of a researcher searching for measures from this ideal. 

Because children's overall capacities are so dependent on their age, it can be very challenging to use the same measure across children of different ages. Young children require simple tasks that are not verbally demanding, while older children can answer more complicated questions. In addition, younger children typically require shorter tasks, often reducing measurement reliability. Yet giving different tasks to different ages can mean that scores are not comparable to one another, making tracking developmental growth challenging in many domains.  

A second set of challenges concern cross-context comparisons.  Ideal developmental measures should be validated in a global context and applicable to children across many cultures and languages. Child development is an issue of global importance [@grantham2007developmental;@masten2014global;@aboud2015global], yet the vast majority of developmental measures are developed in very specific (often English-speaking) contexts [@kidd2022diverse;@singh2023diversity;@richter2019early]. Providing cross-culturally validated measures allows for the collection of comparable data across contexts, opening up opportunities for theoretical synthesis.

A final set of challenges has to do with accessibility. Many gold-standard measures are commercially distributed. They are costly for researchers to use, and in addition, publishers may place barriers on new translation and adaptation. Publishers also typically hold both item information and normative data closely, blocking many types of secondary investigation. 

## LEVANTE

The Learning Variability Network Exchange (LEVANTE) was designed to address these challenges [@frank2025]. LEVANTE provides a software framework for data collection: researchers can use the LEVANTE dashboard to assign both surveys and tasks to children, caregivers, and teachers. Data collected via the LEVANTE dashboard are harmonized and validated and become accessible through a data repository, first to the researchers who collect them and then, through regular data releases, to the broader research community. Through a partnership with the Jacobs Foundation, sites around the world are funded to collect longitudinal data from children using the LEVANTE framework. The eventual goal of LEVANTE is to create a rich dataset documenting children's learning and development across contexts. 

The current manuscript introduces the LEVANTE core tasks, a suite of behavioral measures for children. In our initial development of the task battery, we cast a broad net for important constructs in child development with well-accepted measures that had been used internationally. This process is described in @frank2025. The broad constructs that we selected were executive function, language, mathematics, reasoning, and social cognition, with these being instantiated through a number of well-accepted tasks. 

To create our core tasks, we selected pre-existing measures from the literature that tapped each of these constructs. When possible, we prioritized measures with strong psychometric properties, previous use across a broad range of cultures, applicability across a broad range of ages, and lack of commercial or licensing constraints. This process yielded a series of candidate tasks, which we adapted and re-implemented in an open source web platform. Table \@ref(tab:tasks) shows these tasks, organized by construct. Some of these implementations remained quite close to prior versions, while others by necessity required the creation of additional test items or changes to specifics of their procedure. 

In addition to the constructs described above, we were also interested in the assessment of literacy. The LEVANTE core tasks battery makes use of a number of previously-validated literacy tasks from the Rapid Online Assessment of Reading [@yeatman2021] , including single word reading, sentence reading efficiency, and phonological awareness. We do not report on these tasks extensively here, though we make use of literacy data for validation of language measures. Similarly, we included a commercially-available broad measure of executive function, the Minnesota Executive Function Scale (MEFS) [@mefs], for validation purposes. 
 

```{r tasks, output="asis"}
tasks <- tribble(~`Construct`, ~`LEVANTE name`, ~`Prior names / Source task name`, ~`Adaptive?`, ~Reference,
                 "Executive Function", "Hearts and Flowers", "Hearts and Flowers", "", "XYZ",
                 "", "Memory", "Corsi Block Task", "X", "",
                 "", "Same and Different", "Same Different Selection Task", "X", "",
                 "Language", "Vocabulary", "Picture Vocabulary", "X", "",
                 "", "Sentence Understanding", "Test for Reception of Grammar (TROG)", "X","",
                 "Math", "Math", "Early Grades Math Assessment (EGMA)", "X", "",
                 "Reasoning", "Pattern Matching", "Matrix Reasoning", "X", "",
                 "", "Shape Rotation", "Mental Rotation", "X", "",
                 "Social Cognition", "Stories", "Theory of Mind", "X", "")

papaja::apa_table(
  tasks, landscape = TRUE,
  caption = "The LEVANTE core tasks, presented with their internal label as well as prior labels used in the literature.")
```

## The current study

Here we report on the development and validation of the LEVANTE core tasks. This is an iterative process in which data from 5--12 year old children has been collected across three sites: Bogota, Colombia; Leipzig, Germany; and Ottawa, Canada. In some cases that we note below, we used these data during the data collection process to make minor changes to the tasks. We use data from these three pilot sites both to provide initial evidence on the reliability and validity of the measures and to develop efficient, computer adaptive (CAT) versions of nearly all of the tasks. 

A key component of this process is the use of psychometric models based on item-response theory (IRT) [@embretson2001]. IRT models provide a family of models that allow the joint estimation of the difficulty of individual task items (e.g., math questions) and the ability of individual children. A fitted IRT model provides task parameters that can be used to estimate the ability of a new test taker given their responses on some or all of the same items. In addition, IRT parameters are used in the construction of CATs, which choose the most relevant items to give to estimate the ability of a particular individual. Critically for our purposes, the use of IRT models means that we can provide comparable scores on the same scale to a younger child who saw mostly easier task items and an older child who saw harder items; these models thus allow us to address our first key challenge posed above.  

Because our data come from three sites, each with their own translations and adaptations of the specific tasks, we can also use multi-group IRT models to explore the question of invariance: whether measures function similarly across different groups [@bornstein2016]. While measurement invariance is more commonly discussed in the factor analytic literature, it is also applicable to IRT (where it is sometimes analyzed at the level of individual test items as "differential item function" across groups) [@thissen2024]. Here we use multigroup model comparisons (described below) to investigate whether our tasks measure similarly structured constructs across groups. In particular, where possible, we aim for *scalar invariance*, in which individuals from different groups still show the same relative ordering of difficulty across items (e.g., they still find fractions items harder than division items in a math test). In some cases, we may fall back to *metric invariance*, in which items show different difficulties across groups, or *configural invariance*, in which items show different degrees of ability discrimination as well (e.g., if some problem types are unfamiliar to children in one group and so do not discriminate between high and low ability children). These models allow us to begin to address the second challenge posed above.

<!-- https://www.tandfonline.com/doi/10.1080/00273171.2024.2396148 -->

In what follows, we begin by describing the nine LEVANTE core tasks, organized by construct. We then discuss the process of translation and adaptation that produced the Spanish and German versions of these tasks from the original English source. We then discuss our pilot data collection efforts in Colombia, Germany, and Canada. We present our IRT-based scoring techniques and the results of multi-group comparison, in some cases after selectively removing items that show poor performance. Using scores from these analyses, we then present evidence on the reliability and validity of the tasks, recognizing that in many cases these tasks are still under construction and we anticipate increases in reliability as we iteratively improve items. 

We end by discussing future plans for further internationalization and downward extension of the tasks. Critically, LEVANTE embraces open science values, aiming to create measures and data that are permissively licensed and reusable and extensible by the international research community. These values address our final challenge posed above: The aim of LEVANTE is to minimize barriers to reuse, accelerating progress towards a global science of learning and development. 

# The LEVANTE core tasks

<<<<<<< HEAD
## General Task Properties

=======
>>>>>>> bfcebce (updates to dev change section)
```{r}
task_time_summary <- read_csv(here("03_summaries/tables/task_time_summary.csv"))
site_labels <- c("Canada" = "pilot_western_ca",
                 "Colombia" = "pilot_uniandes_co",
                 "Germany" = "pilot_leuphana_de")
all_scores <- readRDS(here("02_scoring_outputs","scores","scores_combined.rds")) |>
  # bind_rows(mefs) |>
  group_by(user_id, item_task) |>
  arrange(time_started) |>
  mutate(run_number = 1:n(),
         age_gap = age - age[1]) |>
  ungroup() |>
  mutate(site_label = site |> fct_recode(!!!site_labels)) |>
  mutate(task = task |> str_to_title() |> fct_inorder(),
         task_category = task_category |> str_to_title() |> fct_inorder())

task_map <- all_scores |> distinct(item_task, task)

task_time_table <- task_time_summary |>
  filter(!is.na(is_cat)) |>
  filter(item_task %in% core_tasks) |>
  mutate(site_label = site |> fct_recode(!!!site_labels),
         adaptive = if_else(is_cat, "Adaptive", "Non-adaptive")) |>
  left_join(task_map) |>
  select(task, site_label, adaptive, median_diff) |>
  pivot_wider(names_from = c(adaptive, site_label), values_from = median_diff) |>
  select(task, starts_with("Non"), , starts_with("Adaptive")) |>
  arrange(task)

options(papaja.na_string = "–")
task_time_table |>
  papaja::apa_table(
    col.names = c("Task", rep(c("Germany", "Colombia", "Canada"), 2)),
    col_spanners = list("Non-adaptive" = c(2, 4), "Adaptive" = c(5, 7)),
    align = c("l", rep("r", 6)),
    caption = "Median task durations (minutes).")
options(papaja.na_string = "NA")
```


The LEVANTE core tasks are short behavioral tasks that can be presented in a web browser on a tablet or laptop, with responses possible using a touchscreen, keyboard, or mouse. Because of this variability in format of administration, they focus on response correctness not reaction time and so they are mostly untimed. The tasks are designed for simplicity and clarity so as to be accessible to children across a wide age range, and so with only modest exceptions, nearly all are in the format of a multi-alternative forced choice with a maximum of four choices. This uniformity of format means that in most cases instructions can be short and easy to understand, minimizing delays when the tasks are given in sequence as a battery. Figure \@ref{fig:tasks} shows screenshots from a number of tasks. Below we briefly present each task.

Tasks were implemented using jsPsych [@de-leeuw2015] in a common framework. Illustrations were produced by hand while auditory stimuli were generated using AI voice synthesis tools (currently, [elevenlabs.io]()). The tasks generally share a set of usability features including simple instructions, a small number of practice trials, and buttons to replay audio. All tasks are available for demonstration purposes at [http://researcher.levante-network.org](). Source code for the tasks is available at [https://github.com/levante-framework/core-tasks](). Task code and assets are licensed CC-BY-NC 4.0 for non-commercial reuse (including educational use by not-for-profit and governmental entities) with appropriate attribution. Please see repository license for more details.

General task lengths are described below and average task durations are given in Table \@ref(tab:durations). Because of our interest in testing children across a wide range of ages, we intentionally included trials that we anticipated would be both very easy and very hard for children with the aim of deploying these adaptively after we had gathered sufficient data. We wanted to avoid  frustration for children, however, so during pilot testing, we experimented with a number of different stopping rules. In early iterations of pilot testing in Bogota, Colombia, we ended tasks after three incorrect trials; we later modified this rule to end tasks after six incorrect trials. 

## Language 

### Sentence Understanding

The Test for Reception of Grammar (TROG) [@bishop1982] is a multiple-choice measure of receptive grammatical understanding. On each trial, the child hears a spoken sentence and is asked to select one of four pictures that best matches its meaning. The original test contained 20 blocks, each with four items assessing the same grammatical structure. In our adaptation (based on the original TROG, which was permissively licensed for reuse), we removed a small number of items due to changes in cultural norms. In addition, based on early pilot testing showing that many trials were easy for older children, we added a set of several dozen more challenging sentences. All illustrations were remade with details intended to be accessible across a broad range of cultures. The task had 103 separate items.

### Vocabulary

The Vocabulary task was developed as a non-commercial, open alternative to tasks such as the Peabody Picture Vocabulary [@peabody] and the NIH Toolbox Picture Vocabulary Task [@nihtoolbox]; see @long_ma_tan_silverman_frank_yeatman_2025 for more details. In this task, the child is presented with a word and four pictures. They must select the correct picture over the distractor picture. Targets and distractors were selected from the THINGS dataset [@hebart2019things], specifically from the permissively-licensed subset of the data. Each image had a semantically close and semantically far distractor as well as an unrelated distractor image (e.g., target word "acorn" with close distractor being a coconut, and the far and unrelated distractors being keys and laundry). The task included 170 total items. 

## Math

We developed the Math task based on the EGMA (Early Grade Mathematics Assessment) [@platas2014]. This short, paper-and-pencil assessment is widely administered in international contexts for children ages 5--8 and includes number identification, number comparison, missing number, addition, and subtraction sub-tests. In our adaptation, we increased the breadth of the initial item bank, added multiplication, division, and fractions items to extend the age range up. We also added number line identification problems in which children had to place a marker on a number line across a range of different scales (including simpler 0--10 trials and more challenging larger scales as well as a fraction scale). Number line problems of this are strongly related to math ability [@schneider2018]. The primary version of this task included 275 items (though no child saw all of these) and the secondary version for retest purposes included an additional 237. 

## Reasoning

### Pattern Matching

In classic matrix reasoning tasks, participants see a set of abstract figures in a grid format with a missing cell and must choose the most appropriate figure to complete the grid. Matrix reasoning scores are highly correlated with general cognitive ability and educational attainment [e.g., @roth2015]. Here we make use of the Mars-IB matrix reasoning stimuli, an open-source, permissively-licensed set of 80 matrix reasoning problems with retest variants that have been normed with a large sample of adolescents and adults [@chierchia2019].

### Shape Rotation

Mental rotation refers to the ability to imagine the rotation of objects in space, which is strongly related to mathematics and reasoning abilities [@xie2020]. In our version of the mental rotation task [based on @frick2013], children are presented with a target item (e.g., a picture of a rabbit). Then, they are presented with two choice items (e.g., two silhouettes of rabbits). One of the choice items will match the target item when rotated; the other choice item will not match the target item when rotated. The child must select the choice item that matches the target item. Typically, both accuracy and reaction time are measured. Across trials, the degree of rotation varies (e.g., 45, 90, 180 degrees), affecting children’s accuracy and reaction time. We included both two-dimensional duck and rabbit stimuli as well as harder three-dimensional geometric figures from the original stimulus set developed by @shepard1971.


## Executive Function

### Same and Different Selection

The Same and Different Selection task is designed to assess cognitive flexibility in children. It draws upon elements from the "Something's the Same" task [@willoughby2011] and the "Flexible Item Selection Task" [@jacques2001]. In this task, children are presented with sets of items that vary along multiple dimensions, such as shape, color, size, and number. They are required to identify similarities and differences between items based on these dimensions, engaging their ability to shift attention and adapt to changing rules or criteria. The version of the task we developed included successively more difficult blocks of trials in which children were asked to identify two, three, or four pairs of items that were the same in different ways (e.g., matching on different figures). 

### Hearts and Flowers

The Hearts and Flowers task assesses inhibitory control and cognitive flexibility [@davidson2006]. Participants respond according to stimulus type: pressing a key on the same side as a heart (congruent rule), and on the opposite side for a flower (incongruent rule). The task includes three blocks: congruent (hearts only), incongruent (flowers only), and mixed (hearts and flowers). The congruent block serves as a baseline with minimal executive demands. 

### Memory Game

The Corsi Block task is a widely-used measure of visuospatial short-term and working memory [@corsi1972]. In our version of the task, the child sees a grid of squares; during each trial, a subset of squares lights up one at a time in a specific sequence. The child is required to reproduce the sequence by touching the squares: in the first block, the sequence is reproduced in the same order and in the second block, it is reproduced backwards. The task begins with short sequences (e.g., two items) and gradually increases in difficulty (up to seven items) until the child fails two sequences of the same length. We used both 2x2 or 3x3 grids in our pilot testing, with relatively similar results.

## Social Cognition

### Stories task

<<<<<<< HEAD
Assessing social cognition across a wide range of ages is a challenge, and relatively few instruments show good reliability and validity across not just early childhood but older children as well [cf. @heise2025]. For this purpose, we adapted and re-illustrated a storybook task developed by @sotomayor-enriquez2024 in which children hear stories with multiple social reasoning questions embedded, including questions about true and false beliefs, deception, and moral reasoning. Using the published data from this task, we selected a set of items that showed good psychometric properties and that we believed would be most likely to be valid cross-culturally and then supplemented these with additional trials tapping emotional reasoning. The result was a set of six stories, each with approximately five forced-choice social reasoning questions embedded within it. We then developed two sets of alternative re-test stories, in each case using the same structure as an existing story but substituting new details and characters. 

=======
>>>>>>> bfcebce (updates to dev change section)
# Translation and Adaptation

Initially designed in English, core tasks were subsequently adapted for Spanish and German following a set of internationalization procedures in collaboration with researchers at the sites collecting pilot data in Colombia and Germany. We began with AI-generated translations of all verbal materials, including task instructions, items, and other key terminology such as encouragement phrases that appear throughout the core tasks. A professional translator then reviewed and edited the AI translations for general grammatical correctness and appropriateness. Materials were next reviewed by a researcher who was a native speaker of the relevant language and dialect, confirming that the materials were clear and appropriate for the specific task. We then performed back translation using AI translation tools to evaluate which items needed further discussion to ensure cultural relevance and task integrity. For tasks evaluating language skills (Vocabulary and Sentence Understanding), we additionally recruited PhD-level linguists with expertise in language acquisition for both Spanish and German to review items and make recommendations about relevant changes or additions to the item set for that language.

# Pilot Data Collection

The pilot site partnership was an integral component of the core task development. Across three countries and languages, pilot sites provided collaboration on task adaptation and functionality, iterative task testing, and diverse settings for infrastructure deployment. Data collected from pilot sites allowed analyses of construct validity, measurement invariance, test-retest reliability, parameters for computer adaptive testing, and a demonstration of developmental growth measured by the tasks across constructs. Because we continued developing tasks continuously during the ~18 month pilot process, we anticipate that our reported reliability and validity estimates are likely an underestimate of the current task performance. Our analyses average over early versions of the tasks that in some cases had instructions or items that were later revised for clarity. 

One key feature of the LEVANTE framework is that all data are completely de-identified, reducing legal and ethical obstacles to data sharing. No demographic information or other identifiers are entered into the LEVANTE dashboard by the sites, and ages are only given to a one month precision [@frank2025]. We anticipate that sites will add sociodemographic information into the dataset at a later time via an app that is currently under development that ensures that there is no statistical reidentification risk for  individuals (e.g., due to rare combinations of traits in a community). For this reason, here we provide only minimal demographic characterization and analysis of the children from our pilot sites.  

```{r data}
# mefs <- read_rds(here("02_scoring_outputs", "scores", "scores_mefs.rds"))

included_scores <- all_scores |>
  filter(!is.na(age), age >= 5, age <= 12) |>
  filter(run_number == 1 | (run_number == 2 & age_gap > 1/6))

coretask_scores <- included_scores |>
  filter(item_task %in% core_tasks)

coretask_firstrun_scores <- coretask_scores |>
  filter(run_number == 1)
```

```{r}
ages <- coretask_scores |>
  group_by(site, site_label, dataset, user_id) |>
  summarise(age = min(age), 
            n_runs = n())
```

```{r, fig.height = 3}
ggplot(ages, aes(x = age, fill = site)) + 
  facet_wrap(vars(site_label)) + 
  geom_histogram(binwidth = 1, color = "white") +
  scale_fill_solarized(guide = "none") +
  scale_y_continuous(expand = expansion(0, 0)) +
  labs(x = "Age (years)", y = "Number of children")
```

```{r}
source("compute_counts.R")
```

Across all tasks, the sites collected a total of `r sum(site_totals)` runs (instances of a child performing a task; `r get_count_row(1)`) from `r sum(user_totals$n_users)` unique children (`r user_totals |> deframe() |> collapse_value_list()`), of which some were excluded for being incomplete, invalid, or duplicate (`r get_count_row(2)`), for missing item metadata (`r get_count_row(3)`), for missing age (`r get_count_row(4)`), or for age being out of range or having too short of a retest gap (`r get_count_row(5)`), resulting in a total of `r sum(user_score_counts$n_runs)` scored runs (`r user_score_counts |> select(site, n_runs) |> deframe() |> collapse_value_list()`) from `r sum(user_score_counts$n_users)` unique children (`r user_score_counts |> select(site, n_users) |> deframe() |> collapse_value_list()`).

Within the included runs, there were a total of `r total_trial_counts$n_trials` trials (`r site_trial_counts |> select(site, n_trials) |> deframe() |> collapse_value_list()`), of which `r total_trial_counts$excluded` were excluded (`r site_trial_counts |> select(site, excluded) |> deframe() |> collapse_value_list()`) due to too fast or too slow reaction times. For reasons of space we do not disaggregate these exclusions by task, but note that the task with the highest exclusion rate was Pattern Matching (`r percentify(max(task_trial_counts$pct_excluded))` trials excluded), while all the other tasks ranged between `r percentify(min(task_trial_counts$pct_excluded))` and `r percentify(sort(task_trial_counts$pct_excluded, decreasing = TRUE)[2])` trials excluded.

## Colombia

The Universidad de Los Andes conducted school-based data collection across four schools in Bogotá and three schools in the rural areas of Caquetá and Boyacá, Colombia. By partnering with schools, the research team oversaw task data collection with children using a group testing format on tablets provided by the team. Children were tested in groups, with larger groups for older children and The Colombia site collected data from `r user_totals$n_users[user_totals$site == "Colombia"]` children ages 5-12 years.

## Germany

The Max Planck Institute for Evolutionary Anthropology collected data using family-based remote data collection. Participants were recruited via an existing database by first contacting families via telephone with an invitation to participate in the study. Upon agreement, families received an email with information to log into the LEVANTE system and complete the tasks assigned to them at home using their own computer or tablet. Germany provided retest data with follow-up assignments sent to families 3-6 months after initial completion of the tasks. The Germany site collected data from `r user_totals$n_users[user_totals$site == "Germany"]` children, ages 5-12 years, with XX children completing the retest assignments. 

## Canada

Partners at Western University, led by Daniel Ansari, recruited participants from the local community and tested children individually in a clinic setting. The Canadian site collected data from `r user_totals$n_users[user_totals$site == "Canada"]` children, ages 5-12 years. 

# Scoring

## IRT Calibration and Model Selection

```{r model-selection, echo=FALSE, results='asis', message=FALSE, warning=FALSE}

# -- Ensure scores are available  --
sco_path <- here::here("02_scoring_outputs","scores","scores_combined.rds")
all_scores <- readRDS(sco_path)

# -- Load marginal reliability summary if present (used as fallback for selection) --
task_rxx_path <- here::here("02_scoring_outputs","task_rxx.rds")
task_rxx <-readRDS(task_rxx_path)
# -- Site labels should be defined earlier; if not, define a minimal default --
if (!exists("site_labels")) {
  site_labels <- c("pilot_western_ca" = "Canada",
                   "pilot_uniandes_co" = "Colombia",
                   "pilot_leuphana_de" = "Germany")
}
# -- Prefer explicit model selection export when available; otherwise infer from reliability --
sel_path <- here::here("04_paper","display","model_selection.csv")
  model_sel <- task_rxx |>
    mutate(site_label = recode(site, !!!site_labels),
           Model = case_match(itemtype,
                              "rasch" ~ "Rasch",
                              "2pl"   ~ "2PL",
                              .default = toupper(itemtype))) |>
    group_by(task, site_label) |>
    slice_max(order_by = rxx, n = 1, with_ties = FALSE) |>
    ungroup()

# -- Collapse to one model per task (most common across sites) --
per_task_model <- model_sel |>
  count(task, Model,invariance, name = "n_sites") |>
  group_by(task) |>
  slice_max(order_by = n_sites, n = 1, with_ties = FALSE) |>
  ungroup()
# -- Attach mean rxx, if available, for context --

  rxx_table <- task_rxx |>
    mutate(site_label = recode(site, !!!site_labels),
           Model = case_match(itemtype, "rasch" ~ "Rasch", "2pl" ~ "2PL", .default = toupper(itemtype))) |>
    select(task, Model, site_label, rxx) |>
    group_by(task, Model) |>
    summarize(`Mean reliability (rxx)` = mean(rxx, na.rm = TRUE), .groups = "drop")

  display_tbl <- per_task_model |>
    left_join(rxx_table, by = c("task","Model")) |>
    arrange(task)

# -- Render APA-style table --
papaja::apa_table(
  display_tbl, label = "model-selection",
  caption =
    "When a formal model-selection export was not available, the model with highest average rxx served as a descriptive proxy."
)
```

We calibrated each LEVANTE task using Rasch and 2PL models and selected the more constrained model unless fit indices provided strong evidence in favor of the 2PL ($\Delta$BIC $\leq$ −10). Across most tasks, the Rasch specification was adequate, indicating that item discriminations did not vary substantially. Two tasks -- Mental Rotation (rxx = .93) and Same–Different Selection (rxx = .96) -- were better described by the 2PL model, reflecting heterogeneity in how strongly items discriminated among children.

Estimates of marginal reliability highlighted meaningful differences across tasks (Table \@ref(tab:model-selection)). Math (.90), Matrix Reasoning (.82), and Memory Game (.93) all demonstrated high reliability, suggesting precise measurement even in the pilot samples. Vocabulary (.83) and Sentence Understanding/TROG (.73) achieved moderate reliability, consistent with tasks that are informative but may benefit from additional refinement. By contrast, Hearts and Flowers (.45) and Theory of Mind (.41) showed low reliability, underscoring that these implementations currently provide limited psychometric information and require further development before supporting valid cross-site comparisons.

In summary, Rasch calibration suffices for most tasks, while allowing variable discriminations improves model fit for more heterogeneous domains. Several tasks already demonstrate reliability suitable for adaptive testing, whereas those with lower reliabilities should undergo item revision and expansion in subsequent piloting rounds.

## Multigroup Calibration and Measurement Invariance 

```{r invariance-summary, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}

inv_path <- here::here("04_paper","display","invariance.csv")
stopifnot(file.exists(inv_path))

inv <- read_csv(inv_path, show_col_types = FALSE) |>
  mutate(across(everything(), ~ tidyr::replace_na(.x, "")))

# Pick a column to read the invariance level from, if available
lvl_candidates <- intersect(c("Level", "Invariance"), names(inv))

if (length(lvl_candidates) > 0) {
  # Use the first matching column for level extraction
  lvl_col <- lvl_candidates[1]
  inv_levels <- inv |>
    mutate(Level_norm_raw = tolower(.data[[lvl_col]]))
} else if ("Interpretation" %in% names(inv)) {
  # Fall back to Interpretation text
  inv_levels <- inv |>
    mutate(Level_norm_raw = tolower(.data[["Interpretation"]]))
} else {
  # If neither exists, mark as unspecified
  inv_levels <- inv |>
    mutate(Level_norm_raw = "unspecified")
}

inv_levels <- inv_levels |>
  mutate(Level_norm = case_when(
    str_detect(Level_norm_raw, "scalar")    ~ "Scalar",
    str_detect(Level_norm_raw, "metric")    ~ "Metric",
    str_detect(Level_norm_raw, "configur")  ~ "Configural",
    TRUE                                    ~ "Unspecified"
  ))

# Summary counts by invariance level
sum_counts <- inv_levels |>
  count(Level_norm, name = "Number of tasks") |>
  mutate(Level_norm = factor(Level_norm, levels = c("Scalar","Metric","Configural","Unspecified"))) |>
  arrange(Level_norm)

papaja::apa_table(
  sum_counts |>
    rename(`Invariance level` = Level_norm),
  caption = "Summary of multigroup invariance outcomes across LEVANTE tasks.",
  label = "invariance-summary"
)


```

```{r invariance-mapping, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
# Detailed mapping table 
papaja::apa_table(
  inv |>
    select(any_of(c("Goal", "LEVANTE name", "Factor analysis name",
                    "IRT explanation", "Interpretation"))),
  # align   = c("m{2.5cm}", "m{3cm}", "m{5cm}", "m{5cm}", "m{5cm}"),
  align = c("m{2cm}", "m{3cm}", "m{5cm}", "m{5cm}", "m{5cm}"),

  caption = "Measurement invariance mapping across factor analysis and IRT. Scalar invariance supports direct cross-site score comparability; metric and configural indicate progressively weaker comparability.",
  label = "invariance-mapping",
  landscape = TRUE
)
```

We next examined whether the LEVANTE tasks functioned similarly across Colombia, Germany, and Canada using multigroup IRT models. Starting with a configural model (all parameters free), we sequentially imposed equality constraints on discriminations (metric) and then on both discriminations and difficulties (scalar). Table \@ref(tab:invariance-mapping) aligns the factor-analytic and IRT definitions of configural, metric, and scalar invariance, clarifying what kinds of cross-site comparisons each level supports. Because residual variances are not separately estimated in binary IRT, strict invariance was not tested. Model selection relied on $\Delta$BIC, retaining the most constrained model that did not show meaningful loss of fit ($\Delta$BIC $\leq$ +10).

<<<<<<< HEAD
Table \@ref(tab:invariance-summary) summarizes outcomes. Math and Matrix Reasoning supported scalar invariance, indicating no evidence of DIF and suggesting that scores can be compared directly across sites, at least in these pilot samples. Vocabulary supported metric invariance: items discriminated similarly across sites but varied in difficulty, so associations are interpretable but mean-level comparisons require caution. Hearts and Flowers and Theory of Mind showed only configural invariance, implying site-specific item functioning and the need for further revision. For the remaining tasks (e.g., TROG, Memory Game), invariance could not be established given current sample sizes. Taken together, these findings suggest that some tasks are already suitable for international comparability, while others require additional development and piloting.

=======
>>>>>>> bfcebce (updates to dev change section)
## Item-Level Diagnostics

```{r, message=FALSE, warning=FALSE, include=FALSE, results='asis'}

regdir <- here::here("02_score_data/02_fit_irt/model_registry")
# Load helpers
source(here::here("02_score_data/02_fit_irt/irt_modular.R"), chdir = TRUE)
source(here::here("02_score_data/02_fit_irt/registry_helper.R"), chdir = TRUE)
# Sanity check: does the registry have files?
list.files(regdir, recursive = TRUE)
# read in all models in registry
mods <- list_models() |> load_models()
# extract model specifications from model records
mods_coded <- mods |>
  mutate(nfact = map_int(mod_rec, \(mr) mr@nfact),
         itemtype = map_chr(mod_rec, \(mr) mr@itemtype),
         invariance_terms = map(mod_rec, \(mr) mr@invariance),
         invariance = map_chr(invariance_terms, translate_invariance)) |>
  select(-filename, -path, -invariance_terms) |>
  mutate(mod = map(mod_rec, model_from_record))

```  

```{r include=FALSE}

# ---------- helpers ----------
pbis_corrected <- function(mat){
  pbs <- numeric(ncol(mat))
  for(j in seq_len(ncol(mat))){
    xj   <- mat[, j]
    totj <- rowSums(mat[, -j, drop = FALSE], na.rm = TRUE)
    pbs[j] <- suppressWarnings(cor(xj, totj, use = "pairwise.complete.obs"))
  }
  setNames(pbs, colnames(mat))
}

normalize_fit <- function(x){
  if (is.null(x)) return(tibble())
  df <- tryCatch(as.data.frame(x), error = function(e) NULL)
  if (is.null(df)) return(tibble())
  if (!"item" %in% names(df)) {
    rn <- rownames(df)
    df$item <- if (!is.null(rn)) rn else paste0("item_", seq_len(nrow(df)))
    rownames(df) <- NULL
  }
  as_tibble(df)
}

get_infit <- function(mod){
  out <- tryCatch(mirt::itemfit(mod, na.rm = TRUE, fit_stats = "infit"), error = function(e) NULL)
  out <- normalize_fit(out)
  if (!nrow(out)) out <- tibble(item = character())
  if (!"infit"   %in% names(out)) out$infit    <- NA_real_
  if (!"z.infit" %in% names(out)) out$`z.infit` <- NA_real_
  out %>% select(item, infit, `z.infit`)
}

get_sx2 <- function(mod){
  out <- tryCatch(mirt::itemfit(mod, na.rm = TRUE), error = function(e) NULL)
  out <- normalize_fit(out)
  if (!nrow(out)) out <- tibble(item = character())
  for (nm in c("S_X2","df.S_X2","RMSEA.S_X2","p.S_X2")) {
    if (!nm %in% names(out)) out[[nm]] <- NA_real_
  }
  out %>% select(item, S_X2, df.S_X2, RMSEA.S_X2, p.S_X2)
}

get_n_eff <- function(dat) sum(stats::complete.cases(dat))

# Model-level fit pulled from mirt object
get_model_fit <- function(mod){
  tibble(
    AIC    = tryCatch(mirt::extract.mirt(mod, "AIC"),    error = \(e) NA_real_),
    BIC    = tryCatch(mirt::extract.mirt(mod, "BIC"),    error = \(e) NA_real_),
    SABIC  = tryCatch(mirt::extract.mirt(mod, "SABIC"),  error = \(e) NA_real_),
    logLik = tryCatch(as.numeric(mirt::extract.mirt(mod, "logLik")),
                      error = \(e) NA_real_),
    npar   = tryCatch(as.numeric(mirt::extract.mirt(mod, "npar")),
                      error = \(e) NA_real_)
  )
}

<<<<<<< HEAD
# ---------- main per-row summarizer ----------
summarize_item_fit_with_meta <- function(task, model_set, subset, mod_rec, nfact, itemtype, invariance, mod){
  dat <- mod@Data$data

  # classical
  p_correct <- colMeans(dat, na.rm = TRUE)
  pbis      <- tryCatch(pbis_corrected(dat),
                        error = function(e) rep(NA_real_, ncol(dat)) |> setNames(colnames(dat)))
  classical <- tibble(
    item           = names(p_correct),
    p_correct      = unname(p_correct),
    point_biserial = unname(pbis)
=======
```{r item-summary-table, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}

diag_summary_out <- diag_summary %>%
  transmute(
    Task       = task,
    `# Items`  = n_items,
    `Extreme p`    = sprintf("%d (%.0f%%)", n_flag_p,    100 * n_flag_p    / n_items),
    `Low pbis`     = sprintf("%d (%.0f%%)", n_flag_pbis, 100 * n_flag_pbis / n_items),
    `Any flag`     = sprintf("%d (%.0f%%)", n_any_flag,  100 * n_any_flag  / n_items)
>>>>>>> bfcebce (updates to dev change section)
  )

  # IRT
  infit_tbl <- get_infit(mod)
  sx2_tbl   <- get_sx2(mod)

  # align to classical item set (prevents join errors)
  infit_tbl <- right_join(infit_tbl, classical %>% select(item), by = "item")
  sx2_tbl   <- right_join(sx2_tbl,   classical %>% select(item), by = "item")

  n_eff_task   <- get_n_eff(dat)
  n_items_task <- ncol(dat)

  model_fit <- get_model_fit(mod)

  # merge & append metadata + model-level fit
  classical %>%
    left_join(infit_tbl, by = "item") %>%
    left_join(sx2_tbl,   by = "item") %>%
    mutate(
      task        = task,
      model_set   = model_set,
      subset      = subset,
      nfact       = nfact,
      itemtype    = itemtype,
      invariance  = invariance,
      n_eff_task  = n_eff_task,
      n_items     = n_items_task,
      AIC         = model_fit$AIC,
      BIC         = model_fit$BIC,
      SABIC       = model_fit$SABIC,
      logLik      = model_fit$logLik,
      npar        = model_fit$npar
    ) %>%
    relocate(task, model_set, subset, itemtype, invariance, nfact,
             n_eff_task, n_items, item)
}

# ---------- run across ALL rows of mods_coded ----------
# Expect mods_coded to have columns: task, model_set, subset, mod_rec, nfact, itemtype, invariance, mod
all_stats <- purrr::pmap_dfr(
  mods_coded,
  summarize_item_fit_with_meta
)

# ---------- flags ----------
all_stats <- all_stats %>%
  mutate(
    flag_p      = p_correct < .20 | p_correct > .90,
    flag_pb     = point_biserial < .20,
    flag_infit  = ifelse(is.finite(infit), infit < .80 | infit > 1.20, NA),
    flag_zinfit = ifelse(is.finite(`z.infit`), abs(`z.infit`) > 2, NA),
    flag_SX2_01  = ifelse(!is.na(p.S_X2), p.S_X2 < .01,  NA),
    flag_SX2_001 = ifelse(!is.na(p.S_X2), p.S_X2 < .001, NA)
  )

# ---------- rollups (by task + subset + model spec) ----------
task_summary <- all_stats %>%
  group_by(task, model_set, subset, itemtype, invariance, nfact) %>%
  summarise(
    n_eff_task      = first(n_eff_task),
    n_items         = first(n_items),
    AIC             = first(AIC),
    BIC             = first(BIC),
    SABIC           = first(SABIC),
    logLik          = first(logLik),
    npar            = first(npar),
    mean_p          = mean(p_correct, na.rm = TRUE),
    median_p        = median(p_correct, na.rm = TRUE),
    mean_pbis       = mean(point_biserial, na.rm = TRUE),
    n_flag_p        = sum(flag_p, na.rm = TRUE),
    n_flag_pb       = sum(flag_pb, na.rm = TRUE),
    n_flag_infit    = sum(flag_infit, na.rm = TRUE),
    n_flag_zinfit   = sum(flag_zinfit, na.rm = TRUE),
    n_flag_SX2_p01  = sum(flag_SX2_01, na.rm = TRUE),
    n_flag_SX2_p001 = sum(flag_SX2_001, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(task, model_set, subset, itemtype, invariance)

# ---------- save (RDS + CSV) ----------
# Make sure the folder exists
dir.create(here("04_paper","display"), recursive = TRUE, showWarnings = FALSE)

# Detailed item-level
saveRDS(all_stats,     here("04_paper","display","diag_items_allstats.rds"))
write_csv(all_stats,   here("04_paper","display","diag_items_allstats.csv"))

# Summary level
saveRDS(task_summary,  here("04_paper","display","diag_items_summary.rds"))
write_csv(task_summary,here("04_paper","display","diag_items_summary.csv"))

```


# Psychometric properties of tasks

## Developmental change

```{r, fig.width = 8, fig.height = 8}
ggplot(coretask_scores, aes(x = age, y = metric_value)) +
    ggh4x::facet_nested_wrap(vars(task_category, task),
                             nest_line = element_line(), solo_line = TRUE,
                             axes = "x",
                             scales = "free_y") +
    geom_smooth(aes(group = site_label, color = site_label),
                method = "gam", formula = y ~ s(x, bs = "re")) +
    geom_point(aes(colour = site_label), alpha = 0.3) +
    scale_x_continuous(breaks = seq(6, 14, 2)) +
    .scale_colour_site() +
    guides(color = guide_legend(override.aes = list(fill = "white"))) +
    labs(x = "Age (years)", y = "Ability (IRT score)", colour = "Site") +
    theme(legend.position = "bottom")
```

```{r}
# random slopes model
library(lme4)
colnames(coretask_scores)

coretask_scores <- coretask_scores %>%
  mutate(age_c = age - mean(age, na.rm = TRUE))

# drop the correlation between intercept & slope 
fit <- lmer(metric_value ~ age_c + (age_c || site),
             data = coretask_scores,
             control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 2e5)))
fit
```

```{r}

# mean age to un-center for plotting
age_mean <- mean(coretask_scores$age, na.rm = TRUE)

# prediction grid across observed range
age_seq  <- seq(min(coretask_scores$age_c, na.rm = TRUE),
                max(coretask_scores$age_c, na.rm = TRUE),
                length.out = 200)

newdata  <- expand.grid(
  age_c = age_seq,
  site  = levels(factor(coretask_scores$site))
) %>%
  mutate(age = age_c + age_mean)

# site-specific (includes random effects)
newdata$y_hat_site <- predict(fit, newdata = newdata, re.form = NULL)

# population-average (fixed effects only)
newdata$y_hat_fixed <- predict(fit, newdata = newdata, re.form = NA)

# plot
ggplot() +
  geom_point(data = coretask_scores,
             aes(x = age, y = metric_value, colour = site),
             alpha = 0.12, size = 1, show.legend = FALSE) +
  geom_line(data = newdata,
            aes(x = age, y = y_hat_site, colour = site),
            linewidth = 1.1) +
  # overall fixed-effects line (dashed, black)
  geom_line(data = newdata |> distinct(age, y_hat_fixed),
            aes(x = age, y = y_hat_fixed),
            linewidth = 1.2, linetype = 2, colour = "black") +
  labs(x = "Age (years)",
       y = "Ability (IRT score)",
       colour = "Site",
       title  = "Random-intercept & slope by site: fitted developmental trends") +
  theme_minimal(base_size = 12) +
  theme(legend.position = "bottom")
```
## Reliability 
### Marginal reliability estimates

```{r}
task_rxx <- read_rds(here("02_scoring_outputs", "task_rxx.rds"))


task_rxx_table <- task_rxx |>
  rename(item_task = task) |>
  left_join(task_map) |>
  mutate(site_label = site |> fct_recode(!!!site_labels),
         itemtype = itemtype |> fct_recode("Rasch" = "rasch", "2PL" = "2pl"),
         invariance = invariance |> str_to_title()) |>
         # model_label = paste(itemtype, invariance) |> str_trim()) |>
  select(task, itemtype, invariance, site_label, rxx) |>
  pivot_wider(names_from = site_label, values_from = rxx) |>
  # mutate(across(everything(), \(v) v |> replace_na("–"))) |>
  arrange(task)

options(papaja.na_string = "–")
task_rxx_table |>
  rename(Task = task, Parameters = itemtype, Invariance = invariance) |>
  papaja::apa_table(col_spanners = list("Marginal reliability" = c(4, 6)))
options(papaja.na_string = "NA")
```

### Test-retest
<!-- > ### Test-retest [bd edit: can we put correlations in each panel?  ] -->

```{r, output="asis"}
de_retest_scores <- coretask_scores |> 
  filter(site == "pilot_leuphana_de") |>
  group_by(user_id, task) |>
  arrange(user_id, task, age) |>
  filter(n() > 1) 

retest_wide <- de_retest_scores |>
  group_by(user_id, task) |>
  mutate(has_retest = any(age_gap > .05), 
         mean_age = mean(age), 
         age_gap = max(age_gap)) |>
  filter(has_retest) |>
  ungroup() |>  
  pivot_wider(
    id_cols = c(user_id, task_category, task, mean_age, age_gap),
    names_from = run_number,
    values_from = metric_value,
    names_prefix = "run_"
  ) |>
  select(user_id, task_category, task, age = mean_age, age_gap, run_1, run_2)

trt <- retest_wide |>
  group_by(task) |>
  summarise(test_retest_r = cor(run_1, run_2, use = "complete.obs"), 
            n = n(), 
            age_gap = mean(age_gap)*12) |>
  rename(`Task` = task,
         `r` = test_retest_r, 
         `N` = n, 
         `Retest interval (months)` = age_gap)

papaja::apa_table(trt, digits = 2, caption = "Task-wise test-retest correlations computed for Germany data.")
```

```{r, fig.width = 8, fig.height = 8}
ggplot(retest_wide, aes(x = run_1, y = run_2, color = age)) +
  ggh4x::facet_nested_wrap(vars(task_category, task),
                         nest_line = element_line(), solo_line = TRUE,
                         axes = "x",
                         scales = "free_y") +
    # facet_wrap(vars(task)) +
  geom_point() +
  geom_smooth(aes(group = 1), color = "darkgrey", method = "lm") +
  scale_color_viridis(limits = c(5, 12)) +
  labs(x = "Score 1", y = "Score 2", color = "Age (years)") +
  theme(legend.position = "bottom")
```

## Validity 
### Construct validity
We assessed construct validity using confirmatory factor analysis (CFA) specified a priori, based on our theory of which indicators load on each construct. The measurement model specified three latent factors–Reasoning (matrix reasoning, mental rotation), Executive Function (EF; hearts and flowers, memory game, same-different-selection), and Language (grammar, vocab)– and treated math and Social Cognition (Theory of Mind; ToM) as observed outcomes. Math and ToM were modeled as observed outcomes rather than latent factors because each construct contained a single task. Forming a latent factor with one indicator would require strong, unverifiable assumptions about reliability/error variances and can be weakly identified. Treating them as endogenous observed variables keeps the model parsimonious and transparent while allowing them to be predicted by the latent domains and age. We included age to account for developmental change. 

We estimated two models. Model 1 included three latent factors and paths from age to the three factors as well as to math and ToM. Fit was mixed–CFI = .923, TLI = .850, RMSEA = .084 (90% CI .073–.095), SRMR = .059. However, factor correlations (residual, controlling for age) were high: Reasoning–EF .882, Reasoning–Language .747, and EF–Language .921. This suggests a general ability component could underpin these three latent factors. Controlling for age, maths and ToM showed a small positive residual association (r = .15), suggesting minor overlap not captured by age alone.
```{r prep-sem, include=FALSE, cache=TRUE}

# one row per user (metadata + task scores from first run)
meta <- coretask_firstrun_scores %>%
  group_by(user_id) %>%
  summarise(
    age        = min(age, na.rm = TRUE),
    site       = dplyr::first(na.omit(site)),
    site_label = dplyr::first(na.omit(site_label)),
    dataset    = dplyr::first(na.omit(dataset)),
    .groups = "drop"
  )

# wide task scores (first run only)
wide_scores <- coretask_firstrun_scores %>%
  select(user_id, item_task, metric_value) %>%
  distinct() %>%  # guard against accidental duplicates
  tidyr::pivot_wider(
    names_from  = item_task,
    values_from = metric_value
  ) %>%
  left_join(meta, by = "user_id") %>%
  relocate(user_id, site, site_label, dataset, age)

# keep only the core task columns + metadata
vars_core <- c("math","matrix","mrot","sds","hf","mg","trog","vocab","tom")
wide_scores <- wide_scores %>%
  select(user_id, site, site_label, dataset, age, any_of(vars_core))

# standardise
z <- function(x) (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
vars_core_std <- wide_scores %>%
  mutate(across(all_of(vars_core), ~ z(as.numeric(.x))))
str(vars_core_std[vars_core])

```

```{r cfa, main model, include=FALSE, cache=TRUE}
cfa_1 <- "
  reasoning =~ matrix + mrot
  ef        =~ hf + mg + sds
  language  =~ trog + vocab

  reasoning ~ age
  ef        ~ age
  language  ~ age
  math      ~ age
  tom       ~ age"

fit_1 <- cfa(cfa_1, data = vars_core_std,
             estimator = "MLR", std.lv = TRUE, missing = "fiml")
summary(fit_1, fit.measures = TRUE, standardized = TRUE)

```

```{r fig-semplot-main, fig.width=8}

semPlot::semPaths(
  fit_1, 
  what="mod", 
  whatLabels="std", 
  style="lisrel", 
  layout="tree2",
  rotation=3, 
  intercepts=FALSE, 
  residuals=FALSE, 
  thresholds=TRUE,
  intStyle = "multi",
  curvePivot = TRUE, curve = 1,
  # size
  sizeMan=6, sizeLat=10, sizeInt=6,
  sizeMan2=6, sizeLat2=6, sizeInt2=6,
  esize=2, asize=1,
  # bigger text
  label.cex=1.2, edge.label.cex=1.1, edge.label.bg=TRUE,
  label.prop=.7, label.curv=0.1,
  nDigits=1, nCharNodes=8,
  mar=c(6,6,6,6)
)
```
Given the strong inter-factor correlations, we specified a second model with a hierarchical structure: three first-order factors (Reasoning, EF, Language) loading on a second-order general factor (g). Math and ToM were again treated as observed endogenous outcomes regressed on g. We included a single developmental path for age → g Conventional fit indices was similar to Model 1 (CFI = .902, TLI = .857, RMSEA = .081, SRMR = .064). 

Indicator loadings on their first-order factors were moderate–strong (e.g., matrix reasoning = .79; grammar = .78; vocab = .76), and g strongly underpinned the domains (Reasoning = .91; EF = .99; Language = .89; all standardized). Age positively predicted g (β = .69; R²(g) ≈ .48). In turn, g strongly predicted math (β = .87; R² ≈ .76) and moderately predicted ToM (β = .58; R² ≈ .33), yielding sizeable indirect effects of age via g (age → g → Maths ≈ .60; age → g → ToM ≈ .40). After accounting for g, we found a negative residual math–ToM association (r ≈ −.35), indicating that the small positive overlap in Model 1 largely reflected unmodeled general ability.

Overall, factor loadings supported the intended three-factor structure (convergent validity), and age related to each domain as expected. The high inter-factor correlations and strong second-order loadings indicate that much of the variance is shared across domains, consistent with a broad general ability underpinning performance on the nine core tasks.
```{r second fit, include = FALSE, cache = TRUE}
model_g_no_age <- "
  # first-order factors
  reasoning =~ matrix + mrot
  ef        =~ hf + mg + sds        
  language  =~ trog + vocab

  # second-order g
  g =~ reasoning + ef + language

  # observed outcomes of g
  math ~ g
  tom  ~ g
"

fit_g <- sem(model_g_no_age, data = vars_core_std,
             estimator = "MLR", std.lv = TRUE, missing = "fiml")

summary(fit_g, fit.measures = TRUE, standardized = TRUE)
head(lavaan::modindices(fit_g, sort.=TRUE, minimum.value=10), 15)
```

```{r add age, include = FALSE, cache = TRUE}
# add age
model_g_age <- paste0(model_g_no_age, "\n g ~ age")
fit_g_age <- sem(model_g_age, data = vars_core_std,
                 estimator = "MLR", std.lv = TRUE, missing = "fiml")
summary(fit_g_age, fit.measures = TRUE, standardized = TRUE)
head(lavaan::modindices(fit_g_age, sort.=TRUE, minimum.value=10), 15)
```

```{r fig-semplot-g}

semPlot::semPaths(
  fit_g_age,
  whatLabels = "std",
  residuals  = FALSE,
  intercepts = FALSE,
  thresholds = FALSE,
  layout     = "tree2",
  # text sizes
  label.cex       = 1.2,   
  edge.label.cex  = 1,   
  sizeMan = 6,
  sizeLat = 10,
  nCharNodes = 0,
  nDigits=2,
  mar = c(6,6,6,6)       
)
```

### External validity
To assess external validity, we fitted a single-factor EF model with hearts and flowers (hf), memory game (mg), same-different-selection (sds), and the external measure, the Minneosta Executive Function Scale (MEFS) as indicators. We modelled age as a predictor of the latent EF factor. Fit was generally good (CFI = .971, TLI = .943, SRMR = .034; RMSEA = .071, 90% CI .040–.106). Standardised loadings were moderate to strong (hf = .63, mg = .63, sds = .50, MEFS = .67), indicating that MEFS aligns closely with the latent EF construct. Age strongly predicted EF (β = .79; R² ≈ .62), consistent with expected developmental gains.
```{r prep for sem - ef, include = FALSE, cache = TRUE}

ef_tasks <- c("hf","mg","sds","mefs")

# Filter to ef tasks (from included_scores), keep first run 
ef_firstrun_scores <- included_scores %>%
  filter(item_task %in% ef_tasks) %>%
  arrange(user_id, item_task, run_number, time_started) %>%
  group_by(user_id, item_task) %>%
  slice(1) %>%
  ungroup()

# one row per user
meta_ef <- ef_firstrun_scores %>%
  group_by(user_id) %>%
  summarise(
    age        = min(age, na.rm = TRUE),
    site       = dplyr::first(na.omit(site)),
    site_label = dplyr::first(na.omit(site_label)),
    dataset    = dplyr::first(na.omit(dataset)),
    .groups = "drop"
  )

# Wide ef task scores (first run only)
ef_wide_scores <- ef_firstrun_scores %>%
  select(user_id, item_task, metric_value) %>%
  distinct() %>%  # guard against accidental duplicates
  pivot_wider(
    names_from  = item_task,
    values_from = metric_value
  ) %>%
  left_join(meta_ef, by = "user_id") %>%
  relocate(user_id, site, site_label, dataset, age)

# keep only ef columns + metadata
vars_ef <- ef_tasks
ef_wide_scores <- ef_wide_scores %>%
  select(user_id, site, site_label, dataset, age, any_of(vars_ef))

# Standardise ef indicators (same z() you used)
z <- function(x) (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
ef_wide_std <- ef_wide_scores %>%
  mutate(across(all_of(vars_ef), ~ z(as.numeric(.x))))

# str(ef_wide_std[vars_ef])
```

```{r sem fit, include = FALSE, cache = TRUE}
cfa_ef <-  '
EF =~ hf + mg + sds + mefs
EF ~ age
'

fit_ef <- cfa(cfa_ef, data = ef_wide_std,
              estimator = "MLR", std.lv = TRUE, missing = "fiml")
summary(fit_ef, fit.measures = TRUE, standardized = TRUE)
print(fitMeasures(fit_ef, c("cfi","tli","rmsea","srmr", "aic", "bic")))
```

```{r fig-semplot-ef}

semPlot::semPaths(
  fit_ef,
  whatLabels = "std",
  residuals  = FALSE,
  intercepts = FALSE,
  thresholds = FALSE,
  layout     = "tree2",
  # text sizes
  label.cex       = 1.4,   
  edge.label.cex  = 1,   
  sizeMan = 6,
  sizeLat = 10,
  nCharNodes = 0,
  nDigits=2,
  mar = c(6,6,6,6)       
)
```
We ran a similar model to determine the external validity of the language and literacy measures (grammar and vocab) against ROAR tasks including measures of Phonological Awareness (pa), Single Word Reading (swr) and Sentence Reading Efficiency (sre). We fitted a single latent Language factor with ROAR tasks plus grammar (trog) and vocabulary as indicators, and modelled age as a predictor of the latent factor. Fit was borderline acceptable: CFI = .937, TLI = .895, RMSEA = .070, 90% CI .052–.089; SRMR = .070. Standardised loadings were: swr = .743, sre = .557, pa = .685, trog = .655, vocab = .814–all significant, indicating that the ROAR tasks align with grammar and vocabulary on a common language/literacy factor, with vocabulary the strongest indicator. Age positively predicted the latent factor (β = .66; R² ≈ .44), consistent with expected developmental gains.
```{r prep for sem, include = FALSE, cache = TRUE}

lang_vars <- c("swr","sre","pa","vocab","trog")

# first run per user–task
lang_firstrun_scores <- included_scores %>%
  filter(item_task %in% lang_vars) %>%
  arrange(user_id, item_task, run_number, time_started) %>%
  group_by(user_id, item_task) %>%
  slice(1) %>%
  ungroup()

meta_lang <- lang_firstrun_scores %>%
  group_by(user_id) %>%
  summarise(
    age        = min(age, na.rm = TRUE),
    site       = dplyr::first(na.omit(site)),
    site_label = dplyr::first(na.omit(site_label)),
    dataset    = dplyr::first(na.omit(dataset)),
    .groups = "drop"
  )

# create wide df 
lang_wide_scores <- lang_firstrun_scores %>%
  select(user_id, item_task, metric_value) %>%
  distinct() %>%
  pivot_wider(
    names_from  = item_task,
    values_from = metric_value
  ) %>%
  left_join(meta_lang, by = "user_id") %>%
  relocate(user_id, site, site_label, dataset, age)

# keep only language columns + metadata
lang_wide_scores <- lang_wide_scores %>%
  select(user_id, site, site_label, dataset, age, any_of(lang_vars))

# standardise 
z <- function(x) (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
lang_wide_std <- lang_wide_scores %>%
  mutate(across(all_of(intersect(lang_vars, names(.))), ~ z(as.numeric(.x))))
```

```{r sem fit - lang, include = FALSE, cache = TRUE}
# model
cfa_lang <-  '
Language =~ swr + sre + pa + trog + vocab
Language ~ age
'

fit_lang <- cfa(cfa_lang, data = lang_wide_std,
                estimator = "MLR", std.lv = TRUE, missing = "fiml")
# summary(fit_lang, fit.measures = TRUE, standardized = TRUE)
# print(fitMeasures(fit_lang, c("cfi","tli","rmsea","srmr", "aic", "bic")))
```

```{r fig-semplot-lang}
semPlot::semPaths(
  fit_lang,
  whatLabels = "std",
  residuals  = FALSE,
  intercepts = FALSE,
  thresholds = FALSE,
  layout     = "tree2",
  # text sizes
  label.cex       = 1.4,   
  edge.label.cex  = 1,   
  sizeMan = 6,
  sizeLat = 10,
  nCharNodes = 0,
  nDigits=2,
  mar = c(6,6,6,6)       
)
```

# Adaptive task construction

<!-- [bd edit: are we planning to actually specify the models we're using somewhere? would clarify meaning here (and be generally helpful)] -->
Many of the LEVANTE tasks have been adapted and piloted as CATs (Computerized Adaptive Tests). To date, these include the Test For Reception of Grammar (TROG), Vocabulary, Shape Rotation, Matrix Reasoning, Same Difference Selection, and Math. These tasks maintain an ability score, theta, as an estimate of the participant’s skill level that is updated at the end of each trial. They then present participants with the item best suited to their estimated ability, which both improves test-taker experience and yields more information on participant skill per item, allowing for a shorter task with fewer items. 

<!-- [bd edit: i'd cut this bit about the 4 params esp given that we haven't really talked much about item params above in the general case] -->
 The CAT tasks use an adaptive algorithm made available by the jsCat JavaScript library [@ma2025], which offers an implementation of an Item Response Theory (IRT) model including up to 4 parameters: discrimination, a value representing the item’s informativeness in distinguishing high and low ability test-takers, guessing, the probability of selecting the correct response at random, upper asymptote, the maximum likelihood of a correct answer, and difficulty. The present LEVANTE CAT implementation varies difficulty and guessing for each item while holding both discrimination and upper asymptote constant at 1. Items are selected based on Maximum Fisher Information, and theta is updated according to a maximum likelihood estimator, with limits of -6 and 6. 

The LEVANTE CATs are configurable with respect to the initial value of theta and the conditions for ending the task. The starting theta is set at 0 for all CAT tasks currently in use, but can be lowered or raised according to the researcher’s prior expectation of participant ability, for example according to age. Current CAT implementations use stopping rules based on either time or number of items. TROG, Shape Rotation, and Vocabulary each have time limits currently set to 4 minutes, with Matrix Reasoning set to 6 minutes to allow for the increased time typically required to complete items in this task. Items in these tasks are presented together in a single block. Same Difference Selection and Math are each divided into three blocks presented sequentially, with per-block stopping based on number of items. These CATs select from the list of items specific to their current block and proceed to the next block once the target number of items is reached, maintaining one overarching ability estimate for the entire task. Same Difference Selection and Math have time limits of 6 minutes and 8 minutes, respectively. 



# Discussion



* Plans for internationalization
* Plans for downward extension
* Plans for updates



\newpage

# References

<!-- ::: {#refs custom-style="Bibliography"} -->
<!-- ::: -->
