---
title             : "Creation and validation of the LEVANTE core tasks: Internationalized measures of learning and development for children ages 5-12 years"
shorttitle        : "LEVANTE"

author: 
  - name          : "George Kachergis*"
    affiliation   : "1"
  - name          : "Fionnuala O'Reilly*"
    affiliation   : "1"
  - name          : "Mika Braginsky"
    affiliation   : "1"
  - name          : "Amy Lightbody"
    affiliation   : "1"
  - name          : "Katherine Adams Shannon"
    affiliation   : "1"
  - name          : "Zachary Watson"
    affiliation   : "1"
  - name          : "Doria Xiao"
    affiliation   : "1"
  - name          : "Lijin Zhang"
    affiliation   : "1"
  - name          : "Rebecca Zhu"
    affiliation   : "1"
  - name          : "Anya Wanjing Ma"
    affiliation   : "1"
  - name          : "Bria Long"
    affiliation   : "2"
  - name          : "Tonya Murray"
    affiliation   : "1"
  - name          : "Jason Yeatman"
    affiliation   : "1"
  - name          : "Michael Sulik"
    affiliation   : "1"    
  - name          : "Jelena Obradović"
    affiliation   : "1"
  - name          : "Nichola Jenkins"
    affiliation   : "3"
  - name          : "Daniel Ansari"
    affiliation   : "3"
  - name          : "Maria Camilla Perfetti"
    affiliation   : "4"
  - name          : "Julian Mariño"
    affiliation   : "4"
  - name          : "Luise Hornoff"
    affiliation   : "5"  
  - name          : "Manuel Bohn"
    affiliation   : "6"  
  - name          : "Nilam Ram"
    affiliation   : "1"
  - name          : "Benjamin W. Domingue"
    affiliation   : "1"    
  - name          : "Michael C. Frank"
    affiliation   : "1"
    role:
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
      - "Supervision"

affiliation:
  - id            : "1"
    institution   : "Stanford University"
  - id            : "2"
    institution   : "University of California, San Diego"
  - id            : "3"
    institution   : "Western University"
  - id            : "4"
    institution   : "Universidad de los Andes"
  - id            : "5"
    institution   : "Max Planck Institute for Evolutionary Anthropology"
  - id            : "6"
    institution   : "Leuphana University"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

abstract: | 
  We present the Learning Variability Network Exchange (LEVANTE) core tasks, a set of nine short and engaging computer adaptive tasks designed to assess learning and development in children ages 5--12 years across a wide range of languages and cultures. Using a simple and uniform multi-alternative forced choice format, these tasks measure constructs including math, execxutive function, reasoning, and social cognition and can be administered on a tablet or computer both in person or remotely. We describe the design and selection of these instruments, and then report on their reliability and validity in a pilot sample of XYZ children recruited in Colombia, Germany, and Canada. Tasks are scored using item response theory models. These models can be used to create computer adaptive versions of the tasks, allowing the entire battery to be given in under an hour. We discuss the use and extension of these tasks in the service of creating an open dataset to describe variability in children's development and learning across contexts. 


keywords: "cognitive development"
wordcount: "X"

bibliography: "library.bib"

floatsintext: no
linenumbers: yes
draft: no
mask: no

figurelist: no
tablelist: no
footnotelist: no

classoption: "man"
output:
  papaja::apa6_pdf:
    includes:
      in_header: preamble.tex
---


```{r, include = FALSE}
# authors
# Kachergis*, O’Reilly* 
# 
# Major contributors:
# Mika
# Doria
# Mike
# Nilàm
# 
# Other authors:
# Rebecca 
# Kat
# Amy
# Zach
# Lijin
# Ben
# 
# Pilot data contributors
# DE: Luise & Manuel
# CO: Camila & Julian
# CA: Nichola & Daniel 
# 
# Task contributors
# Jason, Tonya (TROG), Anya (VV) 
# Michael Sulik, Jelena Obradovic (EF)

```


```{r setup, include = FALSE}
library(here)
library(papaja)
library(tidyverse)
library(glue)
library(viridis)
source(here("03_summaries","plotting_helper.R"))
source(here("plot_settings.R"))
source(here("03_summaries","scores_helper.R"))

sites <- c("Colombia", "Germany", "Canada") #, "pilot_langcog_us", 
site_pal <- solarized_pal()(length(sites)) |> rlang::set_names(sites)
# r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE)
```

# Introduction

Developmental variability and change during childhood is a focus of intense theoretical and practical interest. From tracking children's growth over time to evaluating intervention outcomes or exploring environmental and contextual moderators, a wide range of scientific goals require accurate assessments. Ideal psychological measures provide efficient, reliable, and valid measures of particular constructs that can be applied across a range of ages, situations, and contexts. Yet, in most cases, a large gap separates the situation of a researcher searching for measures from this ideal. 

Because children's overall capacities are so dependent on their age, it can be very challenging to use the same measure across children of different ages. Young children require simple tasks that are not verbally demanding, while older children can answer more complicated questions. In addition, younger children typically require shorter tasks, often reducing measurement reliability. Yet giving different tasks to different ages can mean that scores are not comparable to one another, making tracking developmental growth challenging in many domains.  

A second set of challenges concern cross-context comparisons.  Ideal developmental measures should be validated in a global context and applicable to children across many cultures and languages. Child development is an issue of global importance [LANCET CITES], yet the vast majority of measures are developed in very specific (often English-speaking) contexts. Providing cross-culturally validated measures allows for the collection of comparable data across contexts, opening up opportunities for theoretical synthesis.

A final set of challenges has to do with accessibility. Many gold-standard measures are commercially distributed. They are costly for researchers to use, and in addition, publishers may place barriers on new translation and adaptation. Publishers also typically hold both item information and normative data closely, blocking many types of secondary investigation. 

In the context of these challenges, we describe the Learning Variability Network Exchange (LEVANTE) [@frank2025]. LEVANTE provides a technical framework for data collection: researchers can use the LEVANTE dashboard to assign both surveys and tasks to children, caregivers, and teachers. Data collected via the dashboard are harmonized and validated and become accessible through a data repository, first to the researchers who collect them and eventually -- through regular releases -- to the broader research community. Through a partnership with the Jacobs Foundation, sites around the world are funded to collect longitudinal data from children using the LEVANTE framework. The eventual goal of LEVANTE is to create a large dataset documenting children's learning and development across contexts. 

The current manuscript introduces the LEVANTE core tasks, a suite of behavioral measures for children developed for LEVANTE. In our initial development of the framework, we cast a broad net for important constructs in child development with well-accepted measures that had been used internationally. This process is described in @frank2025. The broad constructs that we selected were executive function, language, mathematics, reasoning, and social cognition, with these being instantiated through a number of well-accepted tasks. 

To create our core tasks, we selected pre-existing measures from the literature that tapped each of these constructs. When possible we prioritied measures with strong psychometric properties, previous use across a broad range of cultures, applicability across a broad range of ages, and lack of commercial or licensing constraints. This process yielded a series of measures, which we implemented in an open source web platform. Table \@ref(tab:tasks) shows these tasks, organized by construct. 

In addition to the constructs described above, we were also interested in the assessment of literacy. The LEVANTE core tasks battery makes use of a number of previously-validated literacy tasks from the Rapid Online Assessment of Reading [@yeatman2021] , including single word reading, sentence reading efficiency, and phonological awareness. We do not report on these tasks here, though we make use of them for validation of language measures. Similarly, we included a commercially-available broad measure of executive function, the Minnesota Executive Function Scale (MEFS) [@mefs], for validation purposes. 
 

```{r tasks, output="asis"}
tasks <- tribble(~`Construct`, ~`LEVANTE name`, ~`Prior names / Source task name`, ~`Adaptive?`, ~Reference,
                 "Executive Function", "Hearts and Flowers", "Hearts and Flowers", "", "XYZ",
                 "", "Memory", "Corsi Block Task", "X", "",
                 "", "Same and Different", "Same Different Selection Task", "X", "",
                 "Language", "Vocabulary", "Picture Vocabulary", "X", "",
                 "", "Sentence Understanding", "Test for Reception of Grammar (TROG)", "X","",
                 "Math", "Math", "Early Grades Math Assessment (EGMA)", "X", "",
                 "Reasoning", "Pattern Matching", "Matrix Reasoning", "X", "",
                 "", "Shape Rotation", "Mental Rotation", "X", "",
                 "Social Cognition", "Stories", "Theory of Mind", "X", "")

papaja::apa_table(
  tasks, landscape = TRUE,
  caption = "The LEVANTE core tasks, presented with their internal label as well as prior labels used in the literature.")
```

Here we report on the development and validation of these tasks. This is an iterative process in which data from 5--12 year old children has been collected across three sites: Bogota, Colombia; Leipzig, Germany; and Ottawa, Canada. In some cases that we note below, we used these data during the data collection process to make minor changes to the tasks. We use data from these three pilot sites both to provide initial evidence on the reliability and validity of the measures and to develop efficient, computer adaptive (CAT) versions of nearly all of the tasks. 

A key component of this process is the use of psychometric models based on item-response theory (IRT) [@embretson2001]. IRT models provide a family of models that allow the joint estimation of the difficulty of individual task items (e.g., math questions) and the ability of individual children. A fitted IRT model provides task parameters that can be used to estimate the ability of a new test taker given their responses on some or all of the same items. In addition, IRT parameters are used in the construction of CATs, which choose the most relevant items to give to estimate the ability of a particular individual. Critically for our purposes, the use of IRT models means that we can provide comparable scores on the same scale to a younger child who saw mostly easier task items and an older child who saw harder items; these models thus allow us to address our first key challenge posed above.  

Because our data come from three sites, each with their own translations and adaptations of the specific tasks, we can also use multi-group IRT models to explore the question of invariance: whether measures function similarly across different groups [@bornstein2016]. While measurement invariance is more commonly discussed in the factor analytic literature, it is also applicable to IRT (where it is sometimes analyzed at the level of individual test items as "differential item function" across groups) [@thissen2024]. Here we use multigroup model comparisons (described below) to investigate whether our tasks measure similarly structured constructs across groups. In particular, where possible, we aim for *scalar invariance*, in which individuals from different groups still show the same relative ordering of difficulty across items (e.g., they still find fractions items harder than division items in a math test). In some cases, we may fall back to *metric invariance*, in which items show different difficulties across groups, or *configural invariance*, in which items show different degrees of ability discrimination as well (e.g., if some problem types are unfamiliar to children in one group and so do not discriminate between high and low ability children). These models allow us to begin to address the second challenge posed above.

<!-- https://www.tandfonline.com/doi/10.1080/00273171.2024.2396148 -->

In what follows, we begin by describing the nine LEVANTE core tasks, organized by construct. We then discuss the process of translation and adaptation that produced the Spanish and German versions of these tasks from the original English source. We then discuss our pilot data collection efforts in Colombia, Germany, and Canada. We present our IRT-based scoring techniques and the results of multi-group comparison. Using scores from these analyses, we then present preliminary evidence on the reliability and validity of the tasks, recognizing that in many cases these tasks are still under construct and we anticipate increases in reliability as we iteratively improve items. 

We end by discussing future plans for further internationalization and downward extension of the tasks. Critically, LEVANTE embraces open science values, aiming to create measures and data that are permissively licensed and reusable and extensible by the international research community. These values address our final challenge posed above: the aim of LEVANTE is to minimize barriers to reuse, accelerating progress towards a global science of learning and development. 

# The LEVANTE core tasks



The LEVANTE core tasks are implemented using jsPsych [@deleeuw] and can be presented in a web browser on a tablet or laptop, with responses possible using a touchscreen, keyboard, or mouse. Because of this variability in format of administration, they focus on response correctness not reaction time and so they are mostly untimed. The tasks are designed for simplicity and clarity so as to be accessible to children across a wide age range, and so with only modest exceptions, nearly all are in the format of a multi-alternative forced choice with a maximum of four choices. This uniformity of format means that in most cases instructions can be short and easy to understand, minimizing delays when the tasks are given in sequence as a battery. Figure \@ref{fig:tasks} shows screenshots from a number of tasks. In the remainder of this section, we briefly present each of the LEVANTE core tasks.

General task lengths are described below and average task durations are given in Table \@ref(tab:durations). Because of our interest in testing children across a wide range of ages, we intentionally included trials that we anticipated would be both very easy and very hard for children. Overall we worried that this strategy would lead to frustration for children, however, so during pilot testing, we experimented with a number of different stopping rules. In early iterations of pilot testing in Bogota, Colombia, we ended tasks after three incorrect trials; we later modified this rule to end tasks after six incorrect trials. 

All tasks are available for demonstration purposes at [http://researcher.levante-network.org](). Source code for the tasks is available at [https://github.com/levante-framework/core-tasks](). Task code and assets are licensed CC-BY-NC 4.0 for non-commercial reuse (including educational use by not-for-profit and governmental entities) with appropriate attribution. Please see repository license for more details.

## Language 

### Sentence Understanding

The Test for Reception of Grammar (TROG) [@bishop1982] is a multiple-choice measure of receptive grammatical understanding. On each trial, the child hears a spoken sentence and is asked to select one of four pictures that best matches its meaning. The original test contained 20 blocks, each with four items assessing the same grammatical structure. In our adaptation (based on the original TROG, which was permissively licensed for reuse), we removed a small number of items due to changes in cultural norms. In addition, based on early pilot testing showing that many trials were easy for older children, we added a set of several dozen more challenging sentences. All illustrations were remade with details intended to be accessible across a broad range of cultures. The task had 103 separate items.

### Vocabulary

The Vocabulary task was developed as a non-commercial, open alternative to tasks such as the Peabody Picture Vocabulary [@Peabody] and the NIH Toolbox Picture Vocabulary Task [@nihtoolbox]; see @long_ma_tan_silverman_frank_yeatman_2025 for more details. In this task, children are presented with a word and four pictures. Children must select the correct picture over the distractor picture. A first set of targets and distractors were sourced from the THINGS+ dataset [@hebart2019things, @stoinski2024thingsplus], specifically from the permissively-licensed subset of the data; in this item subset (108 items) each image had a semantically close and semantically far distractor as well as an unrelated distractor image (e.g., target word “acorn” with close distractor being a “coconut”, and the far and unrelated distractors being “keys” and “laundry”).  Additional difficult items were included beyond those analyzed in @long_ma_tan_silverman_frank_yeatman_2025. These more difficult items we sourced from translated items from an open-sourced Dutch receptive vocabulary assessment tool, the DAIVT [@bousard2021dutch]; we also manually constructed several of our own items to reach a total of 170.

## Math

We developed the Math task based on the EGMA (Early Grade Mathematics Assessment) [@egma]. This short, paper-and-pencil assessment is widely administered in international contexts for children ages 5--8 and includes number identification, number comparison, missing number, addition, and subtraction sub-tests. In our adaptation, we increased the breadth of the initial item bank, added multiplication, division, and fractions items to extend the age range up. We also added number line identification problems in which children had to place a marker on a number line across a range of different scales (including simpler 0--10 trials and more challenging larger scales as well as a fraction scale). Number line problems of this type have been suggested to be strongly related to math ability [@siegler other refs]. The primary version of this task included 275 items (though no child saw all of these) and the secondary item bank (for retest purposes) included an additional 237. 

## Reasoning

### Matrix Reasoning

We created a matrix reasoning task following the popular Raven’s Progressive Matrices (RPM). RPM is a relatively simple, relatively language-independent pattern recognition task appropriate for children and adults, and it has relatively good test-retest reliability in low- and middle-income contexts (i.e., Kuwait; Abdel-Khalek, 2005), as well as substantial evidence of external validity with other measures of school achievement (Downey et al., 2014; Green et al., 2017; Pind et al., 2003). We used an open-source matrix reasoning item bank Mars-IB



### Shape Rotation

The shape rotation task was 

  <!-- > [bd: note that we describe this as 'mental rotation' below] -->

## Executive Function

### Same Difference Selection

The Same Different Selection task (Obradovi\'c \& Sulik, year?) is designed to assess cognitive flexibility in children. It draws upon elements from the ‘Something's the Same’ task (Willoughby et al., 2012) and the ‘Flexible Item Selection Task’ (Jacques \& Zelazo, 2001). In this task, children are presented with sets of items that vary along multiple dimensions, such as shape, color, size, and number. They are required to identify similarities and differences between items based on these dimensions, thereby engaging their ability to shift attention and adapt to changing rules or criteria.

### Hearts and Flowers

The Hearts and Flowers task (Davidson et al., 2006) assesses inhibitory control and cognitive flexibility. Participants respond according to stimulus type: pressing a key on the same side as a heart (congruent rule), and on the opposite side for a flower (incongruent rule). The task includes three blocks - congruent (hearts only), incongruent (flowers only), and mixed (hearts and flowers). The congruent block serves as a baseline with minimal executive demands. Inhibitory control is typically measured via performance on the incongruent block, which requires overriding a spatially dominant response, while cognitive flexibility is assessed using the mixed block, which demands switching between rules based on the stimulus (Wright \& Diamond, 2014).which demands switching between rules based on the stimulus (Wright \& Diamond, 2014).

### Memory Game

The Corsi Block task is a widely used measure of visuospatial short-term memory [@corsi1972]. In the standard version, a set of four blocks is arranged in a fixed spatial configuration. During each trial, a subset of blocks lights up one at a time in a specific sequence. The child is required to reproduce the sequence by clicking the blocks in the same order. The task begins with short sequences (e.g., two items) and gradually increases in difficulty (up to five or more) until the child fails two sequences of the same length. The longest correctly reproduced sequence reflects their visuospatial span. When adapted for younger children or digital administration, the number of visible blocks may be reduced (e.g., four blocks), and span lengths are typically capped at five to reduce task complexity.

## Social Cognition


### Stories task


# Translation and Adaptation

Initially designed in English, core tasks were subsequently adapted for Spanish and German following a set of internationalization procedures in collaboration with the sites collecting pilot data in Colombia and Germany. We began with AI-generated translations of task instructions, items, and other key terminology such as encouragement phrases that appear throughout the core tasks. A professional translator then reviewed and edited the AI translations prior to a pilot site representative confirming context and appropriateness to the task. Back translation using AI technology facilitated evaluation of meaning and context with discussion of items or wording as needed to ensure both cultural relevance and task integrity. For tasks evaluating language skills (e.g. Vocabulary; Sentence Understanding), an additional step was employed. Linguists with expertise in language acquisition reviewed items and made recommendations about relevant changes or additions specific to the Spanish and German languages.

# Pilot Data Collection

The pilot site partnership represents an integral component of the core task development. Across three countries and three languages, pilot sites provided collaboration on task adaptation and functionality, iterative task testing, and diverse settings for infrastructure deployment. Data collected from pilot sites allowed analyses of construct validity, measurement invariance, test-retest reliability, parameters for computer adaptive testing, and a demonstration of developmental growth measured by the tasks across constructs.  

One key feature of the LEVANTE framework is that all data are completely de-identified, reducing legal and ethical obstacles to data sharing. No demographic information or other identifiers are ever entered into the LEVANTE dashboard by the sites, and ages are only given to a one month precision [@frank2025]. We anticipate that sites will add sociodemographic information into the dataset at a later time via an app that is currently under development that ensures that there is no statistical reidentification risk for  individuals (e.g., due to rare combinations of traits in a community). For this reason, here we provide only minimal demographic characterization and analysis of the children from our pilot sites.  

```{r data}
# mefs <- read_rds(here("02_scoring_outputs", "scores", "scores_mefs.rds"))

site_labels <- c("Canada" = "pilot_western_ca",
                 "Colombia" = "pilot_uniandes_co",
                 "Germany" = "pilot_leuphana_de")

all_scores <- readRDS(here("02_scoring_outputs","scores","scores_combined.rds")) |>
  # bind_rows(mefs) |>
  group_by(user_id, item_task) |>
  arrange(time_started) |>
  mutate(run_number = 1:n(),
         age_gap = age - age[1]) |>
  ungroup() |>
  mutate(site_label = site |> fct_recode(!!!site_labels)) |>
  mutate(task = task |> str_to_title() |> fct_inorder(),
         task_category = task_category |> str_to_title() |> fct_inorder())

included_scores <- all_scores |>
  filter(!is.na(age), age >= 5, age <= 12) |>
  filter(run_number == 1 | (run_number == 2 & age_gap > 1/6))

coretask_scores <- included_scores |>
  filter(item_task %in% core_tasks)

coretask_firstrun_scores <- coretask_scores |>
  filter(run_number == 1)
```


```{r}
ages <- coretask_scores |>
  group_by(site, site_label, dataset, user_id) |>
  summarise(age = min(age), 
            n_runs = n())
```

```{r, fig.height = 3}
ggplot(ages, aes(x = age, fill = site)) + 
  facet_wrap(vars(site_label)) + 
  geom_histogram(binwidth = 1, color = "white") +
  scale_fill_solarized(guide = "none") +
  scale_y_continuous(expand = expansion(0, 0)) +
  labs(x = "Age (years)", y = "Number of children")
```

<!-- For each site, exclusions (raw numbers + percent of site total): -->
<!-- - runs excluded due to: -->
<!--   - missing metadata -->
<!--   - duplicate run -->
<!--   - too short / incomplete -->
<!--   - validation failures (keymashing) -->
<!--   - kids excluded due to  -->
<!--     - missing metadata -->
<!--     - age out of range -->


<!-- Example: Germany recorded a total of X runs, of which N (X%) were excluded due to reason. -->

<!-- Of the included runs, there were a total of NNN trials, of which XYZ were excluded. For reasons of space we do not disaggregate these exclusions by task, but (descriptive text, like most of them were excluded from these two tasks).  -->

<!-- Trial exclusions -->
<!-- - due to RT issues -->


<!-- After exclusion, we ended up with XYZ kids in the site (% of total kids with any runs at all before exclusions). These kids had a total of XYZ tasks (average XYZ, range).  -->
<!-- (note Germany needs retest description too) -->


## Colombia

Partners at the Universidad de Los Andes, led by Julián Mariño, utilized school-based data collection across four schools in Bogotá and three schools in the rural areas of Caquetá and Boyacá, Colombia. By partnering with schools, the research team oversaw task data collection with children using a group testing format on tablets provided by the team. The Colombia site collected data from XX children (XX female; XX male), ages 5-12 years. 

## Germany

Partners at Max Planck Institute for Evolutionary Anthropology, led by Manuel Bohn, collected data using family-based remote data collection. Participants were recruited via an existing database by first contacting families via telephone with an invitation to participate in the study. Upon agreement, families received an email with information to log into the LEVANTE system and complete the tasks assigned to them at home using their own computer or tablet. Germany provided retest data with follow-up assignments sent to families 3-6 months after initial completion of the tasks. The Germany site collected data from XX children (XX female; XX male), ages 5-12 years with XX children completing the retest assignments.

## Canada

Partners at Western University, led by Daniel Ansari, recruited participants from the local community and tested children individually in a clinic setting. The Canadian site collected data from XX children (XX female; XX male), ages 5-12 years. 


# Scoring

```{r invariance, output="asis"}
invariances <- read_csv(here("04_paper","display","invariance.csv")) |>
  mutate(across(everything(), \(v) replace_na(v, "")))

papaja::apa_table(
  invariances |> select(`Invariance`, `Model`, `Factor analysis name`,
                        `IRT explanation`, `Interpretation`),
  align = c("m{2cm}", "m{3cm}", "m{5cm}", "m{5cm}", "m{5cm}"),
  caption = "Measurement invariance across factor analysis and item response theory.",
  landscape = TRUE)
```

## IRT Calibration and Model Selection

```{r model-selection-table, echo=FALSE, results='asis', message=FALSE, warning=FALSE}

# -- Ensure scores are available (first-run data were created earlier in the Rmd) --
if (!exists("all_scores")) {
  sco_path <- here::here("02_scoring_outputs","scores","scores_combined.rds")
  if (file.exists(sco_path)) {
    all_scores <- readRDS(sco_path)
  } else {
    stop("Missing scores file: ", sco_path,
         "\nRun `git lfs pull` in the repo and/or knit the full Rmd so earlier chunks create `all_scores`.")
  }
}
# -- Load marginal reliability summary if present (used as fallback for selection) --
task_rxx_path <- here::here("02_scoring_outputs","task_rxx.rds")
if (file.exists(task_rxx_path)) {
  task_rxx <- tryCatch(
    readRDS(task_rxx_path),
    error = function(e) {
      if (requireNamespace("readr", quietly = TRUE)) {
        readr::read_rds(task_rxx_path)
      } else stop("Could not read task_rxx.rds; install.packages('readr') or ensure it's a valid RDS.")
    }
  )
} else {
  task_rxx <- tibble(task = character(),
                     site = character(),
                     itemtype = character(),   # expected values: "rasch" or "2pl"
                     invariance = character(), # e.g., "scalar", "metric"
                     rxx = numeric())
}
# -- Site labels should be defined earlier; if not, define a minimal default --
if (!exists("site_labels")) {
  site_labels <- c("pilot_western_ca" = "Canada",
                   "pilot_uniandes_co" = "Colombia",
                   "pilot_leuphana_de" = "Germany")
}
# -- Prefer explicit model selection export when available; otherwise infer from reliability --
sel_path <- here::here("04_paper","display","model_selection.csv")
has_sel  <- file.exists(sel_path)
has_rxx  <- nrow(task_rxx) > 0
if (has_sel) {
  # Expect columns: task, site, selected_model (e.g., "Rasch" or "2PL"), optionally BIC, delta_bic
  model_sel <- read_csv(sel_path, show_col_types = FALSE) |>
    mutate(site_label = recode(site, !!!site_labels),
           Model = selected_model)
} else if (has_rxx) {
  # Use the model with highest average reliability per task/site as a descriptive proxy
  model_sel <- task_rxx |>
    mutate(site_label = recode(site, !!!site_labels),
           Model = case_match(itemtype,
                              "rasch" ~ "Rasch",
                              "2pl"   ~ "2PL",
                              .default = toupper(itemtype))) |>
    group_by(task, site_label) |>
    slice_max(order_by = rxx, n = 1, with_ties = FALSE) |>
    ungroup()
} else {
  stop("Neither `04_paper/display/model_selection.csv` nor `02_scoring_outputs/task_rxx.rds` is available.\n",
       "Export a model selection summary or generate the reliability file and re-knit.")
}
# -- Collapse to one model per task (most common across sites) --
per_task_model <- model_sel |>
  count(task, Model, name = "n_sites") |>
  group_by(task) |>
  slice_max(order_by = n_sites, n = 1, with_ties = FALSE) |>
  ungroup()
# -- Attach mean rxx, if available, for context --
if (has_rxx) {
  rxx_table <- task_rxx |>
    mutate(site_label = recode(site, !!!site_labels),
           Model = case_match(itemtype, "rasch" ~ "Rasch", "2pl" ~ "2PL", .default = toupper(itemtype))) |>
    select(task, Model, site_label, rxx) |>
    group_by(task, Model) |>
    summarize(`Mean reliability (rxx)` = mean(rxx, na.rm = TRUE), .groups = "drop")

  display_tbl <- per_task_model |>
    left_join(rxx_table, by = c("task","Model")) |>
    arrange(task)
} else {
  display_tbl <- per_task_model |>
    mutate(`Mean reliability (rxx)` = NA_real_) |>
    arrange(task)
}
# -- Render APA-style table --
papaja::apa_table(
  display_tbl,
  caption = if (has_sel)
    "Selected calibration model per task with mean marginal reliability across sites (from model_selection.csv)."
  else
    "Inferred calibration model per task (highest average reliability across sites) with mean marginal reliability.",
  note = if (has_sel)
    "Model selection followed a BIC rule: 2PL preferred when ΔBIC ≤ −10; otherwise Rasch selected for parsimony."
  else
    "When a formal model-selection export was not available, the model with highest average rxx served as a descriptive proxy."
)
if (knitr::is_latex_output()) {
  papaja::apa_table(
    display_tbl,
    caption = "Inferred calibration model per task (highest average reliability across sites) with mean marginal reliability.",
    note = "When a formal model-selection export was not available, the model with highest average rxx served as a descriptive proxy."
  )
} else {
  knitr::kable(
    display_tbl,
    caption = "Inferred calibration model per task (highest average reliability across sites) with mean marginal reliability."
  )
}
```
Across the set of LEVANTE core tasks, model calibration indicated that the Rasch model generally provided an adequate description of item responses. For the majority of tasks, item discriminations did not vary substantially, and the more parsimonious one-parameter specification was sufficient. In contrast, the Shape Rotation and Same–Different Selection tasks were better fit by the two-parameter logistic model, reflecting heterogeneity in how strongly items differentiated between children with higher and lower ability.

Estimates of marginal reliability further highlighted differences across tasks. Math, Matrix Reasoning, the Multigroup composite, Shape Rotation, and Same–Different Selection all achieved high reliability (rxx values above .80), suggesting that these measures provide stable and precise estimates of ability even in the pilot data. Vocabulary and Sentence Understanding (TROG) showed moderate reliability, consistent with tasks that are informative but may benefit from additional refinement. Two tasks—Hearts and Flowers and Theory of Mind—showed low reliability, indicating that the current implementations are limited in the amount of psychometric information they provide.

Taken together, these findings suggest that Rasch calibration is sufficient for most tasks, while allowing variable discriminations improves model fit for more heterogeneous tasks. The relatively high reliability of several tasks indicates their readiness for use in adaptive testing, whereas tasks with lower reliability will require further development and item revision before they can support valid cross-site comparisons.

## Multigroup Calibration and Measurement Invariance 

To evaluate the comparability of task scores across sites, we estimated a sequence of multigroup IRT models in which increasingly restrictive constraints were applied to item parameters. At the configural level, the same factor structure was specified in each group, but parameters were freely estimated [bd edit: i'm finding myself a little confused here. to the extent that we are running UD models, what would this mean?]. This model served as a baseline for subsequent comparisons. Moving to the metric level, item discriminations were constrained to equality across groups, while difficulties remained free. At the scalar level, both discriminations and difficulties were constrained to equality, imposing the strongest form of invariance.

Model comparisons were conducted using differences in the Bayesian Information Criterion ($\Delta$BIC), supplemented by likelihood ratio tests where applicable. Scalar invariance was retained when $\Delta$BIC values indicated no meaningful loss of fit relative to less constrained models. In such cases, items functioned equivalently across sites, supporting the use of a single scoring scale. When only metric invariance was supported, items showed comparable slopes across groups but differed in their difficulty estimates, suggesting that cross-site comparisons are possible but may require adjustment for baseline differences. In cases where only configural invariance held, tasks exhibited group-specific patterns of item functioning, highlighting the need for further adaptation and validation before pooled analyses can be conducted.

```{r invariance-results, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}

inv_path <- here::here("04_paper","display","invariance.csv")
stopifnot(file.exists(inv_path))

inv <- read_csv(inv_path, show_col_types = FALSE) |>
  mutate(across(everything(), ~ tidyr::replace_na(.x, "")))

# Pick a column to read the invariance level from, if available
lvl_candidates <- intersect(c("Level", "Invariance"), names(inv))

if (length(lvl_candidates) > 0) {
  # Use the first matching column for level extraction
  lvl_col <- lvl_candidates[1]
  inv_levels <- inv |>
    mutate(Level_norm_raw = tolower(.data[[lvl_col]]))
} else if ("Interpretation" %in% names(inv)) {
  # Fall back to Interpretation text
  inv_levels <- inv |>
    mutate(Level_norm_raw = tolower(.data[["Interpretation"]]))
} else {
  # If neither exists, mark as unspecified
  inv_levels <- inv |>
    mutate(Level_norm_raw = "unspecified")
}

inv_levels <- inv_levels |>
  mutate(Level_norm = case_when(
    str_detect(Level_norm_raw, "scalar")    ~ "Scalar",
    str_detect(Level_norm_raw, "metric")    ~ "Metric",
    str_detect(Level_norm_raw, "configur")  ~ "Configural",
    TRUE                                    ~ "Unspecified"
  ))

# Summary counts by invariance level
sum_counts <- inv_levels |>
  count(Level_norm, name = "Number of tasks") |>
  mutate(Level_norm = factor(Level_norm, levels = c("Scalar","Metric","Configural","Unspecified"))) |>
  arrange(Level_norm)

papaja::apa_table(
  sum_counts |>
    rename(`Invariance level` = Level_norm),
  caption = "Summary of multigroup invariance outcomes across LEVANTE tasks."
)

# Detailed mapping table 
papaja::apa_table(
  inv |>
    select(any_of(c("Goal", "LEVANTE name", "Factor analysis name",
                    "IRT explanation", "Interpretation"))),
  align   = c("m{2.5cm}", "m{3cm}", "m{5cm}", "m{5cm}", "m{5cm}"),
  caption = "Measurement invariance mapping across factor analysis and IRT. Scalar invariance supports direct cross-site score comparability; metric and configural indicate progressively weaker comparability."
)
```
The table summarizes the correspondence between factor-analytic definitions of invariance and their IRT counterparts. Configural invariance indicates that the same overall construct is measured across groups, though item parameters may differ, reflecting non-uniform DIF. Metric invariance constrains item discriminations to equality, representing uniform DIF: items discriminate equally well across groups, though their difficulties may vary. Scalar invariance adds equality of item difficulties, eliminating DIF altogether and supporting direct comparability of latent scores across groups. More restrictive forms such as strict invariance, requiring equality of residual variances, are not estimable in binary IRT models. Once scalar invariance is established, differences in group means and variances can be interpreted on a common scale.

Taken together, these definitions provide the basis for evaluating whether the LEVANTE tasks function equivalently across languages and contexts. Evidence for scalar invariance in several tasks supports direct cross-site comparisons, whereas tasks showing only metric or configural invariance highlight areas for further adaptation and refinement.


## Item-Level Diagnostics

In addition to overall model calibration, we examined item-level performance to identify potential weaknesses in the pilot instruments. Classical item statistics included the proportion correct and point-biserial item–total correlations. Items with very low or very high proportions correct (below .20 or above .90) were flagged as potentially uninformative, and items with point-biserial correlations below .20 were flagged as weakly discriminating. Complementary IRT-based diagnostics included inspection of item infit and outfit statistics, although these results are not displayed here. Together, these indices provide a first pass at identifying problematic items that may warrant revision, removal, or further testing in subsequent piloting rounds.

.... DORIA stopped here ....

Classical indices included proportion correct (flagged if <0.20 or >0.90) and point-biserial correlations (flagged if <0.20). 
IRT-based fit statistics included infit and outfit mean squares.
DIF
Removal and Revision of Problematic Items
Ability Estimation
CAT-Specific Scoring

Multigroup models and invariance
Model selection approach
1PL/2PL and different degrees of invariance
Item-level diagnostics
Removal of outlier items
Tests for item-level DIF in cases where we use multigroup scoring

# Psychometric properties of tasks

## Developmental change

```{r, fig.width = 8, fig.height = 8}
ggplot(coretask_scores, aes(x = age, y = metric_value)) +
    ggh4x::facet_nested_wrap(vars(task_category, task),
                             nest_line = element_line(), solo_line = TRUE,
                             axes = "x",
                             scales = "free_y") +
    geom_smooth(aes(group = site_label, color = site_label),
                method = "gam", formula = y ~ s(x, bs = "re")) +
    geom_point(aes(colour = site_label), alpha = 0.3) +
    scale_x_continuous(breaks = seq(6, 14, 2)) +
    .scale_colour_site() +
    guides(color = guide_legend(override.aes = list(fill = "white"))) +
    labs(x = "Age (years)", y = "Ability (IRT score)", colour = "Site") +
    theme(legend.position = "bottom")
```

## Reliability 

### Marginal reliability estimates

```{r}
task_rxx <- read_rds(here("02_scoring_outputs", "task_rxx.rds"))

task_map <- all_scores |> distinct(item_task, task)

task_rxx_table <- task_rxx |>
  rename(item_task = task) |>
  left_join(task_map) |>
  mutate(site_label = site |> fct_recode(!!!site_labels),
         itemtype = itemtype |> fct_recode("Rasch" = "rasch", "2PL" = "2pl"),
         invariance = invariance |> str_to_title()) |>
         # model_label = paste(itemtype, invariance) |> str_trim()) |>
  select(task, itemtype, invariance, site_label, rxx) |>
  pivot_wider(names_from = site_label, values_from = rxx) |>
  # mutate(across(everything(), \(v) v |> replace_na("–"))) |>
  arrange(task)

options(papaja.na_string = "–")
task_rxx_table |>
  rename(Task = task, Parameters = itemtype, Invariance = invariance) |>
  papaja::apa_table(col_spanners = list("Marginal reliability" = c(4, 6)))
options(papaja.na_string = "NA")
```

### Test-retest
<!-- > ### Test-retest [bd edit: can we put correlations in each panel?  ] -->


```{r, output="asis"}
de_retest_scores <- coretask_scores |> 
  filter(site == "pilot_leuphana_de") |>
  group_by(user_id, task) |>
  arrange(user_id, task, age) |>
  filter(n() > 1) 

retest_wide <- de_retest_scores |>
  group_by(user_id, task) |>
  mutate(has_retest = any(age_gap > .05), 
         mean_age = mean(age), 
         age_gap = max(age_gap)) |>
  filter(has_retest) |>
  ungroup() |>  
  pivot_wider(
    id_cols = c(user_id, task_category, task, mean_age, age_gap),
    names_from = run_number,
    values_from = metric_value,
    names_prefix = "run_"
  ) |>
  select(user_id, task_category, task, age = mean_age, age_gap, run_1, run_2)

trt <- retest_wide |>
  group_by(task) |>
  summarise(test_retest_r = cor(run_1, run_2, use = "complete.obs"), 
            n = n(), 
            age_gap = mean(age_gap)*12) |>
  rename(`Task` = task,
         `r` = test_retest_r, 
         `N` = n, 
         `Retest interval (months)` = age_gap)

papaja::apa_table(trt, digits = 2, caption = "Task-wise test-retest correlations computed for Germany data.")
```

```{r, fig.width = 8, fig.height = 8}
ggplot(retest_wide, aes(x = run_1, y = run_2, color = age)) +
  ggh4x::facet_nested_wrap(vars(task_category, task),
                         nest_line = element_line(), solo_line = TRUE,
                         axes = "x",
                         scales = "free_y") +
    # facet_wrap(vars(task)) +
  geom_point() +
  geom_smooth(aes(group = 1), color = "darkgrey", method = "lm") +
  scale_color_viridis(limits = c(5, 12)) +
  labs(x = "Score 1", y = "Score 2", color = "Age (years)") +
  theme(legend.position = "bottom")
```

## Validity 
### Construct validity

SEMs within each construct for language, number, EF?

```{r prep-sem, include=FALSE, cache=TRUE}

# one row per user (metadata + task scores from first run)
meta <- coretask_firstrun_scores %>%
  group_by(user_id) %>%
  summarise(
    age        = min(age, na.rm = TRUE),
    site       = dplyr::first(na.omit(site)),
    site_label = dplyr::first(na.omit(site_label)),
    dataset    = dplyr::first(na.omit(dataset)),
    .groups = "drop"
  )

# wide task scores (first run only)
wide_scores <- coretask_firstrun_scores %>%
  select(user_id, item_task, metric_value) %>%
  distinct() %>%  # guard against accidental duplicates
  tidyr::pivot_wider(
    names_from  = item_task,
    values_from = metric_value
  ) %>%
  left_join(meta, by = "user_id") %>%
  relocate(user_id, site, site_label, dataset, age)

# keep only the core task columns + metadata
vars_core <- c("math","matrix","mrot","sds","hf","mg","trog","vocab","tom")
wide_scores <- wide_scores %>%
  select(user_id, site, site_label, dataset, age, any_of(vars_core))

# standardise
z <- function(x) (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
vars_core_std <- wide_scores %>%
  mutate(across(all_of(vars_core), ~ z(as.numeric(.x))))
str(vars_core_std[vars_core])

```


```{r semplot, echo = FALSE, cache = TRUE, fig.width = 6, fig.height = 6}
library(lavaan)
library(semPlot)

cfa_1 <- '
  reasoning =~ matrix + mrot
  ef        =~ hf + mg + sds + math
  language  =~ trog + vocab
  social    =~ 1*tom
  tom ~~ (1 - 0.75)*tom   #(residual variance = 0.25)
'

fit_1 <- cfa(cfa_1, data = vars_core_std,
             estimator = "MLR", std.lv = TRUE, missing = "fiml")
# print(summary(fit_1, fit.measures = TRUE, standardized = TRUE))
# head(lavaan::modindices(fit_1, sort.=TRUE, minimum.value=10), 15)

```

```{r fig-semplot-main, echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=6, dpi=300, fig.align='center'}

semPlot::semPaths(
  fit_1,
  whatLabels = "std",
  residuals  = FALSE,
  intercepts = FALSE,
  layout     = "tree",
  # text sizes
  label.cex       = 1.2,   
  edge.label.cex  = 1,   
  sizeMan = 7,
  sizeLat = 7,
  nCharNodes = 0,          
  mar = c(6,6,6,6)       
)
```

factors are very highly correlated - try with g factor

```{r second fit, echo = FALSE, cache = TRUE}
model_g <- '
  reasoning =~ matrix + mrot
  ef        =~ hf + mg + sds + math
  language  =~ trog + vocab
  g =~ reasoning + ef + language'

fit_g <- cfa(model_g, data = vars_core_std,
             estimator = "MLR", std.lv = TRUE, missing = "fiml")
# print(summary(fit_g, fit.measures = TRUE, standardized = TRUE))
# head(lavaan::modindices(fit_g, sort.=TRUE, minimum.value=10), 15)
```

```{r fig-semplot-g, echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=6, dpi=300, fig.align='center'}

semPlot::semPaths(
  fit_g,
  whatLabels = "std",
  residuals  = FALSE,
  intercepts = FALSE,
  layout     = "tree",
  # text sizes
  label.cex       = 1.2,   
  edge.label.cex  = .8,   
  sizeMan = 7,
  sizeLat = 7,
  nCharNodes = 0,          
  mar = c(6,6,6,6)       
)
```

### External validity

We next assessed external validity by examining correlations with other measures. In particular, we used ...

MEFS and ROAR

Executive Function

```{r prep for sem - ef, echo = FALSE, cache = TRUE}

ef_tasks <- c("hf","mg","sds","mefs")

# Filter to ef tasks (from included_scores), keep first run 
ef_firstrun_scores <- included_scores %>%
  filter(item_task %in% ef_tasks) %>%
  arrange(user_id, item_task, run_number, time_started) %>%
  group_by(user_id, item_task) %>%
  slice(1) %>%
  ungroup()

# one row per user
meta_ef <- ef_firstrun_scores %>%
  group_by(user_id) %>%
  summarise(
    age        = min(age, na.rm = TRUE),
    site       = dplyr::first(na.omit(site)),
    site_label = dplyr::first(na.omit(site_label)),
    dataset    = dplyr::first(na.omit(dataset)),
    .groups = "drop"
  )

# Wide ef task scores (first run only)
ef_wide_scores <- ef_firstrun_scores %>%
  select(user_id, item_task, metric_value) %>%
  distinct() %>%  # guard against accidental duplicates
  pivot_wider(
    names_from  = item_task,
    values_from = metric_value
  ) %>%
  left_join(meta_ef, by = "user_id") %>%
  relocate(user_id, site, site_label, dataset, age)

# keep only ef columns + metadata
vars_ef <- ef_tasks
ef_wide_scores <- ef_wide_scores %>%
  select(user_id, site, site_label, dataset, age, any_of(vars_ef))

# Standardise ef indicators (same z() you used)
z <- function(x) (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
ef_wide_std <- ef_wide_scores %>%
  mutate(across(all_of(vars_ef), ~ z(as.numeric(.x))))

# str(ef_wide_std[vars_ef])
```

```{r sem fit, echo = FALSE, cache = TRUE}
cfa_ef <-  '
executive_function =~ hf + mg + sds + mefs
executive_function ~ age
'

fit_ef <- cfa(cfa_ef, data = ef_wide_std,
              estimator = "MLR", std.lv = TRUE, missing = "fiml")
# summary(fit_ef, fit.measures = TRUE, standardized = TRUE)
# print(fitMeasures(fit_ef, c("cfi","tli","rmsea","srmr")))

# plot
layout_ef <- matrix(
  byrow = TRUE, nrow = 3,
  data = c(
    "hf","mg","sds","mefs", NA,  "age",
     NA,  NA,   NA,   NA,   NA,  NA,
     NA,  NA, "executive_function", NA, NA, NA
  )
)
```

```{r fig-semplot-ef, echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=6, dpi=300, fig.align='center'}

tidySEM::graph_sem(
  model = fit_ef,
  layout = layout_ef,
  text_size = 3,
  edge_label = "std"
)
```

Language

```{r prep for sem, echo = FALSE, cache = TRUE}

lang_vars <- c("swr","sre","pa","vocab","trog")

# first run per user–task
lang_firstrun_scores <- included_scores %>%
  filter(item_task %in% lang_vars) %>%
  arrange(user_id, item_task, run_number, time_started) %>%
  group_by(user_id, item_task) %>%
  slice(1) %>%
  ungroup()

meta_lang <- lang_firstrun_scores %>%
  group_by(user_id) %>%
  summarise(
    age        = min(age, na.rm = TRUE),
    site       = dplyr::first(na.omit(site)),
    site_label = dplyr::first(na.omit(site_label)),
    dataset    = dplyr::first(na.omit(dataset)),
    .groups = "drop"
  )

# create wide df 
lang_wide_scores <- lang_firstrun_scores %>%
  select(user_id, item_task, metric_value) %>%
  distinct() %>%
  pivot_wider(
    names_from  = item_task,
    values_from = metric_value
  ) %>%
  left_join(meta_lang, by = "user_id") %>%
  relocate(user_id, site, site_label, dataset, age)

# keep only language columns + metadata
lang_wide_scores <- lang_wide_scores %>%
  select(user_id, site, site_label, dataset, age, any_of(lang_vars))

# standardise 
z <- function(x) (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
lang_wide_std <- lang_wide_scores %>%
  mutate(across(all_of(intersect(lang_vars, names(.))), ~ z(as.numeric(.x))))
```

```{r sem fit - lang, echo = FALSE, cache = TRUE}
# model
cfa_lang <-  '
Language =~ swr + sre + pa + trog + vocab
Language ~ age
'

fit_lang <- cfa(cfa_lang, data = lang_wide_std,
                estimator = "MLR", std.lv = TRUE, missing = "fiml")
# print(fitMeasures(fit_lang, c("cfi","tli","rmsea","srmr")))

layout_lang <- matrix(
  byrow = TRUE, nrow = 3,
  data = c(
    "swr","sre","pa","trog","vocab", NA,  "age",
    NA,    NA,   NA,  NA, NA, NA, NA,
    NA,   "Language", NA,  NA,    NA, NA,  NA
  )
)
```

```{r fig-semplot-lang, echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=6, dpi=300, fig.align='center'}

tidySEM::graph_sem(
  model      = fit_lang,
  layout     = layout_lang,
  text_size  = 3,
  edge_label = "std",                 
  show       = c("paths","loadings","residuals")
)

```

# Adaptive task construction

```{r}
task_time_summary <- read_csv(here("03_summaries/tables/task_time_summary.csv"))

task_time_table <- task_time_summary |>
  filter(!is.na(is_cat)) |>
  filter(item_task %in% core_tasks) |>
  mutate(site_label = site |> fct_recode(!!!site_labels),
         adaptive = if_else(is_cat, "Adaptive", "Non-adaptive")) |>
  left_join(task_map) |>
  select(task, site_label, adaptive, median_diff) |>
  pivot_wider(names_from = c(adaptive, site_label), values_from = median_diff) |>
  select(task, starts_with("Non"), , starts_with("Adaptive")) |>
  arrange(task)

options(papaja.na_string = "–")
task_time_table |>
  papaja::apa_table(
    col.names = c("Task", rep(c("Germany", "Colombia", "Canada"), 2)),
    col_spanners = list("Non-adaptive" = c(2, 4), "Adaptive" = c(5, 7)),
    align = c("l", rep("r", 6)),
    caption = "Median task durations (minutes).")
options(papaja.na_string = "NA")
```


<!-- [bd edit: are we planning to actually specify the models we're using somewhere? would clarify meaning here (and be generally helpful)] -->
Many of the LEVANTE tasks have been adapted and piloted as CATs (Computerized Adaptive Tests). To date, these include the Test For Reception of Grammar (TROG), Vocabulary, Shape Rotation, Matrix Reasoning, Same Difference Selection, and Math. These tasks maintain an ability score, theta, as an estimate of the participant’s skill level that is updated at the end of each trial. They then present participants with the item best suited to their estimated ability, which both improves test-taker experience and yields more information on participant skill per item, allowing for a shorter task with fewer items. 

<!-- [bd edit: i'd cut this bit about the 4 params esp given that we haven't really talked much about item params above in the general case] -->
 The CAT tasks use an adaptive algorithm made available by the jsCat JavaScript library [@ma2025], which offers an implementation of an Item Response Theory (IRT) model including up to 4 parameters: discrimination, a value representing the item’s informativeness in distinguishing high and low ability test-takers, guessing, the probability of selecting the correct response at random, upper asymptote, the maximum likelihood of a correct answer, and difficulty. The present LEVANTE CAT implementation varies difficulty and guessing for each item while holding both discrimination and upper asymptote constant at 1. Items are selected based on Maximum Fisher Information, and theta is updated according to a maximum likelihood estimator, with limits of -6 and 6. 

The LEVANTE CATs are configurable with respect to the initial value of theta and the conditions for ending the task. The starting theta is set at 0 for all CAT tasks currently in use, but can be lowered or raised according to the researcher’s prior expectation of participant ability, for example according to age. Current CAT implementations use stopping rules based on either time or number of items. TROG, Shape Rotation, and Vocabulary each have time limits currently set to 4 minutes, with Matrix Reasoning set to 6 minutes to allow for the increased time typically required to complete items in this task. Items in these tasks are presented together in a single block. Same Difference Selection and Math are each divided into three blocks presented sequentially, with per-block stopping based on number of items. These CATs select from the list of items specific to their current block and proceed to the next block once the target number of items is reached, maintaining one overarching ability estimate for the entire task. Same Difference Selection and Math have time limits of 6 minutes and 8 minutes, respectively. 



# Discussion

* Plans for internationalization
* Plans for downward extension
* Plans for updates



\newpage

# References

<!-- ::: {#refs custom-style="Bibliography"} -->
<!-- ::: -->
