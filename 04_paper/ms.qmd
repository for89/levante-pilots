---
title: "Creation and validation of the LEVANTE core tasks: Internationalized measures of learning and development for children ages 5-12 years"
shorttitle: "LEVANTE Measures"

author: 
  - name: "George Kachergis*"
    affiliations:
      - ref: stanford
  - name: "Fionnuala O'Reilly*"
    affiliations:
      - ref: stanford
  - name: "Mika Braginsky"
    orcid: 0000-0001-9039-3220
    affiliations:
      - ref: stanford
  - name: "Amy Lightbody"
    affiliations:
      - ref: stanford
  - name: "Katherine Adams Shannon"
    affiliations:
      - ref: stanford
  - name: "Zachary Watson"
    affiliations:
      - ref: stanford
  - name: "Xingyao Xiao"
    affiliations:
      - ref: stanford
  - name: "Lijin Zhang"
    affiliations:
      - ref: stanford
  - name: "Rebecca Zhu"
    affiliations:
      - ref: stanford
  - name: "Anya Wanjing Ma"
    affiliations:
      - ref: stanford
  - name: "Bria Long"
    affiliations:
      - ref: ucsd
  - name: "Tonya Murray"
    affiliations:
      - ref: stanford
  - name: "Jason Yeatman"
    affiliations:
      - ref: stanford
  - name: "Michael Sulik"
    affiliations:
      - ref: stanford
  - name: "Jelena Obradović"
    affiliations:
      - ref: stanford
  - name: "Nichola Jenkins"
    affiliations:
      - ref: western
  - name: "Daniel Ansari"
    affiliations:
      - ref: western
  - name: "Maria Camilla Perfetti"
    affiliations:
      - ref: uniandes
  - name: "Julian Mariño"
    affiliations:
      - ref: uniandes
  - name: "Luise Hornoff"
    affiliations:
      - ref: mpieva
  - name: "Manuel Bohn"
    affiliations:
      - ref: leuphana
  - name: "Daniel Haun"
    affiliations:
      - ref: leuphana
  - name: "Nilam Ram"
    affiliations:
      - ref: stanford
  - name: "Benjamin W. Domingue"
    affiliations:
      - ref: stanford
  - name: "Michael C. Frank"
    orcid: 0000-0002-7551-4378
    affiliations:
      - ref: stanford
    corresponding: true
    email: mcfrank@stanford.edu
    # role:
    #   - conceptualization
    #   - writing
    #   - editing
    #   - supervision

affiliations:
  - id: stanford
    name: "Stanford University"
  - id: ucsd
    name: "University of California, San Diego"
  - id: western
    name: "Western University"
  - id: uniandes
    name: "Universidad de los Andes"
  - id: mpieva
    name: "Max Planck Institute for Evolutionary Anthropology"
  - id: leuphana
    name: "Leuphana University"

author-note:
  disclosures:
    financial-support: "This work was supported by the Jacobs Foundation."
    # conflict-of-interest: The author has no conflict of interest to declare.

abstract: We present the Learning Variability Network Exchange (LEVANTE) core tasks, a set of nine short and engaging computer adaptive tasks designed to assess learning and development in children ages 5--12 years across a wide range of languages and cultures. Using a simple and uniform multi-alternative forced choice format, these tasks measure constructs including math, execxutive function, reasoning, and social cognition and can be administered on a tablet or computer both in person or remotely. We describe the design and selection of these tasks, and then report on their reliability and validity in a sample of 1034 children recruited from sites in Colombia, Germany, and Canada. Tasks are scored using multi-group item response theory models, allowing testing for measurement invariance. The parameters of these models can then be used to create computer adaptive versions of the tasks, allowing the entire battery to be given in around an hour. We discuss the use,  ongoing refinement, and extension of these tasks in the service of creating an open dataset to describe variability in children's development and learning across contexts.

keywords: ["cognitive development"]
word-count: true

bibliography: library.bib

floatsintext: true
numbered-lines: true
# draft: false
mask: false

# figurelist: no
# tablelist: no
# footnotelist: no

format:
  apaquarto-pdf:
    documentmode: man
    keep-tex: true
    include-in-header: preamble.tex
    fig-pos: H        
    tbl-pos: H  

execute: 
  echo: false
  message: false
  warning: false
  error: true
  cache: false
---

```{r setup}
#| include: false
#| cache: false

library(tidyverse)
library(here)
library(glue)
library(purrr)
library(viridis)
library(flextable)

library(lavaan)
library(mirt)
library(broom.mixed)
library(lmerTest)
library(png)
library(emmeans)

source(here("03_summaries","plotting_helper.R"))
source(here("plot_settings.R"))
```

```{r settings}
#| cache: false

site_labels <- c("uniandes-co" = "pilot_uniandes_co",
                 "mpieva-de" = "pilot_mpieva_de",
                 "western-ca" = "pilot_western_ca")
core_tasks   <- c("matrix","mrot","math","hf","mg","sds",
                  "trog","vocab","tom")

sites <- names(site_labels)
site_pal <- solarized_pal()(length(sites)) |> rlang::set_names(sites)

set_flextable_defaults(theme_fun = theme_apa, arraystretch = 1, padding = 1)

# seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```


# Introduction

Developmental variability and change during childhood is a focus of intense theoretical and practical interest. From tracking children's growth over time to evaluating intervention outcomes and exploring environmental and contextual moderators, a wide range of scientific goals require accurate assessments. Ideal psychological measures provide efficient, reliable, and valid measures of particular constructs that can be applied across a range of ages, situations, and contexts. Yet, in most cases, a large gap separates the situation of a researcher searching for measures from this ideal. 

Because children's overall capacities are so dependent on their age, it can be very challenging to use the same measure across children of different ages. Young children require simple tasks that are not verbally demanding, while older children can answer more complicated questions. In addition, younger children typically require shorter tasks, often reducing measurement reliability. Yet giving different tasks to different ages can mean that scores are not comparable to one another, making tracking developmental growth challenging in many domains.  

A second set of challenges concern cross-context comparisons.  Ideal developmental measures should be validated in a global context and applicable to children across many cultures and languages. Child development is an issue of global importance [@grantham2007developmental;@masten2014global;@aboud2015global], yet the vast majority of developmental measures are developed in very specific (often English-speaking) contexts [@kidd2022diverse;@singh2023diversity;@richter2019early]. 

A final set of challenges has to do with accessibility. Many gold-standard measures are commercially distributed. They are costly for researchers to use, and in addition, publishers may place barriers on new translation and adaptation. Publishers also typically hold both item information and normative data closely, blocking many types of secondary investigation. 

## LEVANTE

The Learning Variability Network Exchange (LEVANTE) was designed to address these challenges [@frank2025]. LEVANTE provides a software framework for data collection: researchers can use the LEVANTE dashboard to assign both surveys and tasks to children, caregivers, and teachers. Data collected via the LEVANTE dashboard are harmonized and validated and become accessible through a data repository, first to the researchers who collect them and then, through regular data releases, to the broader research community. Through a partnership with the Jacobs Foundation, sites around the world are funded to collect longitudinal data from children using the LEVANTE framework. The eventual goal of LEVANTE is to create a rich dataset documenting children's learning and development across contexts. 

The current manuscript introduces the LEVANTE core tasks, a suite of behavioral measures for children. In our initial development of the task battery, we cast a broad net for important constructs in child development with well-accepted measures that had been used internationally. This process is described in @frank2025. The broad constructs that we selected were executive function, language, mathematics, reasoning, and social cognition, with these being instantiated through a number of well-accepted tasks. 

To create our core tasks, we selected pre-existing measures from the literature that tapped each of these constructs. When possible, we prioritized measures with strong psychometric properties, previous use across a broad range of cultures, applicability across a broad range of ages, and lack of commercial or licensing constraints. This process yielded a series of candidate tasks, which we adapted and re-implemented in an open source web platform. @tbl-tasks shows these tasks, organized by construct. Some of these implementations remained quite close to prior versions, while others by necessity required the creation of additional test items or changes to specifics of their procedure. 

In addition to the constructs described above, we were also interested in the assessment of literacy. The LEVANTE core tasks battery makes use of a number of previously-validated literacy tasks from the Rapid Online Assessment of Reading [@yeatman2021], including single word reading, sentence reading efficiency, and phonological awareness. We do not report on these tasks extensively here, though we make use of literacy data for validation of language measures. Similarly, we included a commercially-available broad measure of executive function, the Minnesota Executive Function Scale (MEFS) [@carlson2021], for validation purposes. 
 
::: {.landscape}
| Construct          | LEVANTE task name      | Prior/Source task name               | Adaptive? | Reference |
|:-------------------|:-----------------------|:-------------------------------------|:----------|:----------|
| Executive Function | Hearts and Flowers     | Hearts and Flowers                   | no        | XYZ       |
|                    | Memory                 | Corsi Block Task                     | yes       |           |
|                    | Same and Different     | Same Different Selection Task        | yes       |           |
| Language           | Vocabulary             | Picture Vocabulary                   | yes       |           |
|                    | Sentence Understanding | Test for Reception of Grammar (TROG) | yes       |           |
| Math               | Math                   | Early Grades Math Assessment (EGMA)  | yes       |           |
| Reasoning          | Pattern Matching       | Matrix Reasoning                     | yes       |           |
|                    | Shape Rotation         | Mental Rotation                      | yes       |           |
| Social Cognition   | Stories                | Theory of Mind                       | yes       |           |

: The LEVANTE core tasks, presented with their internal label as well as prior labels used in the literature. {#tbl-tasks}
:::

## The current study

Here we report on the development and validation of the LEVANTE core tasks. This is an iterative process in which data from 5--12 year old children has been collected across three sites: Bogota, Colombia; Leipzig, Germany; and Ottawa, Canada.^[In the remainder of the manuscript, we refer to these sites by their site designators, `uniandes-co`, `mpieva-de`, and `western-ca`. These designators mark the research group and country of data collection. We make this choice to avoid the inference that these data are particularly representative of, for example, German or Colombian children more broadly.] In some cases that we note below, we used these data during the data collection process to make minor changes to the tasks. We use data from these three pilot sites both to provide initial evidence on the reliability and validity of the measures and to develop efficient, computer adaptive (CAT) versions of nearly all of the tasks. 

A key component of this process is the use of psychometric models based on item-response theory (IRT) [@embretson2013]. IRT models provide a family of models that allow the joint estimation of the difficulty of individual task items (e.g., math questions) and the ability of individual children. A fitted IRT model provides task parameters that can be used to estimate the ability of a new test taker given their responses on some or all of the same items. In addition, IRT parameters are used in the construction of CATs, which choose the most relevant items to give to estimate the ability of a particular individual. Critically for our purposes, the use of IRT models means that we can provide comparable scores on the same scale to a younger child who saw mostly easier task items and an older child who saw harder items; these models thus allow us to address our first key challenge posed above.  

Because our data come from three sites, each with their own translations and adaptations of the specific tasks, we can also use multi-group IRT models to explore the question of invariance: whether measures function similarly across different groups [@putnick2016]. While measurement invariance is more commonly discussed in the factor analytic literature, it is also applicable to IRT (where it is sometimes analyzed at the level of individual test items as "differential item function" across groups) [@thissen2025]. Here we use multigroup model comparisons (described below) to investigate whether our tasks measure similarly structured constructs across groups. In particular, where possible, we aim for *scalar invariance*, in which individuals from different groups still show the same relative ordering of difficulty across items (e.g., they still find fractions items harder than division items in a math test). In some cases, we may fall back to *metric invariance*, in which items show different difficulties across groups, or *configural invariance*, in which items show different degrees of ability discrimination as well (e.g., if some problem types are unfamiliar to children in one group and so do not discriminate between high and low ability children). These models allow us to begin to address the second challenge posed above.

<!-- https://www.tandfonline.com/doi/10.1080/00273171.2024.2396148 -->

In what follows, we begin by describing the nine LEVANTE core tasks, organized by construct. We then discuss the process of translation and adaptation that produced the Spanish and German versions of these tasks from the original English source. We then discuss our pilot data collection efforts in Colombia, Germany, and Canada. We present our IRT-based scoring techniques and the results of multi-group comparison, in some cases after selectively removing items that show poor performance. Using scores from these analyses, we then present evidence on the reliability and validity of the tasks, recognizing that in many cases these tasks are still under construction and we anticipate increases in reliability as we iteratively improve items. 

We end by discussing future plans for further internationalization and downward extension of the tasks. Critically, LEVANTE embraces open science values, aiming to create measures and data that are permissively licensed and reusable and extensible by the international research community. These values address our final challenge posed above: The aim of LEVANTE is to minimize barriers to reuse, accelerating progress towards a global science of learning and development. 

# The LEVANTE Core Tasks

```{r load-scores}
all_scores <- read_rds(here("02_scoring_outputs","scores","scores_combined.rds")) |>
  group_by(user_id, item_task) |>
  arrange(time_started) |>
  mutate(run_number = 1:n(),
         age_gap = age - age[1]) |>
  ungroup() |>
  mutate(site_label = site |> fct_recode(!!!site_labels)) |>
  mutate(task = task |> str_to_title() |> fct_inorder(),
         task_category = task_category |> str_to_title() |> fct_inorder())

task_map <- all_scores |> distinct(item_task, task)

included_scores <- all_scores |>
  filter(!is.na(age), age >= 5, age <= 12) |>
  filter(run_number == 1 | (run_number == 2 & age_gap > 1/6))

coretask_scores <- included_scores |>
  filter(item_task %in% core_tasks)

coretask_firstrun_scores <- coretask_scores |>
  filter(run_number == 1)
```

```{r time-summary}
# task_time_summary <- read_csv(here("03_summaries/tables/task_time_summary.csv"))
task_time_summary <- read_rds(here("03_summaries","tables",
                                   "task_time_summary.rds"))

task_time_table <- task_time_summary |>
  filter(!is.na(is_cat)) |>
  filter(item_task %in% core_tasks) |>
  mutate(site_label = site |> fct_recode(!!!site_labels),
         adaptive = if_else(is_cat, "Adaptive", "Non-adaptive")) |>
  left_join(task_map) |>
  select(task, site_label, adaptive, median_diff) |>
  pivot_wider(names_from = c(adaptive, site_label), values_from = median_diff) |>
  select(task, starts_with("Non"), starts_with("Adaptive")) |>
  arrange(task)
```

## General task properties

![Screenshots from each of the nine LEVANTE core tasks. Top row: Sentence Understanding, Vocabulary, Hearts and Flowers; middle row: Memory Game, Same and Different Selection, Math; bottom row: Pattern Matching, Mental Rotation; Stories.](display/tasks.png){#fig-tasks}

The LEVANTE core tasks are short behavioral tasks that can be presented in a web browser on a tablet or laptop, with responses possible using a touchscreen, keyboard, or mouse. Because of this variability in format of administration, they focus on response correctness not reaction time and so they are mostly untimed. The tasks are designed for simplicity and clarity so as to be accessible to children across a wide age range, and so with only modest exceptions, nearly all are in the format of a multi-alternative forced choice with a maximum of four choices. This uniformity of format means that in most cases instructions can be short and easy to understand, minimizing delays when the tasks are given in sequence as a battery. @fig-tasks shows screenshots from each of the tasks. Below we briefly present each task.

Tasks were implemented using jsPsych [@de-leeuw2015] in a common framework. Illustrations were produced by hand while auditory stimuli were generated using AI voice synthesis tools (currently, <https://elevenlabs.io>). The tasks generally share a set of usability features including simple instructions, a small number of practice trials, and buttons to replay audio.

All tasks are available for demonstration purposes at <https://researcher.levante-network.org>. Source code for the tasks is available at <https://github.com/levante-framework/core-tasks>. Task code and assets are licensed CC-BY-NC 4.0 for non-commercial reuse (including educational use by not-for-profit and governmental entities) with appropriate attribution. Please see repository license for more details. We anticipate that once the LEVANTE researcher management dashboard is completed, interested researchers will be able to create accounts and use the tasks in their own research. 

General task lengths are described below and average task duration is given in @tbl-durations. Because of our interest in testing children across a wide range of ages, we intentionally included trials that we anticipated would be both very easy and very hard for children with the aim of deploying these adaptively after we had gathered sufficient data. We wanted to avoid  frustration for children, however, so during pilot testing, we experimented with a number of different stopping rules. In early iterations of pilot testing in Bogota, Colombia, we ended tasks after three incorrect trials; we later modified this rule to end tasks after six incorrect trials. 

```{r}
#| label: tbl-durations
#| tbl-cap: "Median task durations (minutes)."
#| cache: false

task_time_table |>
  rename(Task = task) |>
  flextable() |>
  separate_header() |>
  flextable::autofit(add_w = 0, add_h = 0)
```
## General task properties

The LEVANTE core tasks are short behavioral tasks that can be presented in a web browser on a tablet or laptop, with responses possible using a touchscreen, keyboard, or mouse. Because of this variability in format of administration, they focus on response correctness not reaction time and so they are mostly untimed. The tasks are designed for simplicity and clarity so as to be accessible to children across a wide age range, and so with only modest exceptions, nearly all are in the format of a multi-alternative forced choice with a maximum of four choices. This uniformity of format means that in most cases instructions can be short and easy to understand, minimizing delays when the tasks are given in sequence as a battery. fig-tasks shows screenshots from each of the tasks. Below we briefly present each task.

Tasks were implemented using jsPsych [@de-leeuw2015] in a common framework. Illustrations were produced by hand while auditory stimuli were generated using AI voice synthesis tools (currently, <https://elevenlabs.io>). The tasks generally share a set of usability features including simple instructions, a small number of practice trials, and buttons to replay audio.

All tasks are available for demonstration purposes at <https://researcher.levante-network.org>. Source code for the tasks is available at <https://github.com/levante-framework/core-tasks>. Task code and assets are licensed CC-BY-NC 4.0 for non-commercial reuse (including educational use by not-for-profit and governmental entities) with appropriate attribution. Please see repository license for more details. We anticipate that once the LEVANTE researcher management dashboard is completed, interested researchers will be able to create accounts and use the tasks in their own research. 

General task lengths are described below and average task duration is given in @tbl-durations. Because of our interest in testing children across a wide range of ages, we intentionally included trials that we anticipated would be both very easy and very hard for children with the aim of deploying these adaptively after we had gathered sufficient data. We wanted to avoid  frustration for children, however, so during pilot testing, we experimented with a number of different stopping rules. In early iterations of pilot testing in Bogota, Colombia, we ended tasks after three incorrect trials; we later modified this rule to end tasks after six incorrect trials. 

## Language 

### Sentence Understanding

The Test for Reception of Grammar (TROG) [@bishop1982] is a multiple-choice measure of receptive grammatical understanding. On each trial, the child hears a spoken sentence and is asked to select one of four pictures that best matches its meaning. The original test contained 20 blocks, each with four items assessing the same grammatical structure. In our adaptation (based on the original TROG, which was permissively licensed for reuse), we removed a small number of items due to changes in cultural norms. In addition, based on early pilot testing showing that many trials were easy for older children, we added a set of several dozen more challenging sentences. All illustrations were remade with details intended to be accessible across a broad range of cultures. The task had 103 separate items.

### Vocabulary

The Vocabulary task was developed as a non-commercial, open alternative to tasks such as the Peabody Picture Vocabulary [@dunn1965] and the NIH Toolbox Picture Vocabulary Task [@gershon2013]; see @long_ma_tan_silverman_frank_yeatman_2025 for more details. In this task, the child is presented with a word and four pictures. They must select the correct picture over the distractor picture. Targets and distractors were selected from the THINGS dataset [@hebart2019things], specifically from the permissively-licensed subset of the data. Each image had a semantically close and semantically far distractor as well as an unrelated distractor image (e.g., target word "acorn" with close distractor being a coconut, and the far and unrelated distractors being keys and laundry). The task included 170 total items.

## Math

We developed the Math task based on the EGMA (Early Grade Mathematics Assessment) [@platas2014]. This short, paper-and-pencil assessment is widely administered in international contexts for children ages 5--8 and includes number identification, number comparison, missing number, addition, and subtraction sub-tests. In our adaptation, we increased the breadth of the initial item bank, added multiplication, division, and fractions items to extend the age range up. We also added number line identification problems in which children had to place a marker on a number line across a range of different scales (including simpler 0--10 trials and more challenging larger scales as well as a fraction scale). Number line problems of this are strongly related to math ability [@schneider2018]. The primary version of this task included 275 items (though no child saw all of these) and the secondary version for retest purposes included an additional 237 (512 items in total). 

## Reasoning

### Matrix Reasoning

In classic matrix reasoning tasks, participants see a set of abstract figures in a grid format with a missing cell and must choose the most appropriate figure to complete the grid. Matrix reasoning scores are highly correlated with general cognitive ability and educational attainment [e.g., @roth2015]. Here we make use of the Mars-IB matrix reasoning stimuli, an open-source, permissively-licensed set of 80 matrix reasoning problems with retest variants that have been normed with a large sample of adolescents and adults [@chierchia2019].
 
### Shape Rotation

Mental rotation refers to the ability to imagine the rotation of objects in space, which is strongly related to mathematics and reasoning abilities [@xie2020]. In our version of the mental rotation task [based on @frick2013], children are presented with a target item (e.g., a picture of a rabbit). Then, they are presented with two choice items (e.g., two silhouettes of rabbits). One of the choice items will match the target item when rotated; the other choice item will not match the target item when rotated. The child must select the choice item that matches the target item. Typically, both accuracy and reaction time are measured. Across trials, the degree of rotation varies (e.g., 45, 90, 180 degrees), affecting children’s accuracy and reaction time. We included both two-dimensional duck and rabbit stimuli as well as harder three-dimensional geometric figures from the original stimulus set developed by @shepard1971.

## Executive Function

### Same and Different Selection

The Same and Different Selection task is designed to assess cognitive flexibility in children. It draws upon elements from the "Something's the Same" task [@willoughby2011] but is closest to the "Flexible Item Selection Task" [@jacques2001]. In this task, children are presented with sets of items that vary along multiple dimensions, such as shape, color, size, and number. They are required to identify similarities and differences between items based on these dimensions, engaging their ability to shift attention and adapt to changing rules or criteria. The version of the task we developed included successively more difficult blocks of trials in which children were asked to identify two, three, or four pairs of items that were the same in different ways (e.g., matching on different figures).

### Hearts and Flowers

The Hearts and Flowers task assesses inhibitory control and cognitive flexibility [@davidson2006]. Participants respond according to stimulus type: pressing a key on the same side as a heart (congruent rule), and on the opposite side for a flower (incongruent rule). The task includes three blocks: congruent (hearts only), incongruent (flowers only), and mixed (hearts and flowers). The congruent block serves as a baseline with minimal executive demands. Participants in our version of the task are encouraged to respond as quickly as possible but -- due to variation in administration platform and in contrast to some versions of the task -- we did not provide a hard time limit on each trial, though we scored responses slower than 3000 ms as incorrect. 

### Memory Game

The Corsi Block task is a widely-used measure of visuospatial short-term and working memory [@corsi1972]. In our version of the task, the child sees a grid of squares; during each trial, a subset of squares lights up one at a time in a specific sequence. The child is required to reproduce the sequence by touching the squares: in the first block, the sequence is reproduced in the same order and in the second block, it is reproduced backwards. The task begins with short sequences (e.g., two items) and gradually increases in difficulty (up to seven items) until the child fails two sequences of the same length. We used both 2x2 or 3x3 grids in our pilot testing, with relatively similar results.

## Social Cognition

### Stories task

Assessing social cognition across a wide range of ages is a challenge, and relatively few instruments show good reliability and validity across not just early childhood but older children as well [cf. @heise2025]. For this purpose, we adapted and re-illustrated a storybook task developed by @sotomayor-enriquez2024 in which children hear stories with multiple social reasoning questions embedded, including questions about true and false beliefs, deception, and moral reasoning. Using the published data from this task, we selected a set of items that showed good psychometric properties and that we believed would be most likely to be valid cross-culturally and then supplemented these with additional trials tapping emotional reasoning. The result was a set of six stories, each with approximately five forced-choice social reasoning questions embedded within it. We then developed two sets of alternative re-test stories, in each case using the same structure as an existing story but substituting new details and characters. 

## Translation and adaptation

Initially designed in English, core tasks were subsequently adapted for Spanish and German following a set of internationalization procedures in collaboration with researchers at the sites collecting pilot data in Colombia and Germany. We began with AI-generated translations of all verbal materials, including task instructions, items, and other key terminology such as encouragement phrases that appear throughout the core tasks; we used the DeepL platform for these initial translations. A professional translator then reviewed and edited the AI translations for general grammatical correctness and appropriateness. Materials were next reviewed by a researcher who was a native speaker of the relevant language and dialect, confirming that the materials were clear and appropriate for the specific task. We then performed back translation using AI translation tools to evaluate which items needed further discussion to ensure cultural relevance and task integrity. For tasks evaluating language skills (Vocabulary and Sentence Understanding), we additionally recruited PhD-level linguists with expertise in language acquisition for both Spanish and German to review items and make recommendations about relevant changes or additions to the item set for that language.

# Pilot Data Collection

```{r compute_counts}
percentify <- \(s) as.character(glue("{sprintf('%.1f', s)}%"))

run_data <- read_rds(here("01_fetched_data/run_data.rds")) |>
  # filter out CO data post-pilot sample
  filter(!(site == "pilot_uniandes_co" & time_started > ymd_hms("2025-07-01 00:00:00")))

runs_filtered <- read_rds(here("01_fetched_data/runs_filtered.rds"))

# all runs
run_counts <- run_data |> count(site, task_id, name = "n_total_runs")

# runs filtered to completed, no straightlining, first in admin
run_filtered_counts <- runs_filtered |> count(site, task_id, name = "n_filtered_runs")

# runs in trial data (must have item info)
# trial_data <- read_rds(here(glue("01_fetched_data/trial_data.rds")))
# trial_run_counts <- trial_data |> distinct(site, task_id, run_id) |> count(site, task_id, name = "n_trials_runs")

# runs in filtered trial data (both above filters + not all invalid trials)
task_data <- read_rds(here(glue("01_fetched_data/task_data_nested.rds"))) |> unnest(data)
trial_filtered_run_counts <- task_data |> distinct(site, task_id, run_id) |> count(site, task_id, name = "n_trials_filtered_runs")

# scores (above runs + non-missing age)
score_counts <- all_scores |> filter(item_task != "ha") |> count(site, task_id, name = "n_scores")

# included scores (above scores + age in range + first run or large enough age gap)
included_score_counts <- included_scores |> filter(item_task != "ha") |> count(site, task_id, name = "n_included_scores")

# combined_counts <- list(run_counts, trial_run_counts, run_filtered_counts, trial_filtered_run_counts, score_counts, included_score_counts) |>
counts <- list(run_counts, run_filtered_counts, trial_filtered_run_counts, score_counts, included_score_counts) |>
  reduce(left_join) |>
  left_join(all_scores |> distinct(task_id, item_task, task) |> filter(item_task != "ha")) |>
  filter(item_task %in% core_tasks) |>
  select(-task_id, -item_task) |>
  relocate(task, .before = everything()) |>
  arrange(task) |>
  mutate(site = site |> fct_recode(!!!site_labels))

# write_csv(counts, here("04_paper/display/counts.csv"))

# counts_long <- combined_counts |>
#   pivot_longer(starts_with("n_"), names_to = "count", names_prefix = "n_", values_to = "n") |>
#   mutate(count = count |> fct_inorder())

# anti_join(
# all_scores |> filter(item_task != "ha") |> select(site, task_id, run_id),
# task_data |> distinct(site, task_id, run_id)
# )
# 
# lost_runs <- anti_join(
#   task_data |> filter(item_task %in% irt_tasks, item_task != "ha") |> distinct(site, task_id, run_id),
#   all_scores |> select(site, task_id, run_id)
# ) 
  
  # filter(site == "pilot_western_ca") |>
  # filter(timestamp == min(timestamp))
# run_data |>
#   semi_join(lost_runs) |>
#   filter(!is.na(age)) |>
#   filter(time_started == min(time_started)) |> pull(time_started)
#   pull(run_id) |> pluck(1)

site_counts <- counts |>
  group_by(site) |>
  summarise(across(starts_with("n_"), sum)) |>
  pivot_longer(cols = -site,
               names_to = "count", values_to = "n", names_prefix = "n_")

site_diffs <- site_counts |>
  group_by(site) |>
  mutate(diff = lag(n) - n,
         pct_diff = diff / max(n) * 100,
         excluded = glue("{diff} [{percentify(pct_diff)}]"),
         comparison = paste(lag(count), "-", count)) |>
  ungroup() |>
  filter(count != "total_runs") |>
  select(site, count = comparison, n = excluded)

site_endpoints <- site_counts |>
  group_by(site) |>
  slice(1, n()) |>
  ungroup() |>
  mutate(n = as.character(n))

site_table <- bind_rows(site_endpoints, site_diffs) |>
  pivot_wider(names_from = site, values_from = n) |>
  mutate(count = count |> fct_inorder() |> fct_relevel("included_scores", after = Inf)) |>
  arrange(count) |>
  mutate(count = count |> fct_recode(
    "Total number of runs" = "total_runs",
    "Excluded for being incomplete, invalid, or duplicate" = "total_runs - filtered_runs",
    "Excluded for missing item metadata" = "filtered_runs - trials_filtered_runs",
    "Excluded for missing age" = "trials_filtered_runs - scores",
    "Excluded for age out of range or too short retest gap" = "scores - included_scores",
    "Final number of scored runs" = "included_scores"
  )) |>
  rename(Count = count)

# apa_table(site_table, align = "lrrr")

site_totals <- site_counts |> filter(count == "total_runs") |> select(site, n) |> deframe()

user_totals <- run_data |>
  group_by(site) |>
  summarise(n_users = n_distinct(user_id)) |>
  mutate(site = site |> fct_recode(!!!site_labels))

user_score_counts <- coretask_scores |>
  group_by(site) |>
  summarise(n_users = n_distinct(user_id),
            n_runs = n_distinct(run_id)) |>
  mutate(site = site |> fct_recode(!!!site_labels))

collapse_value_list <- \(l) l |> imap(\(v, n) paste(n, v, sep = ": ")) |> paste(collapse = ", ")

get_count_row <- \(i) {
  site_table |> slice(i) |> select(-Count) |> collapse_value_list()
  # imap(\(v, n) paste(n, v, sep = ": ")) |> paste(collapse = ", ")
}

trial_data_subset <- read_rds(here(glue("01_fetched_data/trial_data_subset.rds")))

trial_counts <- left_join(
  trial_data_subset |> count(site, item_task, name = "n_trials"),
  task_data |> count(site, item_task, name = "n_included_trials")
) |>
  filter(item_task %in% core_tasks) |>
  mutate(site = site |> fct_recode(!!!site_labels),
         n_excluded_trials = n_trials - n_included_trials,
         pct_excluded = n_excluded_trials / n_trials * 100) |>
  mutate(excluded = glue("{n_excluded_trials} [{percentify(pct_excluded)}]"))

task_trial_counts <- trial_counts |>
  group_by(item_task) |>
  summarise(across(where(is.integer), sum)) |>
  mutate(pct_excluded = (n_trials - n_included_trials) / n_trials * 100) |>
  mutate(excluded = glue("{n_excluded_trials} [{percentify(pct_excluded)}]"))

site_trial_counts <- trial_counts |>
  group_by(site) |>
  summarise(across(where(is.integer), sum)) |>
  mutate(pct_excluded = (n_trials - n_included_trials) / n_trials * 100) |>
  mutate(excluded = glue("{n_excluded_trials} [{percentify(pct_excluded)}]"))

total_trial_counts <- trial_counts |>
  group_by() |>
  summarise(across(where(is.numeric), sum)) |>
  mutate(pct_excluded = (n_trials - n_included_trials) / n_trials * 100) |>
  mutate(excluded = glue("{n_excluded_trials} [{percentify(pct_excluded)}]"))

```


The pilot site partnership was an integral component of the core task development. Across three countries and languages, pilot sites provided collaboration on task adaptation and functionality, iterative task testing, and diverse settings for infrastructure deployment. Data collected from pilot sites allowed analyses of construct validity, measurement invariance, test-retest reliability, parameters for computer adaptive testing, and a demonstration of developmental growth measured by the tasks across constructs. Because we continued developing tasks continuously during the ~18 month pilot process, we anticipate that our reported reliability and validity estimates are likely an underestimate of the current task performance. Our analyses average over early versions of the tasks that in some cases had instructions or items that were later revised for clarity. 

One key feature of the LEVANTE framework is that all data are completely de-identified, reducing legal and ethical obstacles to data sharing. No demographic information or other identifiers are entered into the LEVANTE dashboard by the sites, and ages are only given to a one month precision [@frank2025]. We anticipate that sites will add sociodemographic information into the dataset at a later time via an app that is currently under development that ensures that there is no statistical reidentification risk for  individuals (e.g., due to rare combinations of traits in a community). For this reason, here we provide only minimal demographic characterization and analysis of the children from our pilot sites. @fig-age-hist provides the distribution of ages for each site. 

```{r}
#| label: fig-age-hist
#| fig-cap: "Overall distributions of ages for each site."
#| fig-height: 3

ages <- coretask_scores |>
  group_by(site, site_label, dataset, user_id) |>
  summarise(age = min(age), 
            n_runs = n())

ggplot(ages, aes(x = age, fill = site)) + 
  facet_wrap(vars(site_label)) + 
  geom_histogram(binwidth = 1, color = "white") +
  scale_fill_solarized(guide = "none") +
  scale_y_continuous(expand = expansion(0, 0)) +
  labs(x = "Age (years)", y = "Number of children")
```

Across all tasks, the sites collected a total of `{r} sum(site_totals)` runs (`{r} get_count_row(1)`) from `{r} sum(user_totals$n_users)` unique children (`{r} user_totals |> deframe() |> collapse_value_list()`), of which some were excluded for being incomplete, invalid, or duplicate (`{r} get_count_row(2)`), for missing item metadata (`{r} get_count_row(3)`), for missing age (`{r} get_count_row(4)`), or for age being out of range or having too short of a retest gap (`{r} get_count_row(5)`), resulting in a total of `{r} sum(user_score_counts$n_runs)` scored runs (`{r} user_score_counts |> select(site, n_runs) |> deframe() |> collapse_value_list()`) from `{r} sum(user_score_counts$n_users)` unique children (`{r} user_score_counts |> select(site, n_users) |> deframe() |> collapse_value_list()`). 

Within the included runs, there were a total of `{r} total_trial_counts$n_trials` trials (`{r} site_trial_counts |> select(site, n_trials) |> deframe() |> collapse_value_list()`), of which `{r} total_trial_counts$excluded` were excluded (`{r} site_trial_counts |> select(site, excluded) |> deframe() |> collapse_value_list()`) due to too fast or too slow reaction times. For reasons of space we do not disaggregate these exclusions by task, but note that the task with the highest exclusion rate was Pattern Matching (`{r} percentify(max(task_trial_counts$pct_excluded))` trials excluded), while all the other tasks ranged between `{r} percentify(min(task_trial_counts$pct_excluded))` and `{r} percentify(sort(task_trial_counts$pct_excluded, decreasing = TRUE)[2])` trials excluded.

## Colombia

The Universidad de Los Andes conducted school-based data collection across four schools in Bogotá and three schools in the rural areas of Caquetá and Boyacá, Colombia. By partnering with schools, the research team oversaw task data collection with children using a group testing format on tablets provided by the team. Children were tested in groups, with larger groups for older children and smaller groups for younger children. The losandes-co site collected data from `{r} user_totals$n_users[user_totals$site == "uniandes-co"]` children ages 5--12 years. Due to time constraints for school-based administration, children typically completed a subset of the tasks. 

## Germany

The Max Planck Institute for Evolutionary Anthropology and Leuphana University collected data using family-based remote data collection. Participants were recruited via an existing database of Leipzig families by first contacting families via telephone with an invitation to participate in the study. Upon agreement, families received an email with information to log into the LEVANTE system and complete the tasks assigned to them at home using their own computer or tablet. This site provided retest data with follow-up assignments sent to families 4--5 months after initial completion of the tasks. The mpieva-de site collected data from `r user_totals$n_users[user_totals$site == "mpieva-de"]` children, ages 5--12 years. 

## Canada

Partners at Western University (Ontario) recruited participants from the local community, primarily through online outreach (Facebook advertisements and community group posts), supplemented by posters on neighborhood mailboxes and notices shared via local school newsletters. Children were tested individually by a researcher in a dedicated testing space on campus. The western-ca site collected data from `r user_totals$n_users[user_totals$site == "western-ca"]` children, ages 5-12 years. 

# Scoring

Our goal for each task was to extract administration-level ability scores with the highest possible reliability and validity. To do so, we first conducted heuristic item screening to remove poorly functioning items, then fit a sequence of multigroup IRT models to the retained items to estimate latent ability scores. These analyses provided the foundation for the reliability, developmental, and validity results described in subsequent sections.

## Heuristic item pruning and data checking

Prior to fitting psychometric models, we examined item-level performance summaries for each task within each site to identify items that were functioning unusually poorly (e.g., showing average accuracy close to or even below chance). We also reviewed classical diagnostics such as proportion correct, item–total correlations, and missingness rates to flag items that showed little discrimination or appeared anomalous. In some cases this was due to potential translation issues; in others it reflected aspects of item design. We manually excluded these items from further processing (N = 10). In one case, this process helped us identify a bug in the scoring of data for three tasks; we excluded these observations, which were made during a two day period of testing for the mpieva-de site. The resulting cleaned calibration dataset formed the basis for all subsequent IRT estimation and invariance analyses.

## Multigroup IRT model selection

Following item screening, we evaluated measurement invariance across sites using nested multigroup IRT models to assess the comparability of latent ability estimates. The conventional hierarchy of factor-analytic invariance—configural → metric → scalar—was implemented in the IRT framework as successive equality constraints on item parameters (see @tbl-invariance-explainer) for correspondence and interpretation). Configural invariance reflects a shared latent structure, metric invariance constrains item discriminations, and scalar invariance constrains both discriminations and difficulties across sites.

For each task, we compared unidimensional Rasch and 2PL models under these nested invariance constraints. All models were estimated in `mirt` [@mirt]  via marginal maximum likelihood using 500 quadrature points, with expected a posteriori (EAP) scoring. Overlapping items across sites served as anchors to establish a common scale; non-overlapping items were freely estimated within the same multigroup calibration. Model convergence and identification were verified by ensuring stable log-likelihoods and condition numbers below \(10^3\).

Model selection was guided by the Bayesian Information Criterion (BIC). The selected model for each task (see @tbl-task-rxx) represents the most parsimonious combination of model family and invariance level that adequately reproduced cross-site response patterns. Across tasks, Rasch models provided the best fit for most domains, while 2PL models were favored for Same & Different and Shape Rotation, tasks showing greater variability in item discrimination.  

Most tasks achieved scalar invariance, suggesting that both difficulty and discrimination parameters generalized across sites; one task achieved only metric invariance (Same & Different), and two (Theory of Mind and Vocabulary) achieved only configural invariance, indicating consistent dimensional structure but limited parameter equivalence. For interpretive clarity, subsequent cross-site comparisons are restricted to tasks demonstrating at least metric invariance.

## Score computation

Individual ability scores were then computed as expected a posteriori (EAP) estimates based on the final multigroup calibrations. Resulting scores are expressed on a common cross-site scale, enabling the developmental, reliability, and validity analyses described in the following section.

::: {.landscape}
```{r}
#| label: tbl-invariance-explainer
#| tbl-cap: "Measurement invariance across factor analysis and item response theory."

# read and lightly normalize columns
inv_explainer <- read_csv(here("04_paper", "display", "invariance.csv"), show_col_types = FALSE) |>
  mutate(across(everything(), \(v) replace_na(v, "–"))) |>
  transmute(
    Invariance = str_to_title(Invariance),
    Model = case_when(
      str_detect(Model, regex("2\\s*pl|^2pl$", ignore_case = TRUE)) ~ "2PL",
      str_detect(Model, regex("rasch", ignore_case = TRUE)) ~ "Rasch",
      TRUE ~ ifelse(Model == "" | Model == "–", "–", Model)
    ),
    `mirt code`                = ifelse(`mirt code` == "", "–", `mirt code`),
    `Factor-analytic analogue` = ifelse(`Factor analysis name` == "", "–", `Factor analysis name`),
    `IRT explanation`          = ifelse(`IRT explanation` == "", "–", `IRT explanation`),
    `Practical interpretation` = ifelse(Interpretation == "", "–", Interpretation)
  ) |>
  select(Invariance, Model, `mirt code`, `Factor-analytic analogue`, `IRT explanation`, `Practical interpretation`)

# build table (mirrors tbl-task-rxx style)
inv_explainer |>
  flextable() |>
  set_header_labels(
    Invariance = "Invariance",
    Model = "Model",
    `mirt code` = "mirt code",
    `Factor-analytic analogue` = "Factor-analytic analogue",
    `IRT explanation` = "IRT explanation",
    `Practical interpretation` = "Practical interpretation"
  ) |>
  separate_header() |>
  width(j = "Invariance", width = 0.6) |>
  width(j = "Model", width = 0.6) |>
  width(j = "mirt code", width = 1.8) |>
  width(j = "Factor-analytic analogue", width = 2) |>
  width(j = "IRT explanation", width = 2) |>
  width(j = "Practical interpretation", width = 2) |>
  align(align = "left", part = "all") |>
  valign(val = "top", part = "all") |>
  line_spacing(space = 1.1, part = "all") |>
  padding(padding.top = 2, padding.bottom = 2, part = "all") |>
  fontsize(size = 10, part = "all") 
```
:::


# Psychometric properties of tasks

## Developmental change

Across all nine tasks we observed positive age–ability trends. The smoothed lines in Figure @fig-age-trends are roughly parallel within each panel, indicating that sites differ mostly in overall level rather than in developmental rate. Growth appeared steepest for Math and Hearts & Flowers, moderate for Vocabulary, Memory, and Theory of Mind, and shallower for Shape Rotation and Sentence Understanding, which showed positive but weaker age effects.

To quantify developmental change, we fitted separate ordinary least-squares models predicting ability scores (IRT units) from centered age with site fixed effects and an age × site interaction, allowing slopes to vary by site. As summarized in Table @tbl-slopes, age-related slopes were positive for all tasks and sites, indicating that older children performed better on every measure. 

In addition, we calculated per-site zero-order correlations between age and IRT ability scores. These correlations were uniformly positive (r ≈ .40–.70) and of moderate magnitude across tasks, consistent with the regression estimates. The close correspondence between the linear slopes and the smooth trends suggests that, within the studied age range, developmental gains are approximately linear and broadly comparable across cultural contexts.

```{r}
#| label: fig-age-trends
#| fig-cap: "Age related trends by task."
#| fig-width: 8
#| fig-height: 9

ggplot(coretask_scores, aes(age, metric_value)) +
  ggh4x::facet_nested_wrap(vars(task_category, task),
                           nest_line = element_line(), solo_line = TRUE,
                           axes = "x", scales = "free_y") +
  geom_smooth(aes(colour = site_label, group = site_label),
              method = "gam", formula = y ~ s(x, bs = "re"), se = FALSE) +
  geom_point(aes(colour = site_label), alpha = 0.3, size = 1) +
  scale_x_continuous(breaks = seq(6, 14, 2)) +
  .scale_colour_site() +
  guides(colour = guide_legend(override.aes = list(shape = NA, linetype = 1, linewidth = 2))) +
  labs(x = "Age (years)", y = "Ability (IRT score)", colour = "Site") +
  theme(legend.position = "bottom")
```
::: {.landscape}
```{r}
#| label: tbl-slopes
#| tbl-cap: "Age-Related Change in Performance: Task Grand Means, Site Deviations, and Age–Ability Correlations"

analyze_task_slopes <- function(task_name) {
  d <- coretask_scores %>% filter(item_task == task_name)
  d$site  <- as.factor(d$site)
  d$age_c <- scale(d$age, scale = FALSE)[, 1]
  contrasts(d$site) <- contr.sum(nlevels(d$site))

  fit <- lm(metric_value ~ age_c * site, data = d)

  cs <- coef(summary(fit))
  mean_slope <- unname(cs["age_c", "Estimate"])
  mean_se    <- unname(cs["age_c", "Std. Error"])
  mean_t     <- unname(cs["age_c", "t value"])
  mean_p     <- unname(cs["age_c", "Pr(>|t|)"])
  mean_df    <- df.residual(fit)                    

  tr_site <- emtrends(fit, ~ site, var = "age_c")

  dev_tab <- contrast(tr_site, "eff") %>%
    summary(infer = c(TRUE, TRUE)) %>%
    as_tibble() %>%
    transmute(
      site            = sub("\\s*effect$", "", contrast),
      Slope_Deviation = estimate,
      SE              = SE,
      t               = t.ratio,
      df              = df,
      p_value         = p.value
    )

  # per-site zero-order correlation r(age, ability)
  corr_tab <- d %>%
    filter(!is.na(age_c), !is.na(metric_value)) %>%
    mutate(site = as.character(site)) %>%
    group_by(site) %>%
    summarise(
      N = n(),
      r = if (n() >= 3 && sd(age_c) > 0 && sd(metric_value) > 0)
            cor(age_c, metric_value) else NA_real_,
      .groups = "drop"
    ) %>%
    mutate(
      r_t  = ifelse(!is.na(r) & N >= 3 & abs(r) < 1, r * sqrt((N - 2)/(1 - r^2)), NA_real_),
      r_df = ifelse(!is.na(r_t), N - 2, NA_real_),
      r_p  = ifelse(!is.na(r_t), 2 * pt(-abs(r_t), df = r_df), NA_real_)
    )

  # combine: add grand mean stats to every row for that task
  dev_tab %>%
    mutate(
      task = task_name,
      Mean_Slope = mean_slope,
      Mean_SE    = mean_se,
      Mean_t     = mean_t,
      Mean_df    = mean_df,
      Mean_p     = mean_p,
      .before = 1
    ) %>%
    left_join(corr_tab, by = "site")
}

# build table (unchanged mapping)
deviation_raw <- map_dfr(unique(coretask_scores$item_task), analyze_task_slopes) %>%
  mutate(Task = dplyr::recode(as.character(task),
    math   = "Math",
    tom    = "ToM",
    matrix = "Mat.Reason.",
    mrot   = "Ment.Rotat.",
    hf     = "H&F",
    mg     = "Mem",
    sds    = "Same&Diff.",
    trog   = "Sent.Under.",
    vocab  = "Vocab."
  ))

# format p-values (dev p and grand mean p) and print
deviation_print <- deviation_raw %>%
  mutate(
    # fill any missing deviation p from t, df
    p_num = if_else(is.na(p_value) & !is.na(t) & !is.na(df),
                    2 * pt(-abs(t), df), p_value),
    `Deviation p-value` = case_when(
      is.na(p_num)  ~ NA_character_,
      p_num < .001  ~ "<0.001",
      TRUE          ~ sprintf("%.3f", p_num)
    ),
    `Grand Mean p-value` = case_when(
      is.na(Mean_p)    ~ NA_character_,
      Mean_p < .001    ~ "<0.001",
      TRUE             ~ sprintf("%.3f", Mean_p)
    ),
    `r p-value` = case_when(
      is.na(r_p)     ~ NA_character_,
      r_p < .001     ~ "<0.001",
      TRUE           ~ sprintf("%.3f", r_p)
    )
  ) %>%
  transmute(
    Task,
    Site = site,
    `Grand Mean Slope (β)` = round(Mean_Slope, 2),
    `Grand Mean SE`        = round(Mean_SE, 2),
    `Grand Mean p-value`,
    `Deviation from Mean`  = round(Slope_Deviation, 2),
    `Deviation SE`         = round(SE, 2),
    `Deviation p-value`,
    N,
    `r (age, ability)`     = round(r, 2),
    `r p-value`
  )

ft <- flextable(deviation_print) |>
  set_header_labels(
    `Grand Mean Slope (β)` = "GM β",
    `Grand Mean SE`        = "GM SE",
    `Grand Mean p-value`   = "GM p",
    `Deviation from Mean`  = "Δ from GM",
    `Deviation SE`         = "Δ SE",
    `Deviation p-value`    = "Δ p",
    `r (age, ability)`     = "r(age,ability)",
    `r p-value`            = "r p"
  ) |>
  colformat_num(
    j = c("Grand Mean Slope (β)", "Grand Mean SE",
          "Deviation from Mean", "Deviation SE",
          "N", "r (age, ability)"),
    digits = 2
  ) |>
  set_caption("Age-Related Change in Performance: Task Grand Means, Site Deviations, and Age–Ability Correlations") |>
  fontsize(size = 9, part = "all") |>
  line_spacing(space = 0.7, part = "all") |>
  padding(padding.top = 1, padding.bottom = 1,
          padding.left = 1, padding.right = 1, part = "all") |>
  autofit() |>
  width(j = "Task", width = 0.7) |>
  width(j = "Site", width = 1.2) |>
  width(j = c("Grand Mean Slope (β)", "Grand Mean SE", "Grand Mean p-value",
              "Deviation from Mean", "Deviation SE", "Deviation p-value",
              "N", "r (age, ability)", "r p-value"),
        width = 0.55) |>
  height_all(height = 0.15, part = "body") |>
  height(height = 0.15, part = "header") |>
  align(align = "center", part = "all")
ft
```
:::

## Reliability 

One goal for the LEVANTE core tasks is strong reliability both within an individual administration (across test items) and across multiple administrations separated in time. We report on each of these. 

### Marginal reliability

```{r}
#| label: tbl-task-rxx
#| tbl-cap: "Marginal reliabilities for each task and site."

task_rxx <- read_rds(here("02_scoring_outputs/task_rxx.rds"))

trx <- task_rxx |>
  rename(item_task = task) |>
  left_join(task_map) |>
  select(task, site, itemtype, invariance, rxx) |>
  mutate(site = site |> fct_recode(!!!site_labels),
         itemtype = itemtype |> fct_recode("Rasch" = "rasch", "2PL" = "2pl"),
         invariance = invariance |> str_to_sentence()) |>
  arrange(task) |>
  pivot_wider(names_from = site, values_from = rxx,
              names_prefix = "Reliability_") |>
  rename(Task = task, Model = itemtype, Invariance = invariance)

trx |>
  flextable() |>
  separate_header() |>
  flextable::autofit(add_w = 0, add_h = 0)
```
  
IRT models allow the computation of marginal reliability, which captures the inter-relatedness of items in an assessment and hence the proportion of variation attributed to shared variance. In multi-group IRT models, marginal reliabilities can be computed only for individual sub-models.  @tbl-task-rxx shows marginal reliabilities for each task, reporting reliability for each sub-model for the best fitting model. 

```{r}
# prophecy <- function (n = 2, r) {
#   (n * r) /  (1 + (n - 1) * r)
# }

de_retest_scores <- coretask_scores |> 
  filter(site == "pilot_mpieva_de") |>
  group_by(user_id, task) |>
  arrange(user_id, task, age) |>
  filter(n() > 1) 

retest_wide <- de_retest_scores |>
  group_by(user_id, task) |>
  mutate(has_retest = any(age_gap > .05), 
         mean_age = mean(age), 
         age_gap = max(age_gap)) |>
  filter(has_retest) |>
  ungroup() |>  
  pivot_wider(
    id_cols = c(user_id, task_category, task, mean_age, age_gap),
    names_from = run_number,
    values_from = metric_value,
    names_prefix = "run_"
  ) |>
  select(user_id, task_category, task, age = mean_age, age_gap, run_1, run_2)

trt <- retest_wide |>
  group_by(task_category, task) |>
  summarise(test_retest_r = cor(run_1, run_2, use = "complete.obs"), 
            # prophecy_r = prophecy(n = 2, r = test_retest_r),
            n = n(), 
            age_gap = mean(age_gap) * 12) |>
  ungroup() |>
  arrange(task)
```

### Test-retest reliability

We were also interested in capturing the extent to which the LEVANTE tasks yield reliable signal across a delay. The mpieva-de pilot site was able to re-administer tasks to a subset of their original participant group with a retest interval of approximately 4--5 months. We used this re-administration as an opportunity to collect data with computer adaptive versions of many of the tasks (Sentence Understanding, Pattern Matching, Shape Rotation, Math and Vocabulary); see below for further details of computer adaptive versions. In addition, for several tasks (Math, Theory of Mind, Pattern Matching), we used this opportunity to gather item data about new groups of retest items that were designed to match the content of the original items but vary in their surface forms. For the other tasks, we retested using the same items. 

Figure @fig-retest shows the relations between time 1 and time 2 scores. Uptake for retest administrations varied between N=`r min(trt$n)` and N=`r max(trt$n)`; the low number of repeat administrations for Sentence Understanding reflects an error in which this task was not sent to younger children. Each figure panel also shows the test-retest correlation and sample size for the corresponding task.

```{r}
#| label: fig-retest
#| fig-cap: "Test-retest correlations per task (in mpieva-de data)."
#| fig-width: 8
#| fig-height: 9

ggplot(retest_wide, aes(x = run_1, y = run_2, color = age)) +
  ggh4x::facet_nested_wrap(vars(task_category, task),
                           nest_line = element_line(), solo_line = TRUE,
                           axes = "x",
                           scales = "free_y") +
  coord_cartesian(clip = "off") +
  geom_abline(color = "grey", linetype = "dotted") +
  geom_point(alpha = 0.8, size = 1.5) +
  geom_label(data = trt, aes(label = sprintf("r = %.2f", test_retest_r)),
            size = 4, color = "darkgrey", linewidth = 0, x = -Inf, y = Inf,
            vjust = "inward", hjust = "inward") +
  geom_label(data = trt, aes(label = sprintf("N = %s", n)),
            size = 4, color = "darkgrey", linewidth = 0, x = Inf, y = -Inf,
            vjust = "inward", hjust = "inward") +
  geom_smooth(aes(group = 1), color = "darkgrey", method = "lm") +
  scale_color_viridis(limits = c(5, 12)) +
  labs(x = "Score 1", y = "Score 2", color = "Age (years)") +
  theme(legend.position = "bottom")
```

## Validity

```{r prep-sem, include=FALSE}
# one row per user (metadata + task scores from first run)
meta <- coretask_firstrun_scores |>
  group_by(user_id) |>
  summarise(
    age        = min(age, na.rm = TRUE),
    site       = first(na.omit(site)),
    site_label = first(na.omit(site_label)),
    dataset    = first(na.omit(dataset)),
    .groups = "drop"
  )

# wide task scores (first run only)
wide_scores <- coretask_firstrun_scores |>
  select(user_id, item_task, metric_value) |>
  distinct() |>  # guard against accidental duplicates
  pivot_wider(
    names_from  = item_task,
    values_from = metric_value
  ) |>
  left_join(meta, by = "user_id") |>
  relocate(user_id, site, site_label, dataset, age)

# keep only the core task columns + metadata
vars_core <- c("math","matrix","mrot","sds","hf","mg","trog","vocab","tom")
wide_scores <- wide_scores |>
  select(user_id, site, site_label, dataset, age, any_of(vars_core))

# standardise
z <- function(x) (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
vars_core_std <- wide_scores |>
  mutate(across(all_of(vars_core), ~ z(as.numeric(.x))))
# str(vars_core_std[vars_core])
```

```{r sem-g, include=FALSE}
model_g_no_age <- "
  # first-order factors
  reasoning =~ matrix + mrot
  ef        =~ hf + mg + sds        
  language  =~ trog + vocab

  # second-order g
  g =~ reasoning + ef + language

  # observed outcomes of g
  math ~ g
  tom  ~ g
"

fit_g <- lavaan::sem(model_g_no_age, data = vars_core_std,
                     estimator = "MLR", std.lv = TRUE, missing = "fiml")

# store modification indices 
mi_g <- lavaan::modindices(fit_g, sort. = TRUE, minimum.value = 10)
# summary(fit_g, fit.measures = TRUE, standardized = TRUE)
```

```{r sem-g-age, include=FALSE}
#| echo: false
#| results: "hide" 

stopifnot(all(vars_core_std$age > 0, na.rm = TRUE))

# use log age, centered
vars_core_std$log_age_c <- scale(log(vars_core_std$age))[,1]

model_g_age <- "
  reasoning =~ 1*matrix + mrot
  ef        =~ 1*hf     + mg + sds
  language  =~ 1*trog   + vocab

  g =~ 1*reasoning + ef + language

  g ~ log_age_c

  math ~ g
  tom  ~ g

  # resolve Heywood
  ef ~~ 0*ef
"

fit_g_age <- sem(
  model_g_age, data = vars_core_std,
  estimator = "MLR", missing = "fiml",
  std.lv = FALSE, optim.method = "nlminb",
  control = list(iter.max = 50000, rel.tol = 1e-6)
)

summary(fit_g_age, fit.measures = TRUE, standardized = TRUE)

fit_g_age_fit <- lavaan::fitMeasures(fit_g_age)

```
### Construct validity

We assessed construct validity using a hierarchical CFA in which three first-order factors—Reasoning (matrix reasoning, mental rotation), Executive Function (Hearts & Flowers, Memory Game, Same–Different Selection), and Language (grammar, vocabulary)—loaded on a second-order general factor (G; @fig-semplot-g). Mathematics and Social Cognition (Theory of Mind; ToM) were treated as observed endogenous outcomes because each construct had a single task; introducing one-indicator latent variables would require strong, unverifiable assumptions about reliability/error variances. Age was specified to predict G (age → G). Model fit was poor -- CFI: `r round(lavaan::fitMeasures(fit_g_age)["cfi"], 3)`, RMSEA: `r round(lavaan::fitMeasures(fit_g_age)["rmsea"], 3)` RMSEA p-value: `r round(lavaan::fitMeasures(fit_g_age)["rmsea.pvalue"], 3)`. Overall, factor loadings supported the intended three-factor structure (convergent validity), and age related to each domain as expected. The high inter-factor correlations and strong second-order loadings indicate that much of the variance is shared across domains, consistent with a broad general ability underpinning performance on the nine core tasks.

```{r}
#| label: fig-semplot-g
#| fig-cap: "SEM for core tasks with general factor."

source(here::here("03_summaries/modified_sem_plot.R"))

# map names 
name_map <- tibble(
  name   = c("log_age_c","g","math","tom","reasoning","ef","language",
             "matrix","mrot","hf","mg","sds","trog","vocab"),
  pretty = c("Age (log years)","G","Math","Theory of Mind","Reasoning","Executive Function","Language",
             "Matrix Reasoning","Mental Rotation","Hearts & Flowers","Memory Game","Same–Different Selection","Sentence Understanding","Vocabulary")
)

# nodes
nodes <- get_nodes(fit_g_age, columns = c("name","shape")) %>%
  left_join(name_map, by = "name") %>%
  mutate(label = pretty)

# edges (std estimates + stars)
pe <- lavaan::parameterEstimates(fit_g_age, standardized = TRUE)

edges <- pe %>%
  filter(op %in% c("~","=~","~~")) %>%
  transmute(
    from = if_else(op == "~", rhs, lhs),
    to   = if_else(op == "~", lhs, rhs),
    op, est, std.all, pvalue,
    arrow     = if_else(op == "~~", "both", "last"),   # <-- fix here
    curved    = op == "~~" & lhs != rhs,               # curve only covariances
    curvature = if_else(op == "~~" & lhs != rhs, 0.35, NA_real_),
    linetype  = if_else(op == "~~" & lhs != rhs, 2L, 1L)
  ) %>%
  mutate(
    stars = case_when(pvalue < .001 ~ "***",
                      pvalue < .01  ~ "**",
                      pvalue < .05  ~ "*",
                      TRUE ~ ""),
    label = sprintf("%.2f%s", std.all, stars)
  )

layout_g_age <- matrix(
  c(
    NA,       NA,        "log_age_c", NA,     NA,     NA,     NA, NA, NA,
    NA,       NA,        NA,          "g",    NA,     NA,     NA, NA, NA,
    NA,       NA,        NA,          NA,    NA,     NA,     NA, NA, NA,
    "math",   NA, "tom",  NA,  "reasoning", NA,     "ef",   NA,     "language",
    NA,       NA,        NA,          NA,     NA,     NA,     NA, NA, NA,
    NA, NA, "matrix", "mrot",    "hf",        "mg",   "sds",  "trog", "vocab"
  ),
  nrow = 6, byrow = TRUE
)

g <- tidySEM::prepare_graph(
  edges = edges, nodes = nodes, text_size = 2.5, layout = t(layout_g_age),
  rect_width=4.2, rect_height=3,
  ellipses_width=3, ellipses_height=3,
  variance_height=2, variance_width=1.5,
  arrow_angle=15, arrow_length=.1,
  var_arrow_angle=15, var_arrow_length=.1,
  spacing_y=4, spacing_x=4,
  fix_coord=FALSE
)

g$edges$label_fill   <- "white"
g$edges$label_colour <- "black"
g$edges$label_alpha  <- 1

plot(g)
```
```{r prep-sem-ef}
ef_tasks <- c("hf","mg","sds","mefs")

# Filter to ef tasks (from included_scores), keep first run 
ef_firstrun_scores <- included_scores |>
  filter(item_task %in% ef_tasks) |>
  arrange(user_id, item_task, run_number, time_started) |>
  group_by(user_id, item_task) |>
  slice(1) |>
  ungroup()

# one row per user
meta_ef <- ef_firstrun_scores |>
  group_by(user_id) |>
  summarise(
    age        = min(age, na.rm = TRUE),
    site       = first(na.omit(site)),
    site_label = first(na.omit(site_label)),
    dataset    = first(na.omit(dataset)),
    .groups = "drop"
  )

# Wide ef task scores (first run only)
ef_wide_scores <- ef_firstrun_scores |>
  select(user_id, item_task, metric_value) |>
  distinct() |>  # guard against accidental duplicates
  pivot_wider(
    names_from  = item_task,
    values_from = metric_value
  ) |>
  left_join(meta_ef, by = "user_id") |>
  relocate(user_id, site, site_label, dataset, age)

# keep only ef columns + metadata
vars_ef <- ef_tasks
ef_wide_scores <- ef_wide_scores |>
  select(user_id, site, site_label, dataset, age, any_of(vars_ef))

# Standardise ef indicators (same z() you used)
z <- function(x) (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
ef_wide_std <- ef_wide_scores |>
  mutate(across(all_of(vars_ef), ~ z(as.numeric(.x))))

# str(ef_wide_std[vars_ef])
```

```{r sem-ef}
#| echo: false
#| results: "hide" 

cfa_ef <-  '
EF =~ hf + mg + sds + mefs
EF ~ age
'

fit_ef <- cfa(cfa_ef, data = ef_wide_std,
              estimator = "MLR", std.lv = TRUE, missing = "fiml")
summary(fit_ef, fit.measures = TRUE, standardized = TRUE)
#print(fitMeasures(fit_ef, c("cfi","tli","rmsea","srmr", "aic", "bic")))
```

```{r}
#| label: ef-stats
#| include: false
pe_ef <- lavaan::parameterEstimates(fit_ef, standardized = TRUE)

beta_age_EF <- pe_ef |>
  dplyr::filter(lhs == "EF", op == "~", rhs == "age") |>
  dplyr::pull(std.all) |> as.numeric()

r2_EF <- as.numeric(lavaan::lavInspect(fit_ef, "r2")["EF"])

fmt <- function(x, d = 2) formatC(x, digits = d, format = "f")
# standardised loadings (λ) for EF =~ hf + mg + sds + mefs
load_tab <- pe_ef |>
  dplyr::filter(op == "=~", lhs == "EF", rhs %in% c("hf","mg","sds","mefs")) |>
  dplyr::select(rhs, std.all) |>
  tibble::deframe()  # named vector: c(hf = ., mg = ., sds = ., mefs = .)

```
### External validity

To assess external validity, we fitted a single-factor EF model with hearts and flowers (hf), memory game (mg), same-different-selection (sds), and the external measure, the Minnesota Executive Function Scale (MEFS) as indicators (@fig-semplot-ef). We modeled age as a predictor of the latent EF factor. Fit was generally good `r round(lavaan::fitMeasures(fit_ef)["cfi"], 3)`, RMSEA: `r round(lavaan::fitMeasures(fit_ef)["rmsea"], 3)` RMSEA p-value: `r round(lavaan::fitMeasures(fit_ef)["rmsea.pvalue"], 3)`. Standardized loadings were: hf = `r fmt(load_tab["hf"])`, mg = `r fmt(load_tab["mg"])`, sds = `r fmt(load_tab["sds"])`, MEFS = `r fmt(load_tab["mefs"])`, indicating that MEFS aligns closely with the latent EF construct. Age strongly predicted EF ($\beta$ = `r round(beta_age_EF, 2)`; $R^2$ = `r round(r2_EF, 2)`), consistent with expected developmental gains.
```{r}
#| label: fig-semplot-ef
#| fig-cap: "SEM for executive function model."

nodes <- get_nodes(fit_ef, columns = c("name","shape")) |>
  dplyr::left_join(tibble::tibble(
    name   = c("age","EF","hf","mg","sds","mefs"),
    pretty = c("Age (years)","Executive Function",
               "Hearts & Flowers","Memory","Same & Different","MEFS")
  ), by = "name") |>
  dplyr::mutate(label = dplyr::coalesce(pretty, name))

# edges from standardized parameter estimates ---
pe_ef <- lavaan::parameterEstimates(fit_ef, standardized = TRUE)

edges <- pe_ef |>
  dplyr::filter(op %in% c("~","=~","~~")) |>
  dplyr::transmute(
    lhs, rhs, op,
    from = dplyr::if_else(op == "~", rhs, lhs),
    to   = dplyr::if_else(op == "~", lhs, rhs),
    std.all, pvalue,
    arrow = dplyr::case_when(
      op == "~~" & lhs == rhs ~ "both",  # variance loop
      op == "~~" & lhs != rhs ~ "none",  # covariance
      TRUE ~ "last"                       # regressions/loadings
    ),
    curvature = dplyr::if_else(op == "~~" & lhs != rhs, 50, NA_real_),
    linetype  = dplyr::if_else(op == "~~" & lhs != rhs, 2, 1),
    label = dplyr::case_when(
      op %in% c("~","=~") ~ sprintf("%.2f%s", std.all,
        dplyr::case_when(pvalue < .001 ~ "***",
                         pvalue < .01  ~ "**",
                         pvalue < .05  ~ "*", TRUE ~ "")),
      TRUE ~ sprintf("%.2f", std.all)
    )
  ) 

# top-down layout
layout_ef <- matrix(
  c(
    NA, NA, "age", NA, NA,
    NA, NA, "EF",  NA, NA,
    "hf","mg", NA, "sds","mefs" 
  ),
  nrow = 3, byrow = TRUE
)

g_ef <- tidySEM::prepare_graph(
  edges = edges,
  nodes = nodes,
  layout = layout_ef,        
  text_size = 2.8,
  rect_width = 3.4, rect_height = 1.5,
  ellipses_width = 5, ellipses_height = 2,
  variance_height = 3, variance_width = 1.5,
  arrow_angle = 15, arrow_length = .10,
  var_arrow_angle = 15, var_arrow_length = .10,
  spacing_y = 6, spacing_x = 5,
  fix_coord = TRUE
)

g_ef$edges$label_fill   <- "white"
g_ef$edges$label_colour <- "black"
g_ef$edges$label_size   <- 2.8

plot(g_ef)

```

```{r prep-sem-lang}
lang_vars <- c("swr","sre","pa","vocab","trog")

# first run per user–task
lang_firstrun_scores <- included_scores |>
  filter(item_task %in% lang_vars) |>
  arrange(user_id, item_task, run_number, time_started) |>
  group_by(user_id, item_task) |>
  slice(1) |>
  ungroup()

meta_lang <- lang_firstrun_scores |>
  group_by(user_id) |>
  summarise(
    age        = min(age, na.rm = TRUE),
    site       = first(na.omit(site)),
    site_label = first(na.omit(site_label)),
    dataset    = first(na.omit(dataset)),
    .groups = "drop"
  )

# create wide df 
lang_wide_scores <- lang_firstrun_scores |>
  select(user_id, item_task, metric_value) |>
  distinct() |>
  pivot_wider(
    names_from  = item_task,
    values_from = metric_value
  ) |>
  left_join(meta_lang, by = "user_id") |>
  relocate(user_id, site, site_label, dataset, age)

# keep only language columns + metadata
lang_wide_scores <- lang_wide_scores |>
  select(user_id, site, site_label, dataset, age, any_of(lang_vars))

# standardise 
z <- function(x) (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
lang_wide_std <- lang_wide_scores |>
  mutate(across(all_of(lang_vars), \(x) z(as.numeric(x))))
# mutate(across(all_of(intersect(lang_vars, names(.))), ~ z(as.numeric(.x))))
```

```{r sem-lang}
#| echo: false
#| results: "hide" 

# model
cfa_lang <-  '
Language =~ swr + sre + pa + trog + vocab
Language ~ age
'

fit_lang <- cfa(cfa_lang, data = lang_wide_std,
                estimator = "MLR", std.lv = TRUE, missing = "fiml")
summary(fit_lang, fit.measures = TRUE, standardized = TRUE)
pe_lang <- lavaan::parameterEstimates(fit_lang, standardized = TRUE)
fm_lang <- lavaan::fitMeasures(fit_lang)

# standardised beta and R^2
beta_age_Lang <- pe_lang |>
dplyr::filter(lhs == "Language", op == "~", rhs == "age") |>
dplyr::pull(std.all) |> as.numeric()
r2_Lang <- as.numeric(lavaan::lavInspect(fit_lang, "r2")["Language"])

# standarised loadings
load_tab_lang <- pe_lang |>
dplyr::filter(op == "=~", lhs == "Language",
rhs %in% c("swr","sre","pa","trog","vocab")) |>
dplyr::select(rhs, std.all) |>
tibble::deframe()

```

We ran a similar model to determine the external validity of the language and literacy measures (grammar and vocab) against ROAR tasks including measures of Phonological Awareness (pa), Single Word Reading (swr) and Sentence Reading Efficiency (sre). We fitted a single latent Language factor with ROAR tasks plus grammar (trog) and vocabulary as indicators, and modeled age as a predictor of the latent factor (@fig-semplot-lang). 
Fit was generally good `r round(fm_lang["cfi"], 3)`, RMSEA: `r round(fm_lang["rmsea"], 3)` RMSEA p-value: `r round(fm_lang["rmsea.pvalue"], 3)`. Standardized loadings were: SWR = `r round(load_tab_lang["swr"], 2)`, SRE = `r round(load_tab_lang["sre"], 2)`, PA = `r round(load_tab_lang["pa"], 2)`, TROG = `r round(load_tab_lang["trog"], 2)`, Vocabulary = `r round(load_tab_lang["vocab"], 2)`. Age predicted Language ($\beta$ = `r round(beta_age_Lang, 2)`; $R^2$ = `r round(r2_Lang, 2)`) consistent with expected developmental gains. 

```{r}
#| label: fig-semplot-lang
#| fig-cap: "SEM for language model. Abbreviations: Sent.Und.:Sentence Understanding; Vocab: Vocabulary; Single.Wrd.Read: Single Word Reading; Sent.Read.Eff: Sentence Reading Efficiency; Phon.Aware.: Phonological Awareness"

nodes_lang <- get_nodes(fit_lang, columns = c("name","shape")) |>
  dplyr::left_join(tibble::tibble(
    name   = c("age","Language","trog","vocab","swr","sre","pa"),
    pretty = c("Age (years)","Language",
               "Sent.Under","Vocab",
               "Single.Wrd.Read.","Sent.Read.Eff.",
               "Phon.Aware.")
  ), by = "name") |>
  dplyr::mutate(label = dplyr::coalesce(pretty, name))

# edges (standardized, with stars on loadings/paths)
pe_lang <- lavaan::parameterEstimates(fit_lang, standardized = TRUE)

edges_lang <- pe_lang |>
  dplyr::filter(op %in% c("~","=~","~~")) |>
  dplyr::transmute(
    lhs, rhs, op,
    from = dplyr::if_else(op == "~", rhs, lhs),
    to   = dplyr::if_else(op == "~", lhs, rhs),
    std.all, pvalue,
    arrow = dplyr::case_when(
      op == "~~" & lhs == rhs ~ "both",     
      op == "~~" & lhs != rhs ~ "none",     
      TRUE ~ "last"                         
    ),
    curvature = dplyr::if_else(op == "~~" & lhs != rhs, 50, NA_real_),
    linetype  = dplyr::if_else(op == "~~" & lhs != rhs, 2L, 1L),
    label = dplyr::case_when(
      op %in% c("~","=~") ~ sprintf("%.2f%s", std.all,
        dplyr::case_when(pvalue < .001 ~ "***",
                         pvalue < .01  ~ "**",
                         pvalue < .05  ~ "*", TRUE ~ "")),
      TRUE ~ sprintf("%.2f", std.all)
    )
  ) 

# top-down layout
layout_lang <- matrix(
  c(
    NA, NA, "age", NA, NA, NA,
    NA, NA, NA, NA, NA, NA,
    NA, NA, "Language", NA, NA, NA, 
    NA, NA, NA, NA, NA, NA,
    "trog","vocab", NA, NA, NA,
    NA, NA, NA, "swr","sre","pa"         
  ),
  nrow = 6, byrow = TRUE
)

g_lang <- tidySEM::prepare_graph(
  edges = edges_lang,
  nodes = nodes_lang,
  layout = layout_lang,        
  text_size = 2.8,
  rect_width = 3.6, rect_height = 3,
  ellipses_width = 5, ellipses_height = 4,
  spacing_x = 5,
  spacing_y = 6,
  variance_height = 3, variance_width = 2,
  arrow_angle = 15, arrow_length = .10,
  fix_coord = FALSE
)

g_lang$edges$label_fill   <- "white"
g_lang$edges$label_colour <- "black"
g_lang$edges$label_size   <- 3

plot(g_lang)

```

# Adaptive Task Construction

Many of the LEVANTE tasks have been adapted and piloted as CATs (Computerized Adaptive Tests). To date, these include the Test For Reception of Grammar (TROG), Vocabulary, Shape Rotation, Matrix Reasoning, Same Difference Selection, and Math. These tasks maintain an ability score, theta, as an estimate of the participant’s skill level that is updated at the end of each trial. They then present participants with the item best suited to their estimated ability, which both improves test-taker experience and yields more information on participant skill per item, allowing for a shorter task with fewer items. 

The CAT tasks use an adaptive algorithm made available by the jsCat JavaScript library [@ma2025]. The present LEVANTE CAT implementation varies difficulty and guessing for each item while holding both discrimination and upper asymptote constant at 1. Items are selected based on Maximum Fisher Information, and theta is updated according to a maximum likelihood estimator, with limits of -6 and 6. 

The LEVANTE CATs are configurable with respect to the initial value of theta and the conditions for ending the task. The starting theta is set at 0 for all CAT tasks currently in use, but can be lowered or raised according to the researcher’s prior expectation of participant ability, for example according to age. Current CAT implementations use stopping rules based on either time or number of items. TROG, Shape Rotation, and Vocabulary each have time limits currently set to 4 minutes, with Matrix Reasoning set to 6 minutes to allow for the increased time typically required to complete items in this task. Items in these tasks are presented together in a single block. Same Different Selection and Math are each divided into three blocks presented sequentially, with per-block stopping based on number of items. These CATs select from the list of items specific to their current block and proceed to the next block once the target number of items is reached, maintaining one overarching ability estimate for the entire task. Same Difference Selection and Math have time limits of 6 minutes and 8 minutes, respectively. 

# General Discussion

In this paper, we presented the LEVANTE core tasks -- nine psychometrically grounded tasks for children aged 5–12 that provide efficient, reliable, and valid assessment across language, mathematics, reasoning, executive function, and social cognition. LEVANTE is designed to tackle three core challenges: to span development and enable valid comparisons between younger and older children; to ensure cross-cultural comparability so that measures function equivalently across diverse contexts; and to make the tasks openly available (non-commercial) to maximise access, transparency, and reproducibility.

Using pilot data from three different countries, collected via three different administration methods (in-school, in-lab, and at-home administration), we provided preliminary evidence of reliability and validity. Further, using computer adaptive testing algorithms, we are able to deploy efficient versions of these tasks that produce scores for children across a wide range of abilities in just a few minutes.  

We hope that these tasks will become the backbone of the LEVANTE framework, in which sites around the world collaborate to collect longitudinal data measuring children's learning and development as well as the overlapping contexts in which they develop. De-identified data collected using the tasks presented here flow into a shared repository, becoming part of a large-scale dataset that we hope will enable a wide variety of downstream investigations. The power of these data is that -- especially when combined with contextual data about children’s caregivers, home environment, school, and built environment -- they afford one analysis of the sources of developmental and contextual variability at global scale.

Towards this goal, one strength of our approach here is that it easily accommodates future internationalization and adaptation. In fact, at the time of writing we are working on adaptations for two more languages (French and Dutch) and intend to further broaden the set of languages for which the tasks are available. As part of this process, we have created an site using the CrowdIn for generating and reviewing translation materials. The LEVANTE researcher website documents the use of this platform and our ongoing language adaptation efforts: <https://researcher.levante-network.org>.

Our current task set is designed for children ages 5--12, but we are currently pursuing downward extension of many of these tasks for children ages 2--5. By design, the forced choice, tablet-based format of our tasks makes them highly amenable to use with younger children [@frank2016]. In our downward extension work, we are adding easier items to each task, increasing the number of practice trials, and increasing the amount of instructions. We anticipate that not all tasks will be usable for all ages, but for at least the executive function and language tasks there is strong evidence of usability with children under age 5 [@davidson2006;@bishop1982]. 

Several of the nine tasks remain under construction, and we expect gains in reliability and validity as items are refined, distractors improved, and instructions localised. We are actively monitoring task performance and iteratively refining items as new data accrue across sites. This includes updating IRT calibrations, conducting routine checks for differential item functioning, and making revisions guided by fit diagnostics and error patterns. We are attempting to strike a balance between minimising bias and ensuring that observed differences reflect genuine developmental variation rather than artefacts of the measurement process.

In sum, we have taken steps here towards a core set of tasks for measuring children’s learning and development. We hope that this work is the beginning, rather than the end, of a process of iterative refinement and growth that results in a common, open measurement toolkit for developmental science.

\newpage

# References

::: {#refs}
:::
