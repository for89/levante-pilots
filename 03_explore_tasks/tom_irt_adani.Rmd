---
title: "TOM IRT Models"
author: "Adani Abutto"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    toc: true
    toc_depth: 3
    toc_float: true
    theme: cosmo
    highlight: tango

header-includes:
    - \usepackage{setspace}\doublespacing
---

# Background Info

Info on data structure:

<https://docs.google.com/document/d/1v-nqBxk3R9cSbvJshaBsWQp7KUtBjQSdMNXHw9NDfKg/edit?tab=t.0#heading=h.c60fcjq089m>

TOM item bank:

<https://docs.google.com/spreadsheets/d/1MlU4eOd45XVMg7HrnTDGZ3rv1cfNjvjpdc8e_edQqQk/edit?gid=520489060#gid=520489060>

Automated runs through tasks:

<https://crowdin.com/project/levantetranslations/screenshots>

Airtables:

[airtable.com/appe2p0S3xk4DL2qc/shrSnlS1BwADsrBGq](airtable.com/appe2p0S3xk4DL2qc/shrSnlS1BwADsrBGq)

<https://airtable.com/appIk9XNTZZns1F1F/shr50bi3Y8MeNCjQE>

------------------------------------------------------------------------------------------

# Setup

```{r setup, include = T, message = F, warning = F}

# load relevant libraries and functions
library(here)
library(glue)          # for working with images
library(png)           
library(grid)
library(patchwork)
library(DT)            # for nicer tables
library(corrr)         # for correlations
library(mirt)          # for IRT models
library(tidyverse)     # for everything else
library(psych)
library(dplyr)

# set default code chunk options
knitr::opts_chunk$set(echo = T, warning = F, message = F)

# set default plot theme 
theme_set(theme_classic() + 
            theme(text = element_text(size = 32))) 

# fix print width for knitted doc
options(width = 70)

# set random seed
set.seed(1)

```

First, let's fetch the data.

```{r}

# read in the data
tom_data_long =
  read_rds(here("01_fetched_data/task_data_nested.rds")) %>%
  # keep only tom tasks
  filter(item_task == "tom") %>%
  # unnest
  unnest(data) %>%
  # filter out missing item uids
  filter(site != "pilot_langcog_us") %>%
  # convert booleans to numeric
  mutate(correct = unlist(correct)) %>%
  mutate(correct = case_when(correct == T ~ 1,
                             correct == F ~ 0,
                             TRUE ~ NA_real_))

# add age information
df.age =
  read_rds(here(glue("01_fetched_data/run_data.rds"))) %>%
  select(run_id, age)

tom_data_long =
  tom_data_long %>%
  left_join(df.age, by = "run_id")

```

Let's have a look at key aspects of the data:

```{r}

# number of subjects
Hmisc::describe(tom_data_long$user_id)

# number of runs
Hmisc::describe(tom_data_long$run_id)

# number of items (38)
Hmisc::describe(tom_data_long$item_uid)

# number of item groups (6)
Hmisc::describe(tom_data_long$item_group)

# test sites
Hmisc::describe(tom_data_long$dataset)

```

And then let's run simple item and subject statistics:

```{r}

# first, filter out erroneous leuphana_DE runs
unique(tom_data_long$response)

tom_data_long =
  tom_data_long %>%
  filter(answer != "hostile-attribution-scene3-q2-ans1")

unique(tom_data_long$response)

# % correct per item per site
tom_data_long %>%
  group_by(site, item_uid) %>%
  summarise(n = n(),
            mean_age = round(mean(age, na.rm = T),
                             2),
            n_correct = sum(correct == 1),
            pct_correct = round(mean(correct == 1), 3)) %>%
  arrange(site, pct_correct) %>%
  datatable()

# subject-level descriptives
tom_data_long %>%
  group_by(site, user_id, run_id) %>%
  summarise(correct = round(mean(correct, na.rm = T),
                            2),
            age = round(mean(age, na.rm = T),
                        2),
            n_items = n_distinct(item_uid)) %>%
  datatable()

```

We see that there are **38 items** total, clustered into **6** **item groups** (deception,
interpretation, moral reasoning, reality/false belief, reference, and second order).

There are also **three test sites: CA, CO, and DE**, but not all test sites have data for
all 38 items. That's how we end up with 102 table entries (rather than the full 3\*38 =
114).

```{r}

tom_data_long %>%
  distinct(site, item_uid) %>%
  count(site,
        name = "n_items_with_data")

```

Now let's also check if there are any items for which particular sites have NO data:

```{r}

tom_data_long %>%
  count(site, item_uid) %>%
  pivot_wider(names_from = site,
              values_from = n,
              values_fill = 0) %>%
  arrange(across(everything(), ~ .x == 0,
                 .names = "missing_{.col}")) %>%
  datatable()

```

There are **7 items for which this is true**. We will need to account for this in our
model code.

Now, for each of our 38 (-7) items, we have **four different IRT models** we want to run:

1.  1PL/Rasch model
2.  1PL/Rasch model but with configural scoring
3.  2PL model
4.  2PL model but with configural scoring

With these, we will obtain **item difficulty and item discrimination** for each of our
items.

------------------------------------------------------------------------------------------

As a sanity check, let's first plot % correct per item across age:

```{r fig.width = 12, fig.height = 8}

tom_data_long %>%
  ggplot(aes(x = age,
             y = correct,
             color = site)) +
  geom_point(alpha = 0.3, size = 1) +
  geom_smooth(method = "loess", se = T) +
  scale_y_continuous(limits = c(0, 1),
                     labels = scales::percent) +
  labs(x = "Age (years)",
       y = "% correct",
       color = "Site",
       title = "% correct by item, age, and site") +
  facet_wrap(~ item_uid,
             ncol = 6) +
  theme_classic(base_size = 12) +
  theme(legend.position = "bottom")

```

Looks reasonable. Now let's prep the IRT data:

```{r}

# first, let's create a wide-format data frame
run_group_df =
  tom_data_long %>%
  distinct(run_id, site)

tom_irt_data =
  tom_data_long %>%
  # keep only relevant cols
  select(user_id, run_id, site, item, item_uid,
         item_group, timestamp, trial_id, response, answer, correct) %>%
  # if there are multiple responses for the same item in a single run, keep only the *last* response for each child
  arrange(run_id, item_uid) %>%
  group_by(run_id, item_uid) %>%
  slice_tail(n = 1) %>%
  # pivot to wide format
  ungroup() %>%
  pivot_wider(id_cols = run_id,
              names_from = item_uid,
              values_from = correct) %>%
  left_join(run_group_df, by = "run_id")

# now, before we can run our models, we need to exclude two types of items:

# 1) for any model: items with no variance
novar_items =
  c("tom_second_order_reality_check_2")

# 2) for multigroup models: items that are not shared across sites (i.e., for which some sites have no data)
nonshared_items =
  tom_data_long %>%
  count(site, item_uid) %>%
  pivot_wider(names_from = site, values_from = n, values_fill = 0) %>%
  # grab the items ones with 0 responses in any of the sites
  filter(if_any(everything(), ~ .x == 0)) %>%
  pull(item_uid)

# create new dfs in which these items are removed
items_to_remove =
  setdiff(nonshared_items, novar_items)

df.single =
  tom_irt_data %>%
  select(-(novar_items))

df.multigroup =
  tom_irt_data %>%
  select(-c(novar_items, nonshared_items))

```

Now let's create a response matrix for our IRT models:

```{r}

# keep only cols with correct/incorrect vals (1 and 0)
response_matrix =
  df.single %>%
  column_to_rownames("run_id") %>%
  select(-c(site)) %>%
  as.data.frame()

group_vec =
  df.single %>%
  distinct(run_id, site) %>%
  filter(run_id %in% rownames(response_matrix)) %>%
  arrange(match(run_id, rownames(response_matrix))) %>%
  pull(site)

names(group_vec) =
  rownames(response_matrix)

# create group vector with site information
response_matrix_multigroup =
  df.multigroup %>%
  column_to_rownames("run_id") %>%
  select(-c(site)) %>%
  as.data.frame()

group_vec_multigroup =
  df.multigroup %>%
  distinct(run_id, site) %>%
  filter(run_id %in% rownames(response_matrix_multigroup)) %>%
  arrange(match(run_id, rownames(response_matrix_multigroup))) %>%
  pull(site)

names(group_vec_multigroup) =
  rownames(response_matrix_multigroup)

```

# First Pass: Various IRT Models

Now let's prep our functions for running our IRT models.

```{r}

# The function takes two parameters: the model type (1pl vs 2pl), and the invariance structure (configural vs nonconfigural)
fit_irt_model =
  function(response_matrix,
           group_vec = NULL,
           model_type = "Rasch",
           configural = T) {
    
    if (!is.null(group_vec)) {
    # configural model: all parameters can vary freely
    invariance_setting = if (configural) {
      NULL
      }
    # nonconfigural model: group means and variance can vary, but item parameters are fixed
    else {
      c(colnames(response_matrix),
        "free_means", "free_var")
    }
    
    model = 
      multipleGroup(data = response_matrix_multigroup,
                    model = 1, # unidimensional
                    itemtype = model_type,
                    group = group_vec,
                    invariance = invariance_setting,
                    verbose = T,
                    technical = list(NCYCLES = 5000))
    } else {
      model = mirt(data = response_matrix,
                   model = 1,
                   itemtype = model_type,
                   verbose = TRUE,
                   technical = list(NCYCLES = 5000))
    }
    
    return(model)
    }

```

Now, using the function we defined above, let's run our four models for all of the items:

```{r message = F, warning = F}

# fit 1PL Rasch model, nonconfigural
mod_rasch =
  fit_irt_model(response_matrix,
                group_vec = NULL,
                model_type = "Rasch",
                configural = F)

```

```{r message = F, warning = F}

# fit 2PL model, nonconfigural
mod_2pl =
  fit_irt_model(response_matrix,
                group_vec = NULL,
                model_type = "2PL",
                configural = F)

```

```{r message = F, warning = F}

# fit 1 PL Rasch model, *configural*
mod_rasch_configural =
  fit_irt_model(response_matrix_multigroup,
                model_type = "Rasch",
                group_vec = NULL,
                configural = T)

```

```{r message = F, warning = F}

# fit 2 PL Rasch model, *configural*
mod_2pl_configural =
  fit_irt_model(response_matrix_multigroup,
                model_type = "2PL",
                group_vec = NULL,
                configural = T)

```

```{r}

# create function to extract parameters for plotting
item_params = function(model, model_name = "unknown_model") {
  # extract item-level coefficients
  coefs = coef(model, simplify = T)$items

  if (is.null(coefs) || nrow(coefs) == 0) return(tibble())

  bic_value = model@Fit[["BIC"]]
  
  tibble(
    item_uid = rownames(coefs),
    a = coefs[, "a1"],
    d = coefs[, "d"],
    b = -coefs[, "d"] / coefs[, "a1"],
    BIC = bic_value,
    site = "pooled",
    model = model_name
  )
}

```

Then, let's grab the parameters for each item and plot them.

```{r}

# grab parameters
item_params_all =
  bind_rows(item_params(mod_rasch,
                        "rasch"),
            item_params(mod_2pl,
                        "2pl"),
            item_params(mod_rasch_configural,
                        "rasch_configural"),
            item_params(mod_2pl_configural,
                        "2pl_configural"))

```

First, let's print a big table with item difficulties and item discriminations, sorted by
model.

Higher discrimination values are better. For difficulty, lower value means easier item,
higher value means harder item.

```{r}

item_params_all %>%
  select(item_uid, model, a, b, BIC) %>%
  mutate(across(c(a, b, BIC), ~ round(.x, 3))) %>%
  rename(discr = a,
         diff = b) %>%
  arrange(model, diff) %>%
  datatable()

```

And here is a table showing only the items that performed worst in previous analyses:

```{r}

prev_worst_performers = c("tom_reference_reference",
                          "tom_moral_reasoning_reality_check_1",
                          "tom_moral_reasoning_reality_check_2",
                          "tom_deception_reality_check_3",
                          "tom_deception_reality_check_1")

item_params_all %>%
  select(item_uid, site, model, a, b) %>%
  mutate(across(c(a, b), ~ round(.x, 3))) %>%
  rename(discr = a,
         diff = b) %>%
  arrange(model, site, diff) %>%
  filter(item_uid %in% prev_worst_performers) %>%
  arrange(diff, decreasing = T) %>%
  datatable()

```

If we look at JUST RASCH CONFIGURAL (overall 'winning' model for TOM on overlapping
items):

```{r}

item_params_all %>%
  select(item_uid, site, model, a, b) %>%
  mutate(across(c(a, b), ~ round(.x, 3))) %>%
  rename(discr = a,
         diff = b) %>%
  arrange(model, site, diff) %>%
  filter(item_uid %in% prev_worst_performers) %>%
  filter(model == "rasch_configural") %>%
  arrange(diff, decreasing = T) %>%
  datatable()

```

## Corr Matrix

Run correlation matrix to check how consistent item difficulty and item discrimination
estimates are across models:

```{r}

params_wide =
  item_params_all %>%
  select(item_uid, model, a, b) %>%
  pivot_wider(names_from = model,
              values_from = c(a, b))

# correlate discrimination (a)
cor_a =
  params_wide %>%
  select(starts_with("a_")) %>%
  # exclude Rasch models because discrimination is constant
  select(-starts_with("a_rasch")) %>%
  select(-starts_with("a_rasch_configural")) %>%
  correlate(method = "pearson")

cor_a %>%
  shave(upper = T)

# correlate difficulty (b)
cor_b =
  params_wide %>%
  select(starts_with("b_")) %>%
  correlate(method = "pearson")

cor_b %>%
  shave(upper = T)

```

```{r fig.width = 10, fig.height = 10}

cor_b %>%
  select(-c(term)) %>%
  as.matrix() %>%
  corPlot(main = "Correlations between Item DIFFICULTY Parameters across Models",
          numbers = T,
          upper = T,
          cex = 1)

```

```{r fig.width = 10, fig.height = 10}

cor_a %>%
  select(-c(term)) %>%
  as.matrix() %>%
  corPlot(main = "Correlations between Item DISCRIMINATION Parameters across Models",
          symbols = T,
          numbers = T,
          upper = T,
          cex = 1,)

```

## Item Difficulty

Just item difficulties by model:

```{r fig.width = 14, fig.height = 13}

# plot item difficulties for each item
ggplot(item_params_all,
       aes(x = item_uid,
           y = b,
           color = model,
           group = model)) +
  geom_point(position = position_dodge(width = 0.1),
             size = 4,
             alpha = .5) +
  geom_hline(yintercept = 0,
             color = "black",
             linetype = "dashed") +
  labs(title = "Item Difficulties by Model",
       subtitle = "higher/+ is harder, lower/- is easier",
       y = "Difficulty (b)",
       x = "Item") +
  ylim(-10, 10) +
  theme_minimal(base_size = 28) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  coord_flip()

```

## Item Discrimination

Just item discrimination by model (discrimination is fixed for Rasch model, so no variance
there):

```{r fig.width = 14, fig.height = 13}

# plot item discrimination for each item
ggplot(item_params_all,
       aes(x = item_uid,
           y = a,
           color = model,
           group = model)) +
  geom_point(position = position_dodge(width = 0.1),
             size = 4,
             alpha = .5) +
  geom_hline(yintercept = 1,
             color = "darkgreen",
             linetype = "dashed") +
  labs(title = "Item Discrimination by Model",
       subtitle = "closer to 0 & - is worse, ≥ 1 is desirable",
       y = "Discrimination (a)",
       x = "Item") +
  scale_y_continuous(limits = c(-5, 5),
                     breaks = c(seq(-5, 5, 1))) +
  theme_minimal(base_size = 28) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  coord_flip()

```

## ICCs

```{r}

# set function
compute_icc_curve =
  function(a, b, item_uid, model,
           theta_range = seq(-4, 4, length.out = 100)) {
    tibble(theta = theta_range,
           prob = 1 / (1 + exp(-a * (theta_range - b))),
           a = a,
           b = b,
           item_uid = item_uid,
           model = model)
    }

# apply accros items
icc_curves_all =
  item_params_all %>%
  select(a, b, item_uid, model) %>%
  # ..1 = a, ..2 = b, ..3 = item_uid, ..4 = model
  pmap_dfr(~ compute_icc_curve(a = ..1,
                               b = ..2,
                               item_uid = ..3,
                               model = ..4))

```

```{r fig.width = 39, fig.height = 20}

icc_curves_all %>%
  ggplot(aes(x = theta, y = prob, color = model)) +
  geom_line(size = 2, alpha = .5) +
  facet_wrap(~ item_uid, scales = "free_y") +
  labs(title = "Item Characteristic Curves (ICCs)",
       x = "Theta",
       y = "Estimate for P(correct response)",
       color = "Model") +
  theme_classic(base_size = 24) +
  theme(legend.position = "top")

```

Just the worst performing items:

```{r fig.width = 10, fig.height = 15}

icc_curves_all %>%
  filter(item_uid %in% prev_worst_performers) %>% 
  ggplot(aes(x = theta, y = prob, color = model, group = model)) +
  geom_line(size = 2, alpha = .5) +
  facet_wrap(~ item_uid + model, scales = "free_y", ncol = 4) +
  labs(title = "Item Characteristic Curves (ICCs)",
       subtitle = "Only Worst Performers",
       x = "Theta",
       y = "Estimate for P(correct response)",
       color = "Model") +
  ylim(0, 1) +
  theme_classic(base_size = 10) +
  theme(legend.position = "top")

```

If we look at JUST RASCH CONFIGURAL (overall 'winning' model for TOM on overlapping
items):

```{r fig.height = 3, fig.width = 9}

icc_curves_all %>%
  # filter(item_uid == "tom_reference_reference") %>%
  filter(item_uid %in% prev_worst_performers) %>% 
  filter(model == "rasch_configural") %>% 
  ggplot(aes(x = theta, y = prob)) +
  geom_line(size = 2, alpha = .5) +
  facet_wrap(~ item_uid, scales = "free_y", ncol = 5) +
  labs(title = "Item Characteristic Curve (ICC)",
       # subtitle = "Only Worst Performers",
       x = "Theta",
       y = "Estimate for P(correct response)",
       color = "Model") +
  ylim(0, 1) +
  theme_classic(base_size = 10) +
  theme(legend.position = "top")

```

------------------------------------------------------------------------------------------

# Second Pass: Just 2PL Scalar Model

First, let's create a data frame without the problematic items as listed here:

<https://docs.google.com/document/d/1lJRC68uo-kbo2RNXxjv8ebEXcNyVnNBtA5_mUmWuQ0g/edit?tab=t.0>

```{r}

stinker_items = 
  c("tom_reference_reference",
    "tom_moral_reasoning_reality_check_1",
    "tom_moral_reasoning_reality_check_2",
    "tom_deception_reality_check_1",
    "tom_deception_reality_check_3")

df.downward_extension =
  tom_irt_data %>%
  select(-(stinker_items))

# create a new matrix; keep only cols with correct/incorrect vals (1 and 0)
response_matrix_downward_extension =
  df.downward_extension %>%
  column_to_rownames("run_id") %>%
  select(-c(site)) %>%
  as.data.frame()

# create new object with group info
group_vec_downward_extension =
  df.downward_extension %>%
  distinct(run_id, site) %>%
  filter(run_id %in% rownames(response_matrix_downward_extension)) %>%
  arrange(match(run_id, rownames(response_matrix_downward_extension))) %>%
  pull(site)

names(group_vec) =
  rownames(response_matrix_downward_extension)

```

Then, let's fit our 2PL scalar model:

```{r}

# fit model
mod_2pl_scalar =
  multipleGroup(data = response_matrix_downward_extension,
                model = 1,
                group = group_vec_downward_extension,
                itemtype = "2PL",
                invariance = c(colnames(response_matrix_downward_extension),
                               mean = "free", var = "free"),
                verbose = T,
                technical = list(NCYCLES = 5000))

```

Now grab the parameter estimates from the model:

```{r}

# grab parameters
item_params_scalar = function(model, model_name = "2pl_scalar", item_names) {
  group_names = model@Data$groupNames
  num_items = length(item_names)

  # Extract shared difficulty parameters from Group 1
  item_par_objects_group1 = model@ParObjects$pars[[1]]@ParObjects$pars[1:num_items]

  d_vals = map_dbl(item_par_objects_group1, function(x) {
    d_idx = which(x@parnames == "d")
    x@par[d_idx]
  })

  # Extract site-specific discriminations and compute difficulty
  all_params = map_dfr(seq_along(group_names), function(g) {
    group_pars = model@ParObjects$pars[[g]]@ParObjects$pars[1:num_items]

    a_vals = map_dbl(group_pars, function(x) {
      a_idx = which(x@parnames == "a1")
      x@par[a_idx]
    })

    tibble(
      item_uid = item_names,
      a = a_vals,
      d = d_vals,
      b = -d_vals / a_vals,
      site = group_names[g],
      model = model_name
    )
  })

  return(all_params)
}

item_names = colnames(response_matrix_downward_extension)

item_params_2pl_scalar = item_params_scalar(
  mod_2pl_scalar,
  item_names = item_names
)

```

First, let's put all the estimates into a table, grouped by item and site:

```{r}

item_params_2pl_scalar %>%
  select(item_uid, model, a, b, site) %>%
  mutate(across(c(a, b), ~ round(.x, 3))) %>%
  rename(discr = a,
         diff = b) %>%
  arrange(model, diff) %>%
  datatable()

```

And then plot them. Item difficulties are held constant, so let's look at item
discrimination:

```{r fig.width = 20, fig.height = 22}

# plot item discrimination for each item
ggplot(item_params_2pl_scalar,
       aes(x = item_uid,
           y = a,
           color = site)) +
  geom_point(position = position_dodge(width = 0.1),
             size = 4,
             alpha = .5) +
  geom_hline(yintercept = 1,
             color = "darkgreen",
             linetype = "dashed") +
  labs(title = "Item Discrimination by Model",
       subtitle = "closer to 0 & - is worse, ≥ 1 is desirable",
       y = "Discrimination (a)",
       x = "Item") +
  scale_y_continuous(limits = c(-20, 20),
                     breaks = c(seq(-15, 15, 5))) +
  facet_grid(~site) +
  theme_minimal(base_size = 36) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1),
        legend.position = "none") +
  coord_flip()

```

And then ICC:

**The lines are identical across sites so we only see 1 (instead of 3).**

```{r fig.width = 30, fig.height = 30}

compute_icc_curve = function(a, b, item_uid, model, site) {
  tibble(
    theta = seq(-4, 4, length.out = 200),
    prob = 1 / (1 + exp(-a * (theta - b))),
    item_uid = item_uid,
    model = model,
    site = site
  )
}

icc_curves_nonstinker =
  item_params_2pl_scalar %>%
  select(a, b, item_uid, model, site) %>%
  # ..1 = a, ..2 = b, ..3 = item_uid, ..4 = model
  pmap_dfr(~ compute_icc_curve(a = ..1,
                               b = ..2,
                               item_uid = ..3,
                               model = ..4,
                               site = ..5))

icc_curves_nonstinker %>%
  ggplot(aes(x = theta, y = prob, color = site)) +
  geom_line(size = 1.2, alpha = .5) +
  facet_wrap(~ item_uid, scales = "free_y",
             ncol = 6) +
  ylim(0, 1) +
  annotate("text", x = 0, y = 0.1,
         label = "identical parameters across sites",
         size = 3.5, hjust = 0, color = "gray30") +
  labs(title = "ICCs by Item + Site",
       subtitle = "Scalar 2PL model",
       x = "Theta",
       y = "Estimate for P(correct response)",
       color = "Site") +
  theme_classic(base_size = 20) +
  theme(legend.position = "top")

```

------------------------------------------------------------------------------------------

**\*\* TODO \*\***: Model outfit statistics
