---
title: "TOM IRT Models"
author: "Adani Abutto"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    toc: true
    toc_depth: 3
    toc_float: true
    theme: cosmo
    highlight: tango

header-includes:
    - \usepackage{setspace}\doublespacing
---

# Background Info

Info on data structure:

<https://docs.google.com/document/d/1v-nqBxk3R9cSbvJshaBsWQp7KUtBjQSdMNXHw9NDfKg/edit?tab=t.0#heading=h.c60fcjq089m>

TOM item bank:

<https://docs.google.com/spreadsheets/d/1MlU4eOd45XVMg7HrnTDGZ3rv1cfNjvjpdc8e_edQqQk/edit?gid=520489060#gid=520489060>

Automated runs through tasks:

<https://crowdin.com/project/levantetranslations/screenshots>

Airtables:

[airtable.com/appe2p0S3xk4DL2qc/shrSnlS1BwADsrBGq](airtable.com/appe2p0S3xk4DL2qc/shrSnlS1BwADsrBGq)

<https://airtable.com/appIk9XNTZZns1F1F/shr50bi3Y8MeNCjQE>

------------------------------------------------------------------------------------------

# Setup

```{r setup, include = T, message = F, warning = F}

# load relevant libraries and functions
library(here)
library(glue)          # for working with images
library(png)           
library(grid)
library(patchwork)
library(DT)            # for nicer tables
library(corrr)         # for correlations
library(mirt)          # for IRT models
library(tidyverse)     # for everything else
library(dplyr)

# set default code chunk options
knitr::opts_chunk$set(echo = T, warning = F, message = F)

# set default plot theme 
theme_set(theme_classic() + 
            theme(text = element_text(size = 32))) 

# fix print width for knitted doc
options(width = 70)

# set random seed
set.seed(1)

```

First, let's fetch the data.

```{r}

# read in the data
tom_data_long =
  read_rds(here("01_fetched_data/task_data_nested.rds")) %>%
  # keep only tom tasks
  filter(item_task == "tom") %>%
  # unnest
  unnest(data) %>%
  # filter out missing item uids
  filter(site != "pilot_langcog_us") %>%
  # convert booleans to numeric
  mutate(correct = unlist(correct)) %>%
  mutate(correct = case_when(correct == T ~ 1,
                             correct == F ~ 0,
                             TRUE ~ NA_real_))

# add age information
df.age =
  read_rds(here(glue("01_fetched_data/run_data.rds"))) %>%
  select(run_id, age)

tom_data_long =
  tom_data_long %>%
  left_join(df.age, by = "run_id")

```

Let's have a look at key aspects of the data:

```{r}

# number of subjects (681)
Hmisc::describe(tom_data_long$user_id)

# number of runs (787)
Hmisc::describe(tom_data_long$run_id)

# number of items (38)
Hmisc::describe(tom_data_long$item_uid)

# number of item groups (6)
Hmisc::describe(tom_data_long$item_group)

# test sites (4)
Hmisc::describe(tom_data_long$dataset)

```

And then let's run simple item and subject statistics:

```{r}

# % correct per item per site
tom_data_long %>%
  group_by(site, item_uid) %>%
  summarise(n = n(),
            mean_age = round(mean(age, na.rm = T),
                             2),
            n_correct = sum(correct == 1),
            pct_correct = round(mean(correct == 1), 3)) %>%
  arrange(site, pct_correct) %>%
  datatable()

# subject-level descriptives
tom_data_long %>%
  group_by(site, user_id, run_id) %>%
  summarise(correct = round(mean(correct, na.rm = T),
                            2),
            age = round(mean(age, na.rm = T),
                        2),
            n_items = n_distinct(item_uid)) %>%
  datatable()

```

We see that there are **38 items** total, clustered into **6** **item groups** (deception,
interpretation, moral reasoning, reality/false belief, reference, and second order).

There are also **three test sites: CA, CO, and DE**, but not all test sites have data for
all 38 items. That's how we end up with 102 table entries (rather than the full 3\*38 =
114).

```{r}

tom_data_long %>%
  distinct(site, item_uid) %>%
  count(site,
        name = "n_items_with_data")

```

Now let's also check if there are any items for which particular sites have NO data:

```{r}

tom_data_long %>%
  count(site, item_uid) %>%
  pivot_wider(names_from = site,
              values_from = n,
              values_fill = 0) %>%
  arrange(across(everything(), ~ .x == 0,
                 .names = "missing_{.col}")) %>%
  datatable()

```

There are **7 items for which this is true**. We will need to account for this in our
model code.

Now, for each of our 38 (-7) items, we have **four different IRT models** we want to run:

1.  1PL/Rasch model
2.  1PL/Rasch model but with configural scoring
3.  2PL model
4.  2PL model but with configural scoring

With these, we will obtain **item difficulty and item discrimination** for each of our
items.

------------------------------------------------------------------------------------------

# IRT Models

As a sanity check, let's plot % correct per item across age:

```{r}

tom_data_long %>%
  ggplot(aes(x = age,
             y = correct,
             color = site)) +
  geom_point(alpha = 0.3, size = 1) +
  geom_smooth(method = "loess", se = T) +
  scale_y_continuous(limits = c(0, 1),
                     labels = scales::percent) +
  labs(x = "Age (years)",
       y = "% correct",
       color = "Site",
       title = "% correct by item, age, and site") +
  facet_wrap(~ item_uid,
             ncol = 6) +
  theme_classic(base_size = 12) +
  theme(legend.position = "bottom")

```

Looks reasonable. Now let's prep the IRT data:

```{r}

# first, let's create a wide-format data frame
run_group_df =
  tom_data_long %>%
  distinct(run_id, site)

tom_irt_data =
  tom_data_long %>%
  # keep only relevant cols
  select(user_id, run_id, site, item, item_uid,
         item_group, timestamp, trial_id, response, answer, correct) %>%
  # if there are multiple responses for the same item in a single run, keep only the *last* response for each child
  arrange(run_id, item_uid) %>%
  group_by(run_id, item_uid) %>%
  slice_tail(n = 1) %>%
  # pivot to wide format
  ungroup() %>%
  pivot_wider(id_cols = run_id,
              names_from = item_uid,
              values_from = correct) %>%
  left_join(run_group_df, by = "run_id")

# now, before we can run our models, we need to exclude two types of items:

# 1) for any model: items with no variance
novar_items =
  c("tom_second_order_reality_check_2")

# 2) for multigroup models: items that are not shared across sites (i.e., for which some sites have no data)
nonshared_items =
  tom_data_long %>%
  count(site, item_uid) %>%
  pivot_wider(names_from = site, values_from = n, values_fill = 0) %>%
  # grab the items ones with 0 responses in any of the sites
  filter(if_any(everything(), ~ .x == 0)) %>%
  pull(item_uid)

# create new dfs in which these items are removed
items_to_remove =
  setdiff(nonshared_items, novar_items)

df.single =
  tom_irt_data %>%
  select(-(novar_items))

df.multigroup =
  tom_irt_data %>%
  select(-c(novar_items, nonshared_items))

```

Now let's create a response matrix for our IRT models:

```{r}

# keep only cols with correct/incorrect vals (1 and 0)
response_matrix =
  df.single %>%
  column_to_rownames("run_id") %>%
  select(-c(site)) %>%
  as.data.frame()

group_vec =
  df.single %>%
  distinct(run_id, site) %>%
  filter(run_id %in% rownames(response_matrix)) %>%
  arrange(match(run_id, rownames(response_matrix))) %>%
  pull(site)

names(group_vec) =
  rownames(response_matrix)

# create group vector with site information
response_matrix_multigroup =
  df.multigroup %>%
  column_to_rownames("run_id") %>%
  select(-c(site)) %>%
  as.data.frame()

group_vec_multigroup =
  df.multigroup %>%
  distinct(run_id, site) %>%
  filter(run_id %in% rownames(response_matrix_multigroup)) %>%
  arrange(match(run_id, rownames(response_matrix_multigroup))) %>%
  pull(site)

names(group_vec_multigroup) =
  rownames(response_matrix_multigroup)

```

Now let's prep our functions for running our IRT models.

```{r}

# The function takes two parameters: the model type (1pl vs 2pl), and the invariance structure (configural vs nonconfigural)
fit_irt_model =
  function(response_matrix,
           group_vec = NULL,
           model_type = "Rasch",
           configural = T) {
    
    if (!is.null(group_vec)) {
    # configural model: all parameters can vary freely
    invariance_setting = if (configural) {
      NULL
      }
    # nonconfigural model: group means and variance can vary, but item parameters are fixed
    else {
      c(colnames(response_matrix),
        "free_means", "free_var")
    }
    
    model = 
      multipleGroup(data = response_matrix_multigroup,
                    model = 1, # unidimensional
                    itemtype = model_type,
                    group = group_vec,
                    invariance = invariance_setting,
                    verbose = T,
                    technical = list(NCYCLES = 5000))
    } else {
      model = mirt(data = response_matrix,
                   model = 1,
                   itemtype = model_type,
                   verbose = TRUE,
                   technical = list(NCYCLES = 5000))
    }
    
    return(model)
    }

```

Now, using the function we defined above, let's run our four models for all of the items:

```{r message = F, warning = F, echo = F}

# fit 1PL Rasch model, nonconfigural
mod_rasch =
  fit_irt_model(response_matrix,
                group_vec = NULL,
                model_type = "Rasch",
                configural = F)

```

```{r message = F, warning = F, echo = F}

# fit 2PL model, nonconfigural
mod_2pl =
  fit_irt_model(response_matrix,
                group_vec = NULL,
                model_type = "2PL",
                configural = F)

```

```{r echo = F, message = F, warning = F}

# fit 1 PL Rasch model, *configural*
mod_rasch_configural =
  fit_irt_model(response_matrix_multigroup,
                model_type = "Rasch",
                group_vec = NULL,
                configural = T)

```

```{r message = F, warning = F, echo = F}

# fit 2 PL Rasch model, *configural*
mod_2pl_configural =
  fit_irt_model(response_matrix_multigroup,
                model_type = "2PL",
                group_vec = NULL,
                configural = T)

```

```{r}

# create function to extract parameters for plotting
item_params = function(model, model_name = "unknown_model") {
  # extract item-level coefficients
  coefs = coef(model, simplify = T)$items

  if (is.null(coefs) || nrow(coefs) == 0) return(tibble())

  bic_value = model@Fit[["BIC"]]
  
  tibble(
    item_uid = rownames(coefs),
    a = coefs[, "a1"],
    d = coefs[, "d"],
    b = -coefs[, "d"] / coefs[, "a1"],
    BIC = bic_value,
    site = "pooled",
    model = model_name
  )
}

```

Then, let's grab the parameters for each item and plot them.

```{r}

# grab parameters
item_params_all =
  bind_rows(item_params(mod_rasch,
                        "rasch"),
            item_params(mod_2pl,
                        "2pl"),
            item_params(mod_rasch_configural,
                        "rasch_configural"),
            item_params(mod_2pl_configural,
                        "2pl_configural"))

```

Let's print a big table with item difficulties and item discriminations, sorted by model.

Higher discrimination values are better. For difficulty, lower value means easier item,
higher value means harder item.

***TODO 1: INCLUDE MODEL BIC AND FIX ERRONEOUS ITEM_UID ASSIGNMENT***

***TODO 2: WHY DO CONFIGURAL MODELS NOT SHOW UP?***

```{r}

item_params_all %>%
  select(item_uid, model, a, b, BIC) %>%
  mutate(across(c(a, b, BIC), ~ round(.x, 3))) %>%
  rename(discr = a,
         diff = b) %>%
  arrange(model, diff) %>%
  datatable()

```

And here is a table showing only the items that performed worst in previous analyses:

```{r}

prev_worst_performers = c("tom_reference_reference",
                          "tom_moral_reasoning_reality_check_1",
                          "tom_moral_reasoning_reality_check_2",
                          "tom_deception_reality_check_3",
                          "tom_deception_reality_check_1")

item_params_all %>%
  select(item_uid, site, model, a, b) %>%
  mutate(across(c(a, b), ~ round(.x, 3))) %>%
  rename(discr = a,
         diff = b) %>%
  arrange(model, site, diff) %>%
  filter(item_uid %in% prev_worst_performers) %>%
  arrange(diff, decreasing = T) %>%
  datatable()

```

## Corr Matrix

Run correlation matrix to check how consistent item difficulty and item discrimination
estimates are across models:

```{r}

params_wide =
  item_params_all %>%
  select(item_uid, model, a, b) %>%
  pivot_wider(names_from = model,
              values_from = c(a, b))

# correlate discrimination (a)
cor_a =
  params_wide %>%
  select(starts_with("a_")) %>%
  # exclude Rasch models because discrimination is constant
  select(-starts_with("a_rasch")) %>%
  select(-starts_with("a_rasch_configural")) %>%
  correlate(method = "pearson")

cor_a %>%
  shave(upper = T) %>%
  fashion()

# correlate difficulty (b)
cor_b =
  params_wide %>%
  select(starts_with("b_")) %>%
  correlate(method = "pearson")

cor_b %>%
  shave(upper = T) %>%
  fashion()

```

```{r}



```

```{r}



```

## Item Difficulty

Just item difficulties by model:

```{r fig.width = 14, fig.height = 13}

# plot item difficulties for each item
ggplot(item_params_all,
       aes(x = item_uid,
           y = b,
           color = model,
           group = model)) +
  geom_point(position = position_dodge(width = 0.1),
             size = 4) +
  labs(title = "Item Difficulties by Model",
       subtitle = "higher is harder, lower is easier",
       y = "Difficulty (b)",
       x = "Item") +
  ylim(-10, 10) +
  theme_minimal(base_size = 28) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  coord_flip()

```

## Item Discrimination

Just item discrimination by model (discrimination is fixed for Rasch model, so no variance
there):

```{r fig.width = 14, fig.height = 13}

# plot item discrimination for each item
ggplot(item_params_all,
       aes(x = item_uid,
           y = a,
           color = model,
           group = model)) +
  geom_point(position = position_dodge(width = 0.1),
             size = 4) +
  labs(title = "Item Discrimination by Model",
       subtitle = "closer to 0 is worse, ≥ 1 is desirable",
       y = "Discrimination (a)",
       x = "Item") +
  ylim(-5, 5) +
  theme_minimal(base_size = 28) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  coord_flip()

```

# ICCs

```{r}

# set function
compute_icc_curve =
  function(a, b, item_uid, model,
           theta_range = seq(-4, 4, length.out = 100)) {
    tibble(theta = theta_range,
           prob = 1 / (1 + exp(-a * (theta_range - b))),
           a = a,
           b = b,
           item_uid = item_uid,
           model = model)
    }

# apply accros items
icc_curves_all =
  item_params_all %>%
  select(a, b, item_uid, model) %>%
  # ..1 = a, ..2 = b, ..3 = item_uid, ..4 = model
  pmap_dfr(~ compute_icc_curve(a = ..1,
                               b = ..2,
                               item_uid = ..3,
                               model = ..4))

```

```{r fig.width = 39, fig.height = 20}

icc_curves_all %>%
  ggplot(aes(x = theta, y = prob, color = model)) +
  geom_line(size = 1, alpha = .5) +
  facet_wrap(~ item_uid, scales = "free_y") +
  labs(title = "Item Characteristic Curves (ICCs)",
       x = "Theta",
       y = "Estimate for P(correct response)",
       color = "Model") +
  theme_minimal(base_size = 24) +
  theme(legend.position = "top")

```

------------------------------------------------------------------------------------------

# Test-Retest Data

TODO

```{r}



```
