```{r}
library(tidyverse)
library(glue)
library(here)

sites <- c("ca_pilot", "co_pilot", "de_pilot")

task_data_nested <- sites |>
  set_names() |>
  map(\(s) read_rds(here(glue("01_processed_data/{s}/task_data_nested.rds")))) |>
  list_rbind(names_to = "site")

task_data_combined <- task_data_nested |>
  select(-task_id) |>
  unnest(data)
```

Get trog data

```{r}

trog <- filter(task_data_combined, 
             task_id %in% c("trog"))

trog

```

Get ages

```{r}
participants <- sites |>
  set_names() |>
  map(\(s) read_rds(here(glue("00_prepped_data/{s}/participants.rds")))) |>
  list_rbind(names_to = "site")

run_ages <- participants |>
  select(user_id, ages) |>
  unnest(ages)

ages <- run_ages |>
  group_by(user_id) |>
  summarise(age = mean(age))

trog <- left_join(trog, ages)

colnames(trog)
View (trog)

# ISSUE 1 : For one trog item, the question is written out e.g., the pencil is neither long nor red. #  Note use 'item_uid' field only for now not 'item_id'.

```

Linking trial-level data to item-level metadata

```{r}

id_map <- read_csv(here("02_score_data/item_metadata/pilot-item-ID mapping.csv"))

trial_id_map <- id_map |>
  mutate(trials = trials |> str_split(",")) |>
  unnest(trials) |>
  rename(trial_id = trials) |>
  mutate(trial_id = str_trim(trial_id))

# Join trial-level data to IRT item IDs

trog <- trog |>
  left_join(trial_id_map, by = "trial_id") |>
  filter(!is.na(item_uid))

trog

```

Exploring item types

```{r}

table(trog$corpus_trial_type)
sort(unique(trog$corpus_trial_type))

# Dupicate of 'post modified subject - update
trog <- trog |>
  mutate(corpus_trial_type = case_when(
    corpus_trial_type == "post modified subject" ~ "postmodified subject",
    TRUE ~ corpus_trial_type
  ))

sort(unique(trog$corpus_trial_type))
sort(unique(trog$item_id))
length(unique(trog$item_id)) # must be using Trog2 (2003 version) as Trog1 only has 80 items

```

Get the sum scores

```{r}

colnames(trog)

trog_runs <- trog |>
  group_by(site, user_id, run_id) |>
  summarise(
    correct = mean(correct, na.rm = TRUE),  # mean accuracy per run
    age = mean(age, na.rm = TRUE),          # average age
    n_items = n_distinct(item_uid)          # how many unique items were completed
  )

ggplot(trog_runs, aes(x = age, y = correct)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess") +
  ylim(0, 1) +
  facet_wrap(~site)

```

Item completion patterns across countries

```{r}

ggplot(trog_runs, aes(x = n_items)) +
  geom_histogram(binwidth = 1) +
  facet_wrap(~site)

```

Extract coefficeints from IRT model

```{r}

# Load multigroup IRT model outputs
best_multigroup <- readRDS(here("02_scored_data", "irt_outputs", "multigroup_best_outputs.rds"))
multigroup_scores <- readRDS(here("02_scored_data", "scores", "scores_multigroup.rds"))

View(best_multigroup)

trog_models <- best_multigroup$coefs[[which(best_multigroup$task_id == "trog")]]
length(trog_models)
str(trog_models)

# Assign site labels and flatten
trog_coefs <- trog_models |> 
  filter(str_detect(item, "^trog_"))  

# Calculate mean difficulty (d) per item across sites
trog_difficulty_overall <- trog_coefs |>
  group_by(item) |>
  summarise(mean_d = mean(d, na.rm = TRUE), .groups = "drop") |>
  arrange(mean_d)

# Assign block numbers (4 items per block, 25 blocks total)
trog_difficulty_blocks <- trog_difficulty_overall |>
  mutate(block = ceiling(row_number() / 4))

# Join block info back into full site-level IRT data
trog_difficulty <- trog_coefs |>
  left_join(trog_difficulty_blocks |> select(item, mean_d, block), by = "item")

# Check
trog_difficulty |> count(block)
View(trog_difficulty)


```

Plot item difficulty (mean across sites)

```{r}

ggplot(trog_difficulty_blocks, aes(x = reorder(item, mean_d), y = mean_d)) +
  geom_point() +
  coord_flip() +
  labs(
    x = "Item (ordered by mean difficulty)",
    y = "Mean Difficulty (d)",
    title = "TROG Item Difficulty (Mean Across Sites)"
  ) +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 6))

ggplot(trog_runs, aes(x = age, y = correct, col = site)) +
  geom_point(alpha = 0.5) +
  geom_smooth() +
  labs(
    x = "Age",
    y = "Accuracy",
    title = "TROG Accuracy by Age and Site"
  ) +
  theme_minimal()


```

Exploratory

```{r}

# Examining corpus trial type

trog_type <- trog |>
  group_by(site, user_id, run_id, corpus_trial_type) |>
  summarise(correct = mean(correct, na.rm = TRUE),
            age = mean(age, na.rm = TRUE))

ggplot(trog_type, aes(x = age, y = correct, col = corpus_trial_type)) +
  geom_smooth(method = "loess") +
  facet_wrap(~site)

ggplot(trog_type, aes(x = age, y = correct, col = corpus_trial_type)) +
  geom_smooth(method = "loess") +
  geom_rug(aes(x = age), sides = "b", alpha = 0.2) +
  facet_wrap(~site)

# Difficulty patterns across countries

item_difficulty <- trog |>
  group_by(site, item_uid) |>
  summarise(p_correct = mean(correct, na.rm = TRUE)) |>
  pivot_wider(names_from = site, values_from = p_correct)


install.packages("GGally")
library(GGally)

item_difficulty |>
  select(-item_uid) |>
  ggpairs(title = "Pairwise Site-Level Item Difficulties")



```